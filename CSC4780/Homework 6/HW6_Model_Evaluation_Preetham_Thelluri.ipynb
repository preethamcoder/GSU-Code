{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 6: Model Evaluation 2\n",
    "As in the previous assignments, in this homework assignment you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "This assignment will continue to utilize a copy of the extracted feature dataset we used in Homework 5. Recall that the dataset has been processed by performing log, z-score and range scaling. We continuing to use more than one partition worth of data, so for the scaling, the mean, standard deviation, minimum, and maximum were calculated using data from both partitions so that a global scaling can be performed on each partition. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This assignment will continue to use [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB) for a training set and [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD) as a testing set. \n",
    "\n",
    "---\n",
    "\n",
    "For this assignment, cleaning, transforming, and normalization of the data has been completed using both partitions to find the various minimum, maximum, standard deviation, and mean values needed to perform these operations. Recall from lecture that we should not perform these operations on each partition individually, but as a whole as there may(will) be different values for these in different partitions. \n",
    "\n",
    "For example, if we perform simple range scaling on each partition individually and we see a range of 0 to 100 in one partition and 0 to 10 in another. After individual scaling the values with 100 in the first would be mapped to 1 just like the values that had 10 in the second. This can cause serious performance problems in your model, so I have made sure that the normalization was treated properly for you. \n",
    "\n",
    "Below you will find the full partitions and `toy` sampled data from each partition, where only 20 samples from each of our 5 classes have been included in the data.  \n",
    "\n",
    "#### Full\n",
    "- [Full Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition1ExtractedFeatures.csv)\n",
    "- [Full Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "#### Toy\n",
    "- [Toy Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition1ExtractedFeatures.csv)\n",
    "- [Toy Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the two files, you should load each into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "As was done in Homework 5, for each of the models we evaluate in this assignmnet, you will calculate the True Skill Statistic score using the test data from Partition 2 to determine which model performs the best for classifying the positive flaring class.\n",
    "\n",
    "    True skill statistic (TSS) = TPR + TNR - 1 = TPR - (1-TNR) = TPR - FPR\n",
    "\n",
    "Where:\n",
    "\n",
    "    True positive rate (TPR) = TP/(TP+FN) Also known as recall or sensitivity\n",
    "    True negative rate (TNR) = TN/(TN+FP) Also known as specificity or selectivity\n",
    "    False positive rate (FPR) = FP/(FP+TN) = (1-TNR) Also known as fall-out or false alarm ratio\n",
    "\n",
    "\n",
    "**Recall**\n",
    "\n",
    "    True positive (TP)\n",
    "    True negative (TN)\n",
    "    False positive (FP)\n",
    "    False negative (FN)\n",
    "    \n",
    "See [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for more information.\n",
    "\n",
    "Below is a function implemented to provide your score for each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tss(y_true, y_predict):\n",
    "    '''\n",
    "    Calculates the true skill score for binary classification based on the output of the confusion\n",
    "    table function\n",
    "    \n",
    "        Parameters:\n",
    "            y_true   : A vector/list of values that represent the true class label of the data being evaluated.\n",
    "            y_predict: A vector/list of values that represent the predicted class label for the data being evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            tss_value (float): A floating point value (-1.0,1.0) indicating the TSS of the input data\n",
    "    '''\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    \n",
    "    return tp_rate - fp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In addition to the TSS, you will be asked to also calculate the Heidke Skill Score (HSS) to see how much better your model performs than a random forecast.  \n",
    "\n",
    "Below is a function implemented to provide your score fore each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hss(y_true, y_predict):\n",
    "    '''\n",
    "    Calculates the Heidke Skill Score for binary classification based on the output of the confusion\n",
    "    table function.\n",
    "    \n",
    "    The HSS measures the fractional improvement of the forecast over the standard forecast.\n",
    "    The \"standard forecast\" is usually the number correct by chance or the proportion \n",
    "    correct by chance.\n",
    "    \n",
    "        Parameters:\n",
    "            y_true   : A vector/list of values that represent the true class label of the data being evaluated.\n",
    "            y_predict: A vector/list of values that represent the predicted class label for the data being evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            hss_value (float): A floating point value (-inf,1.0) indicating the HSS of the input data. \n",
    "                Negative values indicate that the chance forecast is better, 0 means no skill, and a perfect forecast obtains a HSS of 1.\n",
    "    '''\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    #print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    P = float(TP + FN)\n",
    "    N = float(FP + TN)\n",
    "    numerator = 2*((TP * TN) - (FN * FP))\n",
    "    denominator = P*(FN + TN) + N*(TP + FP)\n",
    "    \n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As in the previous assignment, we will be utilizing a binary classification of our 5 class dataset. So, below is the helper function to change our class labels from the 5 class target feature to the binary target feature. The function is implemented to take a dataframe (e.g. our `abt`) and prepares it for a binary classification by merging the `X`- and `M`-class samples into one group, and the rest (`NF`, `B`, and `C`) into another group, labeled with `1`s and `0`s, respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomize_X_y(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    dichotomizes the dataset and split it into the features (X) and the labels (y).\n",
    "    \n",
    "    :return: two np.ndarray objects X and y.\n",
    "    \"\"\"\n",
    "    data_dich = data.copy()\n",
    "    data_dich['lab'] = data_dich['lab'].map({'NF': 0, 'B': 0, 'C': 0, 'M': 1, 'X': 1})\n",
    "    y = data_dich['lab']\n",
    "    X = data_dich.drop(['lab'], axis=1)\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reading the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/FDS'\n",
    "data_file = \"normalized_partition1ExtractedFeatures.csv\"\n",
    "data_file2 = \"normalized_partition2ExtractedFeatures.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file).replace('\\\\', '/'))\n",
    "abt2 = pd.read_csv(os.path.join(data_dir, data_file2).replace('\\\\', '/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Run Feature Selection\n",
    "\n",
    "Below you have code to perform feature selction using [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). The scoring function being used is [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif).\n",
    "\n",
    "Once feature selection is done with this one method, a set of training and testing dataframes are constructed by doing the following:\n",
    "\n",
    "* Utilizing the `get_support` function of the feature selection object we get a mask of the features we will select from our original analytics base table DataFrame.  \n",
    "\n",
    "* The mask of selected features is then paired with the `loc` function on our datframe containing only the descriptive features to get our selected featrues on all rows in our feature dataframe.\n",
    "\n",
    "* The set of selected features are concatenated with our labels to construct a training dataset.\n",
    "\n",
    "* This process was then repeated to construct the testing set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "\n",
    "# Split the target and descriptive features for Partition 1 into two \n",
    "# different DataFrame objects\n",
    "df_labels = abt['lab'].copy()\n",
    "df_feats = abt.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Split the target and descriptive features for Partition 2 inot two\n",
    "# different DataFrame Objects\n",
    "df_test_labels = abt2['lab'].copy()\n",
    "df_test_feats = abt2.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Do feature selection\n",
    "feats1 = SelectKBest(f_classif, k=numFeat).fit(df_feats, df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTUSJH_min</th>\n",
       "      <th>TOTUSJH_max</th>\n",
       "      <th>TOTUSJH_median</th>\n",
       "      <th>TOTUSJH_mean</th>\n",
       "      <th>TOTUSJH_linear_weighted_average</th>\n",
       "      <th>TOTUSJH_quadratic_weighted_average</th>\n",
       "      <th>TOTUSJH_last_value</th>\n",
       "      <th>ABSNJZH_max</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>...</th>\n",
       "      <th>ABSNJZH_gderivative_stddev</th>\n",
       "      <th>ABSNJZH_average_absolute_change</th>\n",
       "      <th>ABSNJZH_average_absolute_derivative_change</th>\n",
       "      <th>ABSNJZH_avg_mono_increase_slope</th>\n",
       "      <th>SAVNCPP_avg_mono_decrease_slope</th>\n",
       "      <th>SAVNCPP_avg_mono_increase_slope</th>\n",
       "      <th>SAVNCPP_dderivative_stddev</th>\n",
       "      <th>SAVNCPP_average_absolute_change</th>\n",
       "      <th>SAVNCPP_gderivative_stddev</th>\n",
       "      <th>SAVNCPP_average_absolute_derivative_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.248654</td>\n",
       "      <td>0.264179</td>\n",
       "      <td>0.257628</td>\n",
       "      <td>0.257166</td>\n",
       "      <td>0.257336</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.253303</td>\n",
       "      <td>0.246850</td>\n",
       "      <td>0.221667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>0.157880</td>\n",
       "      <td>0.150370</td>\n",
       "      <td>0.175954</td>\n",
       "      <td>0.907985</td>\n",
       "      <td>0.099690</td>\n",
       "      <td>0.100066</td>\n",
       "      <td>0.102358</td>\n",
       "      <td>0.085923</td>\n",
       "      <td>0.098160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.148949</td>\n",
       "      <td>0.161652</td>\n",
       "      <td>0.154316</td>\n",
       "      <td>0.154432</td>\n",
       "      <td>0.154392</td>\n",
       "      <td>0.154432</td>\n",
       "      <td>0.152454</td>\n",
       "      <td>0.080530</td>\n",
       "      <td>0.113363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118672</td>\n",
       "      <td>0.117204</td>\n",
       "      <td>0.113976</td>\n",
       "      <td>0.121236</td>\n",
       "      <td>0.966180</td>\n",
       "      <td>0.031892</td>\n",
       "      <td>0.034760</td>\n",
       "      <td>0.034980</td>\n",
       "      <td>0.036871</td>\n",
       "      <td>0.032019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.161101</td>\n",
       "      <td>0.174518</td>\n",
       "      <td>0.164688</td>\n",
       "      <td>0.165918</td>\n",
       "      <td>0.166817</td>\n",
       "      <td>0.167077</td>\n",
       "      <td>0.163801</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.158542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163447</td>\n",
       "      <td>0.159358</td>\n",
       "      <td>0.156617</td>\n",
       "      <td>0.163217</td>\n",
       "      <td>0.948398</td>\n",
       "      <td>0.049728</td>\n",
       "      <td>0.051331</td>\n",
       "      <td>0.052093</td>\n",
       "      <td>0.062940</td>\n",
       "      <td>0.045289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.292981</td>\n",
       "      <td>0.279195</td>\n",
       "      <td>0.278289</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.270127</td>\n",
       "      <td>0.260010</td>\n",
       "      <td>0.141124</td>\n",
       "      <td>0.188205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161562</td>\n",
       "      <td>0.139561</td>\n",
       "      <td>0.129280</td>\n",
       "      <td>0.138083</td>\n",
       "      <td>0.943507</td>\n",
       "      <td>0.059199</td>\n",
       "      <td>0.058710</td>\n",
       "      <td>0.059671</td>\n",
       "      <td>0.075896</td>\n",
       "      <td>0.051173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.091531</td>\n",
       "      <td>0.104345</td>\n",
       "      <td>0.097824</td>\n",
       "      <td>0.098008</td>\n",
       "      <td>0.098146</td>\n",
       "      <td>0.097687</td>\n",
       "      <td>0.094275</td>\n",
       "      <td>0.063610</td>\n",
       "      <td>0.086037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092659</td>\n",
       "      <td>0.090478</td>\n",
       "      <td>0.085964</td>\n",
       "      <td>0.089760</td>\n",
       "      <td>0.975876</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>0.025694</td>\n",
       "      <td>0.025180</td>\n",
       "      <td>0.028386</td>\n",
       "      <td>0.023367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88552</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.181140</td>\n",
       "      <td>0.205758</td>\n",
       "      <td>0.190417</td>\n",
       "      <td>0.192371</td>\n",
       "      <td>0.188244</td>\n",
       "      <td>0.185497</td>\n",
       "      <td>0.180158</td>\n",
       "      <td>0.108809</td>\n",
       "      <td>0.137283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>0.140601</td>\n",
       "      <td>0.134034</td>\n",
       "      <td>0.138010</td>\n",
       "      <td>0.939266</td>\n",
       "      <td>0.057497</td>\n",
       "      <td>0.065002</td>\n",
       "      <td>0.062841</td>\n",
       "      <td>0.056108</td>\n",
       "      <td>0.061766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88553</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.049787</td>\n",
       "      <td>0.058479</td>\n",
       "      <td>0.054282</td>\n",
       "      <td>0.054346</td>\n",
       "      <td>0.054770</td>\n",
       "      <td>0.055035</td>\n",
       "      <td>0.057382</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>0.058834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077821</td>\n",
       "      <td>0.071568</td>\n",
       "      <td>0.069265</td>\n",
       "      <td>0.073404</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>0.014260</td>\n",
       "      <td>0.014678</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>0.012104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88554</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>0.054027</td>\n",
       "      <td>0.053349</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>0.061008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076017</td>\n",
       "      <td>0.078785</td>\n",
       "      <td>0.079414</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.987869</td>\n",
       "      <td>0.012566</td>\n",
       "      <td>0.012885</td>\n",
       "      <td>0.013014</td>\n",
       "      <td>0.015375</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88555</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.115870</td>\n",
       "      <td>0.106425</td>\n",
       "      <td>0.106316</td>\n",
       "      <td>0.108176</td>\n",
       "      <td>0.107972</td>\n",
       "      <td>0.105768</td>\n",
       "      <td>0.101883</td>\n",
       "      <td>0.121610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099209</td>\n",
       "      <td>0.094667</td>\n",
       "      <td>0.089387</td>\n",
       "      <td>0.097608</td>\n",
       "      <td>0.973282</td>\n",
       "      <td>0.022278</td>\n",
       "      <td>0.027569</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.028041</td>\n",
       "      <td>0.023464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88556</th>\n",
       "      <td>B</td>\n",
       "      <td>0.344644</td>\n",
       "      <td>0.365650</td>\n",
       "      <td>0.356484</td>\n",
       "      <td>0.356761</td>\n",
       "      <td>0.359311</td>\n",
       "      <td>0.359726</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>0.166042</td>\n",
       "      <td>0.255197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188376</td>\n",
       "      <td>0.174548</td>\n",
       "      <td>0.166489</td>\n",
       "      <td>0.184904</td>\n",
       "      <td>0.910494</td>\n",
       "      <td>0.093319</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>0.095506</td>\n",
       "      <td>0.083846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88557 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTUSJH_min  TOTUSJH_max  TOTUSJH_median  TOTUSJH_mean  \\\n",
       "0      NF     0.248654     0.264179        0.257628      0.257166   \n",
       "1      NF     0.148949     0.161652        0.154316      0.154432   \n",
       "2      NF     0.161101     0.174518        0.164688      0.165918   \n",
       "3      NF     0.263200     0.292981        0.279195      0.278289   \n",
       "4      NF     0.091531     0.104345        0.097824      0.098008   \n",
       "...    ..          ...          ...             ...           ...   \n",
       "88552  NF     0.181140     0.205758        0.190417      0.192371   \n",
       "88553  NF     0.049787     0.058479        0.054282      0.054346   \n",
       "88554  NF     0.047382     0.062495        0.054054      0.054722   \n",
       "88555  NF     0.094972     0.115870        0.106425      0.106316   \n",
       "88556   B     0.344644     0.365650        0.356484      0.356761   \n",
       "\n",
       "       TOTUSJH_linear_weighted_average  TOTUSJH_quadratic_weighted_average  \\\n",
       "0                             0.257336                            0.255972   \n",
       "1                             0.154392                            0.154432   \n",
       "2                             0.166817                            0.167077   \n",
       "3                             0.273310                            0.270127   \n",
       "4                             0.098146                            0.097687   \n",
       "...                                ...                                 ...   \n",
       "88552                         0.188244                            0.185497   \n",
       "88553                         0.054770                            0.055035   \n",
       "88554                         0.054027                            0.053349   \n",
       "88555                         0.108176                            0.107972   \n",
       "88556                         0.359311                            0.359726   \n",
       "\n",
       "       TOTUSJH_last_value  ABSNJZH_max  ABSNJZH_stddev  ...  \\\n",
       "0                0.253303     0.246850        0.221667  ...   \n",
       "1                0.152454     0.080530        0.113363  ...   \n",
       "2                0.163801     0.134300        0.158542  ...   \n",
       "3                0.260010     0.141124        0.188205  ...   \n",
       "4                0.094275     0.063610        0.086037  ...   \n",
       "...                   ...          ...             ...  ...   \n",
       "88552            0.180158     0.108809        0.137283  ...   \n",
       "88553            0.057382     0.048622        0.058834  ...   \n",
       "88554            0.053345     0.045427        0.061008  ...   \n",
       "88555            0.105768     0.101883        0.121610  ...   \n",
       "88556            0.366120     0.166042        0.255197  ...   \n",
       "\n",
       "       ABSNJZH_gderivative_stddev  ABSNJZH_average_absolute_change  \\\n",
       "0                        0.169989                         0.157880   \n",
       "1                        0.118672                         0.117204   \n",
       "2                        0.163447                         0.159358   \n",
       "3                        0.161562                         0.139561   \n",
       "4                        0.092659                         0.090478   \n",
       "...                           ...                              ...   \n",
       "88552                    0.148995                         0.140601   \n",
       "88553                    0.077821                         0.071568   \n",
       "88554                    0.076017                         0.078785   \n",
       "88555                    0.099209                         0.094667   \n",
       "88556                    0.188376                         0.174548   \n",
       "\n",
       "       ABSNJZH_average_absolute_derivative_change  \\\n",
       "0                                        0.150370   \n",
       "1                                        0.113976   \n",
       "2                                        0.156617   \n",
       "3                                        0.129280   \n",
       "4                                        0.085964   \n",
       "...                                           ...   \n",
       "88552                                    0.134034   \n",
       "88553                                    0.069265   \n",
       "88554                                    0.079414   \n",
       "88555                                    0.089387   \n",
       "88556                                    0.166489   \n",
       "\n",
       "       ABSNJZH_avg_mono_increase_slope  SAVNCPP_avg_mono_decrease_slope  \\\n",
       "0                             0.175954                         0.907985   \n",
       "1                             0.121236                         0.966180   \n",
       "2                             0.163217                         0.948398   \n",
       "3                             0.138083                         0.943507   \n",
       "4                             0.089760                         0.975876   \n",
       "...                                ...                              ...   \n",
       "88552                         0.138010                         0.939266   \n",
       "88553                         0.073404                         0.986529   \n",
       "88554                         0.079521                         0.987869   \n",
       "88555                         0.097608                         0.973282   \n",
       "88556                         0.184904                         0.910494   \n",
       "\n",
       "       SAVNCPP_avg_mono_increase_slope  SAVNCPP_dderivative_stddev  \\\n",
       "0                             0.099690                    0.100066   \n",
       "1                             0.031892                    0.034760   \n",
       "2                             0.049728                    0.051331   \n",
       "3                             0.059199                    0.058710   \n",
       "4                             0.022294                    0.025694   \n",
       "...                                ...                         ...   \n",
       "88552                         0.057497                    0.065002   \n",
       "88553                         0.014260                    0.014678   \n",
       "88554                         0.012566                    0.012885   \n",
       "88555                         0.022278                    0.027569   \n",
       "88556                         0.093319                    0.093930   \n",
       "\n",
       "       SAVNCPP_average_absolute_change  SAVNCPP_gderivative_stddev  \\\n",
       "0                             0.102358                    0.085923   \n",
       "1                             0.034980                    0.036871   \n",
       "2                             0.052093                    0.062940   \n",
       "3                             0.059671                    0.075896   \n",
       "4                             0.025180                    0.028386   \n",
       "...                                ...                         ...   \n",
       "88552                         0.062841                    0.056108   \n",
       "88553                         0.013554                    0.018163   \n",
       "88554                         0.013014                    0.015375   \n",
       "88555                         0.024832                    0.028041   \n",
       "88556                         0.089917                    0.095506   \n",
       "\n",
       "       SAVNCPP_average_absolute_derivative_change  \n",
       "0                                        0.098160  \n",
       "1                                        0.032019  \n",
       "2                                        0.045289  \n",
       "3                                        0.051173  \n",
       "4                                        0.023367  \n",
       "...                                           ...  \n",
       "88552                                    0.061766  \n",
       "88553                                    0.012104  \n",
       "88554                                    0.011461  \n",
       "88555                                    0.023464  \n",
       "88556                                    0.083846  \n",
       "\n",
       "[88557 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a training dataset from Partition 1 with only the selected descriptive \n",
    "# features and the target feature\n",
    "df_selected_feats1 = df_feats.loc[:, feats1.get_support()]\n",
    "df_train_set1 = pd.concat([df_labels, df_selected_feats1], axis=1)\n",
    "\n",
    "# Construct a testing dataset from Partition 2 with only the selected descriptive\n",
    "# features and the target feature\n",
    "df_test_selected_feats1 = df_test_feats.loc[:, feats1.get_support()]\n",
    "df_test_set1 = pd.concat([df_test_labels, df_test_selected_feats1], axis=1)\n",
    "df_test_set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q1 (5 points)\n",
    "\n",
    "Using the feature selection task above as a template, you will now perform feature selection again to produce a second set of training and testing data. This time you will use the [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). \n",
    "\n",
    "Instead of using the [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) that is shown in the example documentation linked above, you will be utilizing the [LassoLars](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars) as your `estimator`.  You should also set the `max_features` to the number of features we are going to select (20 features). \n",
    "\n",
    "**Note:** The LassoLars class is a regression model and will not work on our string class lables. So, you need to map the `NF`, `B`, `C`, `M`, `X` labels to a range between `-1` and `1` before you attempt to fit the feature selection model to them. You can try evenly spacing them, or forcing the classes that will be used as the negative class into a tight range and those that will be used as the postive class into another tight range.  Maybe this another location for you to do hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "abt_cpy = abt.copy()\n",
    "abt2_cpy = abt2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_dderivative_mean</th>\n",
       "      <th>TOTUSJZ_gderivative_mean</th>\n",
       "      <th>SAVNCPP_gderivative_mean</th>\n",
       "      <th>ABSNJZH_kurtosis</th>\n",
       "      <th>TOTUSJH_gderivative_mean</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANSHR_slope_of_longest_mono_decrease</th>\n",
       "      <th>...</th>\n",
       "      <th>SHRGT45_slope_of_longest_mono_decrease</th>\n",
       "      <th>EPSX_slope_of_longest_mono_decrease</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTBSQ_gderivative_stddev</th>\n",
       "      <th>TOTPOT_average_absolute_change</th>\n",
       "      <th>TOTBSQ_average_absolute_change</th>\n",
       "      <th>MEANPOT_difference_of_medians</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_increase</th>\n",
       "      <th>TOTBSQ_avg_mono_increase_slope</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.979145</td>\n",
       "      <td>0.979265</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.683983</td>\n",
       "      <td>0.166475</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.953375</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.073789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.979309</td>\n",
       "      <td>0.979332</td>\n",
       "      <td>0.977080</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.682688</td>\n",
       "      <td>0.122735</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.998197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998806</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.979237</td>\n",
       "      <td>0.979280</td>\n",
       "      <td>0.976580</td>\n",
       "      <td>0.047912</td>\n",
       "      <td>0.680145</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.996353</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.017374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.979829</td>\n",
       "      <td>0.979674</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.183616</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.979351</td>\n",
       "      <td>0.976705</td>\n",
       "      <td>0.047934</td>\n",
       "      <td>0.684250</td>\n",
       "      <td>0.103106</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.997813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73487</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>0.979341</td>\n",
       "      <td>0.976041</td>\n",
       "      <td>0.052463</td>\n",
       "      <td>0.684865</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.997778</td>\n",
       "      <td>0.997607</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73488</th>\n",
       "      <td>NF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979596</td>\n",
       "      <td>0.979535</td>\n",
       "      <td>0.975673</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.696048</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.996946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997911</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.992283</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.012674</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>0.014201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73489</th>\n",
       "      <td>C</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.979785</td>\n",
       "      <td>0.979718</td>\n",
       "      <td>0.979449</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>0.695232</td>\n",
       "      <td>0.214363</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.973915</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.049894</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.019388</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.057426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73490</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.978214</td>\n",
       "      <td>0.978635</td>\n",
       "      <td>0.978052</td>\n",
       "      <td>0.048476</td>\n",
       "      <td>0.659593</td>\n",
       "      <td>0.184096</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999053</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.994456</td>\n",
       "      <td>0.020409</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.035119</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.043784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73491</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.979393</td>\n",
       "      <td>0.975928</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.687184</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.993584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.992271</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73492 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTBSQ_slope_of_longest_mono_decrease  TOTUSJZ_dderivative_mean  \\\n",
       "0      NF                               0.999113                  0.979145   \n",
       "1      NF                               0.999991                  0.979309   \n",
       "2      NF                               0.999990                  0.979237   \n",
       "3      NF                               0.999751                  0.979829   \n",
       "4      NF                               0.999948                  0.979352   \n",
       "...    ..                                    ...                       ...   \n",
       "73487  NF                               0.999872                  0.979317   \n",
       "73488  NF                               1.000000                  0.979596   \n",
       "73489   C                               0.999981                  0.979785   \n",
       "73490   B                               0.999581                  0.978214   \n",
       "73491  NF                               0.999994                  0.979405   \n",
       "\n",
       "       TOTUSJZ_gderivative_mean  SAVNCPP_gderivative_mean  ABSNJZH_kurtosis  \\\n",
       "0                      0.979265                  0.975866          0.046063   \n",
       "1                      0.979332                  0.977080          0.030247   \n",
       "2                      0.979280                  0.976580          0.047912   \n",
       "3                      0.979674                  0.975500          0.038173   \n",
       "4                      0.979351                  0.976705          0.047934   \n",
       "...                         ...                       ...               ...   \n",
       "73487                  0.979341                  0.976041          0.052463   \n",
       "73488                  0.979535                  0.975673          0.039513   \n",
       "73489                  0.979718                  0.979449          0.063704   \n",
       "73490                  0.978635                  0.978052          0.048476   \n",
       "73491                  0.979393                  0.975928          0.063802   \n",
       "\n",
       "       TOTUSJH_gderivative_mean  ABSNJZH_stddev  \\\n",
       "0                      0.683983        0.166475   \n",
       "1                      0.682688        0.122735   \n",
       "2                      0.680145        0.105429   \n",
       "3                      0.694549        0.183616   \n",
       "4                      0.684250        0.103106   \n",
       "...                         ...             ...   \n",
       "73487                  0.684865        0.057351   \n",
       "73488                  0.696048        0.136841   \n",
       "73489                  0.695232        0.214363   \n",
       "73490                  0.659593        0.184096   \n",
       "73491                  0.687184        0.044799   \n",
       "\n",
       "       MEANPOT_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.999915   \n",
       "1                                    0.999982   \n",
       "2                                    0.999988   \n",
       "3                                    0.999948   \n",
       "4                                    0.999954   \n",
       "...                                       ...   \n",
       "73487                                0.999655   \n",
       "73488                                0.999709   \n",
       "73489                                0.999804   \n",
       "73490                                0.999951   \n",
       "73491                                0.999985   \n",
       "\n",
       "       MEANSHR_slope_of_longest_mono_decrease  ...  \\\n",
       "0                                    0.998414  ...   \n",
       "1                                    0.998197  ...   \n",
       "2                                    0.999814  ...   \n",
       "3                                    0.999569  ...   \n",
       "4                                    0.997813  ...   \n",
       "...                                       ...  ...   \n",
       "73487                                0.999328  ...   \n",
       "73488                                0.996946  ...   \n",
       "73489                                0.999597  ...   \n",
       "73490                                0.999761  ...   \n",
       "73491                                0.993584  ...   \n",
       "\n",
       "       SHRGT45_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.998884   \n",
       "1                                    0.998806   \n",
       "2                                    0.999086   \n",
       "3                                    0.999646   \n",
       "4                                    0.999126   \n",
       "...                                       ...   \n",
       "73487                                0.998228   \n",
       "73488                                0.997911   \n",
       "73489                                0.999917   \n",
       "73490                                0.999053   \n",
       "73491                                0.999607   \n",
       "\n",
       "       EPSX_slope_of_longest_mono_decrease  \\\n",
       "0                                 0.999664   \n",
       "1                                 0.998982   \n",
       "2                                 0.996353   \n",
       "3                                 0.999499   \n",
       "4                                 0.999903   \n",
       "...                                    ...   \n",
       "73487                             0.997778   \n",
       "73488                             0.999393   \n",
       "73489                             0.999618   \n",
       "73490                             0.999981   \n",
       "73491                             0.992271   \n",
       "\n",
       "       USFLUX_slope_of_longest_mono_decrease  TOTBSQ_gderivative_stddev  \\\n",
       "0                                   0.953375                   0.015288   \n",
       "1                                   0.999580                   0.002713   \n",
       "2                                   0.998553                   0.004391   \n",
       "3                                   0.996641                   0.014514   \n",
       "4                                   0.997705                   0.004602   \n",
       "...                                      ...                        ...   \n",
       "73487                               0.997607                   0.002863   \n",
       "73488                               0.992283                   0.005927   \n",
       "73489                               0.973915                   0.025836   \n",
       "73490                               0.994456                   0.020409   \n",
       "73491                               0.999920                   0.000939   \n",
       "\n",
       "       TOTPOT_average_absolute_change  TOTBSQ_average_absolute_change  \\\n",
       "0                            0.000074                        0.026092   \n",
       "1                            0.000017                        0.005966   \n",
       "2                            0.000032                        0.009646   \n",
       "3                            0.000147                        0.028324   \n",
       "4                            0.000045                        0.008830   \n",
       "...                               ...                             ...   \n",
       "73487                        0.000015                        0.005738   \n",
       "73488                        0.000049                        0.012674   \n",
       "73489                        0.000237                        0.049894   \n",
       "73490                        0.000103                        0.035119   \n",
       "73491                        0.000004                        0.002307   \n",
       "\n",
       "       MEANPOT_difference_of_medians  USFLUX_slope_of_longest_mono_increase  \\\n",
       "0                           0.000041                               0.028674   \n",
       "1                           0.000035                               0.003590   \n",
       "2                           0.000003                               0.007625   \n",
       "3                           0.000033                               0.016026   \n",
       "4                           0.000005                               0.007592   \n",
       "...                              ...                                    ...   \n",
       "73487                       0.000006                               0.010243   \n",
       "73488                       0.000217                               0.005476   \n",
       "73489                       0.000085                               0.019388   \n",
       "73490                       0.000019                               0.024067   \n",
       "73491                       0.000018                               0.001562   \n",
       "\n",
       "       TOTBSQ_avg_mono_increase_slope  SAVNCPP_max  \n",
       "0                            0.021381     0.073789  \n",
       "1                            0.005345     0.009592  \n",
       "2                            0.007204     0.017374  \n",
       "3                            0.021110     0.037189  \n",
       "4                            0.006826     0.018851  \n",
       "...                               ...          ...  \n",
       "73487                        0.005423     0.005171  \n",
       "73488                        0.009679     0.014201  \n",
       "73489                        0.022464     0.057426  \n",
       "73490                        0.020761     0.043784  \n",
       "73491                        0.001973     0.002329  \n",
       "\n",
       "[73492 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "lasso_labs = abt_cpy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso_feats = abt_cpy.drop(['lab'], axis=1)\n",
    "lasso_df_feats = SelectFromModel(max_features = numFeat, estimator = LassoLars( alpha = 0, eps =1)).fit(lasso_feats, lasso_labs)\n",
    "lasso_train = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_train_set = pd.concat([abt['lab'], lasso_train], axis = 1, join='inner')\n",
    "lasso_train_set\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_dderivative_mean</th>\n",
       "      <th>TOTUSJZ_gderivative_mean</th>\n",
       "      <th>SAVNCPP_gderivative_mean</th>\n",
       "      <th>ABSNJZH_kurtosis</th>\n",
       "      <th>TOTUSJH_gderivative_mean</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANSHR_slope_of_longest_mono_decrease</th>\n",
       "      <th>...</th>\n",
       "      <th>SHRGT45_slope_of_longest_mono_decrease</th>\n",
       "      <th>EPSX_slope_of_longest_mono_decrease</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTBSQ_gderivative_stddev</th>\n",
       "      <th>TOTPOT_average_absolute_change</th>\n",
       "      <th>TOTBSQ_average_absolute_change</th>\n",
       "      <th>MEANPOT_difference_of_medians</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_increase</th>\n",
       "      <th>TOTBSQ_avg_mono_increase_slope</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.979145</td>\n",
       "      <td>0.979265</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.683983</td>\n",
       "      <td>0.166475</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.953375</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.073789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.979309</td>\n",
       "      <td>0.979332</td>\n",
       "      <td>0.977080</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.682688</td>\n",
       "      <td>0.122735</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.998197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998806</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.979237</td>\n",
       "      <td>0.979280</td>\n",
       "      <td>0.976580</td>\n",
       "      <td>0.047912</td>\n",
       "      <td>0.680145</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.996353</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.017374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.979829</td>\n",
       "      <td>0.979674</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.183616</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.979351</td>\n",
       "      <td>0.976705</td>\n",
       "      <td>0.047934</td>\n",
       "      <td>0.684250</td>\n",
       "      <td>0.103106</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.997813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73487</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>0.979341</td>\n",
       "      <td>0.976041</td>\n",
       "      <td>0.052463</td>\n",
       "      <td>0.684865</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.997778</td>\n",
       "      <td>0.997607</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73488</th>\n",
       "      <td>C</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979596</td>\n",
       "      <td>0.979535</td>\n",
       "      <td>0.975673</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.696048</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.996946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997911</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.992283</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.012674</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>0.014201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73489</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.979785</td>\n",
       "      <td>0.979718</td>\n",
       "      <td>0.979449</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>0.695232</td>\n",
       "      <td>0.214363</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.973915</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.049894</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.019388</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.057426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73490</th>\n",
       "      <td>C</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.978214</td>\n",
       "      <td>0.978635</td>\n",
       "      <td>0.978052</td>\n",
       "      <td>0.048476</td>\n",
       "      <td>0.659593</td>\n",
       "      <td>0.184096</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999053</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.994456</td>\n",
       "      <td>0.020409</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.035119</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.043784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73491</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.979393</td>\n",
       "      <td>0.975928</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.687184</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.993584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.992271</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73492 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTBSQ_slope_of_longest_mono_decrease  TOTUSJZ_dderivative_mean  \\\n",
       "0      NF                               0.999113                  0.979145   \n",
       "1      NF                               0.999991                  0.979309   \n",
       "2      NF                               0.999990                  0.979237   \n",
       "3      NF                               0.999751                  0.979829   \n",
       "4      NF                               0.999948                  0.979352   \n",
       "...    ..                                    ...                       ...   \n",
       "73487  NF                               0.999872                  0.979317   \n",
       "73488   C                               1.000000                  0.979596   \n",
       "73489  NF                               0.999981                  0.979785   \n",
       "73490   C                               0.999581                  0.978214   \n",
       "73491  NF                               0.999994                  0.979405   \n",
       "\n",
       "       TOTUSJZ_gderivative_mean  SAVNCPP_gderivative_mean  ABSNJZH_kurtosis  \\\n",
       "0                      0.979265                  0.975866          0.046063   \n",
       "1                      0.979332                  0.977080          0.030247   \n",
       "2                      0.979280                  0.976580          0.047912   \n",
       "3                      0.979674                  0.975500          0.038173   \n",
       "4                      0.979351                  0.976705          0.047934   \n",
       "...                         ...                       ...               ...   \n",
       "73487                  0.979341                  0.976041          0.052463   \n",
       "73488                  0.979535                  0.975673          0.039513   \n",
       "73489                  0.979718                  0.979449          0.063704   \n",
       "73490                  0.978635                  0.978052          0.048476   \n",
       "73491                  0.979393                  0.975928          0.063802   \n",
       "\n",
       "       TOTUSJH_gderivative_mean  ABSNJZH_stddev  \\\n",
       "0                      0.683983        0.166475   \n",
       "1                      0.682688        0.122735   \n",
       "2                      0.680145        0.105429   \n",
       "3                      0.694549        0.183616   \n",
       "4                      0.684250        0.103106   \n",
       "...                         ...             ...   \n",
       "73487                  0.684865        0.057351   \n",
       "73488                  0.696048        0.136841   \n",
       "73489                  0.695232        0.214363   \n",
       "73490                  0.659593        0.184096   \n",
       "73491                  0.687184        0.044799   \n",
       "\n",
       "       MEANPOT_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.999915   \n",
       "1                                    0.999982   \n",
       "2                                    0.999988   \n",
       "3                                    0.999948   \n",
       "4                                    0.999954   \n",
       "...                                       ...   \n",
       "73487                                0.999655   \n",
       "73488                                0.999709   \n",
       "73489                                0.999804   \n",
       "73490                                0.999951   \n",
       "73491                                0.999985   \n",
       "\n",
       "       MEANSHR_slope_of_longest_mono_decrease  ...  \\\n",
       "0                                    0.998414  ...   \n",
       "1                                    0.998197  ...   \n",
       "2                                    0.999814  ...   \n",
       "3                                    0.999569  ...   \n",
       "4                                    0.997813  ...   \n",
       "...                                       ...  ...   \n",
       "73487                                0.999328  ...   \n",
       "73488                                0.996946  ...   \n",
       "73489                                0.999597  ...   \n",
       "73490                                0.999761  ...   \n",
       "73491                                0.993584  ...   \n",
       "\n",
       "       SHRGT45_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.998884   \n",
       "1                                    0.998806   \n",
       "2                                    0.999086   \n",
       "3                                    0.999646   \n",
       "4                                    0.999126   \n",
       "...                                       ...   \n",
       "73487                                0.998228   \n",
       "73488                                0.997911   \n",
       "73489                                0.999917   \n",
       "73490                                0.999053   \n",
       "73491                                0.999607   \n",
       "\n",
       "       EPSX_slope_of_longest_mono_decrease  \\\n",
       "0                                 0.999664   \n",
       "1                                 0.998982   \n",
       "2                                 0.996353   \n",
       "3                                 0.999499   \n",
       "4                                 0.999903   \n",
       "...                                    ...   \n",
       "73487                             0.997778   \n",
       "73488                             0.999393   \n",
       "73489                             0.999618   \n",
       "73490                             0.999981   \n",
       "73491                             0.992271   \n",
       "\n",
       "       USFLUX_slope_of_longest_mono_decrease  TOTBSQ_gderivative_stddev  \\\n",
       "0                                   0.953375                   0.015288   \n",
       "1                                   0.999580                   0.002713   \n",
       "2                                   0.998553                   0.004391   \n",
       "3                                   0.996641                   0.014514   \n",
       "4                                   0.997705                   0.004602   \n",
       "...                                      ...                        ...   \n",
       "73487                               0.997607                   0.002863   \n",
       "73488                               0.992283                   0.005927   \n",
       "73489                               0.973915                   0.025836   \n",
       "73490                               0.994456                   0.020409   \n",
       "73491                               0.999920                   0.000939   \n",
       "\n",
       "       TOTPOT_average_absolute_change  TOTBSQ_average_absolute_change  \\\n",
       "0                            0.000074                        0.026092   \n",
       "1                            0.000017                        0.005966   \n",
       "2                            0.000032                        0.009646   \n",
       "3                            0.000147                        0.028324   \n",
       "4                            0.000045                        0.008830   \n",
       "...                               ...                             ...   \n",
       "73487                        0.000015                        0.005738   \n",
       "73488                        0.000049                        0.012674   \n",
       "73489                        0.000237                        0.049894   \n",
       "73490                        0.000103                        0.035119   \n",
       "73491                        0.000004                        0.002307   \n",
       "\n",
       "       MEANPOT_difference_of_medians  USFLUX_slope_of_longest_mono_increase  \\\n",
       "0                           0.000041                               0.028674   \n",
       "1                           0.000035                               0.003590   \n",
       "2                           0.000003                               0.007625   \n",
       "3                           0.000033                               0.016026   \n",
       "4                           0.000005                               0.007592   \n",
       "...                              ...                                    ...   \n",
       "73487                       0.000006                               0.010243   \n",
       "73488                       0.000217                               0.005476   \n",
       "73489                       0.000085                               0.019388   \n",
       "73490                       0.000019                               0.024067   \n",
       "73491                       0.000018                               0.001562   \n",
       "\n",
       "       TOTBSQ_avg_mono_increase_slope  SAVNCPP_max  \n",
       "0                            0.021381     0.073789  \n",
       "1                            0.005345     0.009592  \n",
       "2                            0.007204     0.017374  \n",
       "3                            0.021110     0.037189  \n",
       "4                            0.006826     0.018851  \n",
       "...                               ...          ...  \n",
       "73487                        0.005423     0.005171  \n",
       "73488                        0.009679     0.014201  \n",
       "73489                        0.022464     0.057426  \n",
       "73490                        0.020761     0.043784  \n",
       "73491                        0.001973     0.002329  \n",
       "\n",
       "[73492 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso2_labs = abt2_cpy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso2_feats = abt2_cpy.drop(['lab'], axis =1)\n",
    "lasso_df_test_feats = lasso2_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test_set = pd.concat([abt2['lab'], lasso_test], axis = 1, join = 'inner')\n",
    "lasso_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2 (5 points)\n",
    "\n",
    "In this question, you will again perform the feature selection task [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). However, this time, you will be utililizing a random forest model called [ExtraTressClssifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) as the `estimator`. \n",
    "\n",
    "You need set the `max_features` of the SelectFromModel object to the number of features we are going to select (20 features). \n",
    "\n",
    "You also need to set the `n_estimators` of the random forest algorithm to `75` when you construct it.\n",
    "\n",
    "**Note:** This method allows you to utilize our string class labels, so you don't need to map the lables to any other values. You can use the labels that were used in the original example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_quadratic_weighted_average</th>\n",
       "      <th>TOTUSJZ_linear_weighted_average</th>\n",
       "      <th>TOTUSJZ_quadratic_weighted_average</th>\n",
       "      <th>SAVNCPP_mean</th>\n",
       "      <th>SAVNCPP_var</th>\n",
       "      <th>SAVNCPP_linear_weighted_average</th>\n",
       "      <th>SAVNCPP_quadratic_weighted_average</th>\n",
       "      <th>USFLUX_last_value</th>\n",
       "      <th>R_VALUE_min</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTUSJH_max</th>\n",
       "      <th>TOTUSJH_average_absolute_derivative_change</th>\n",
       "      <th>TOTUSJH_last_value</th>\n",
       "      <th>ABSNJZH_gderivative_stddev</th>\n",
       "      <th>ABSNJZH_last_value</th>\n",
       "      <th>ABSNJZH_slope_of_longest_mono_increase</th>\n",
       "      <th>ABSNJZH_avg_mono_increase_slope</th>\n",
       "      <th>SHRGT45_last_value</th>\n",
       "      <th>id</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.880515</td>\n",
       "      <td>0.930299</td>\n",
       "      <td>0.930350</td>\n",
       "      <td>0.925501</td>\n",
       "      <td>0.918802</td>\n",
       "      <td>0.925125</td>\n",
       "      <td>0.925156</td>\n",
       "      <td>0.951962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>0.233363</td>\n",
       "      <td>0.271143</td>\n",
       "      <td>0.166540</td>\n",
       "      <td>0.113146</td>\n",
       "      <td>0.153355</td>\n",
       "      <td>0.147190</td>\n",
       "      <td>0.292429</td>\n",
       "      <td>0.294529</td>\n",
       "      <td>0.073789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.829410</td>\n",
       "      <td>0.886908</td>\n",
       "      <td>0.886192</td>\n",
       "      <td>0.854969</td>\n",
       "      <td>0.878886</td>\n",
       "      <td>0.853792</td>\n",
       "      <td>0.854869</td>\n",
       "      <td>0.923537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.116892</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.101681</td>\n",
       "      <td>0.247061</td>\n",
       "      <td>0.398784</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.894595</td>\n",
       "      <td>0.882857</td>\n",
       "      <td>0.887249</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.883033</td>\n",
       "      <td>0.927167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>0.176855</td>\n",
       "      <td>0.120471</td>\n",
       "      <td>0.118634</td>\n",
       "      <td>0.050708</td>\n",
       "      <td>0.106157</td>\n",
       "      <td>0.108718</td>\n",
       "      <td>0.205895</td>\n",
       "      <td>0.280851</td>\n",
       "      <td>0.017374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.925004</td>\n",
       "      <td>0.941832</td>\n",
       "      <td>0.941771</td>\n",
       "      <td>0.899735</td>\n",
       "      <td>0.917619</td>\n",
       "      <td>0.897710</td>\n",
       "      <td>0.897125</td>\n",
       "      <td>0.966551</td>\n",
       "      <td>0.708787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328616</td>\n",
       "      <td>0.228363</td>\n",
       "      <td>0.320618</td>\n",
       "      <td>0.167205</td>\n",
       "      <td>0.136556</td>\n",
       "      <td>0.121103</td>\n",
       "      <td>0.147689</td>\n",
       "      <td>0.470788</td>\n",
       "      <td>0.330699</td>\n",
       "      <td>0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.865675</td>\n",
       "      <td>0.894999</td>\n",
       "      <td>0.894419</td>\n",
       "      <td>0.882598</td>\n",
       "      <td>0.886480</td>\n",
       "      <td>0.882251</td>\n",
       "      <td>0.881813</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140699</td>\n",
       "      <td>0.147472</td>\n",
       "      <td>0.124584</td>\n",
       "      <td>0.115037</td>\n",
       "      <td>0.095203</td>\n",
       "      <td>0.095102</td>\n",
       "      <td>0.104563</td>\n",
       "      <td>0.538216</td>\n",
       "      <td>0.305167</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73487</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.774293</td>\n",
       "      <td>0.851174</td>\n",
       "      <td>0.849670</td>\n",
       "      <td>0.835645</td>\n",
       "      <td>0.861763</td>\n",
       "      <td>0.838511</td>\n",
       "      <td>0.840411</td>\n",
       "      <td>0.893933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066916</td>\n",
       "      <td>0.132699</td>\n",
       "      <td>0.047050</td>\n",
       "      <td>0.070496</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.063659</td>\n",
       "      <td>0.068696</td>\n",
       "      <td>0.197497</td>\n",
       "      <td>0.055015</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73488</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.843262</td>\n",
       "      <td>0.888837</td>\n",
       "      <td>0.887663</td>\n",
       "      <td>0.863003</td>\n",
       "      <td>0.888197</td>\n",
       "      <td>0.859556</td>\n",
       "      <td>0.856926</td>\n",
       "      <td>0.921493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134423</td>\n",
       "      <td>0.165114</td>\n",
       "      <td>0.113201</td>\n",
       "      <td>0.133634</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.122110</td>\n",
       "      <td>0.512786</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.014201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73489</th>\n",
       "      <td>C</td>\n",
       "      <td>0.939821</td>\n",
       "      <td>0.956245</td>\n",
       "      <td>0.956056</td>\n",
       "      <td>0.910589</td>\n",
       "      <td>0.934832</td>\n",
       "      <td>0.912872</td>\n",
       "      <td>0.913686</td>\n",
       "      <td>0.971788</td>\n",
       "      <td>0.747176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429922</td>\n",
       "      <td>0.289465</td>\n",
       "      <td>0.416328</td>\n",
       "      <td>0.219139</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>0.185848</td>\n",
       "      <td>0.195310</td>\n",
       "      <td>0.519211</td>\n",
       "      <td>0.410030</td>\n",
       "      <td>0.057426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73490</th>\n",
       "      <td>B</td>\n",
       "      <td>0.926733</td>\n",
       "      <td>0.950542</td>\n",
       "      <td>0.949976</td>\n",
       "      <td>0.904882</td>\n",
       "      <td>0.916292</td>\n",
       "      <td>0.904619</td>\n",
       "      <td>0.903956</td>\n",
       "      <td>0.972461</td>\n",
       "      <td>0.673997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395401</td>\n",
       "      <td>0.250973</td>\n",
       "      <td>0.366209</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>0.071047</td>\n",
       "      <td>0.167420</td>\n",
       "      <td>0.204119</td>\n",
       "      <td>0.228974</td>\n",
       "      <td>0.145593</td>\n",
       "      <td>0.043784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73491</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.743557</td>\n",
       "      <td>0.823504</td>\n",
       "      <td>0.823789</td>\n",
       "      <td>0.811892</td>\n",
       "      <td>0.839582</td>\n",
       "      <td>0.808945</td>\n",
       "      <td>0.807987</td>\n",
       "      <td>0.881571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.049404</td>\n",
       "      <td>0.056536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434650</td>\n",
       "      <td>0.002329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73492 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTBSQ_quadratic_weighted_average  TOTUSJZ_linear_weighted_average  \\\n",
       "0      NF                           0.880515                         0.930299   \n",
       "1      NF                           0.829410                         0.886908   \n",
       "2      NF                           0.834038                         0.894928   \n",
       "3      NF                           0.925004                         0.941832   \n",
       "4      NF                           0.865675                         0.894999   \n",
       "...    ..                                ...                              ...   \n",
       "73487  NF                           0.774293                         0.851174   \n",
       "73488  NF                           0.843262                         0.888837   \n",
       "73489   C                           0.939821                         0.956245   \n",
       "73490   B                           0.926733                         0.950542   \n",
       "73491  NF                           0.743557                         0.823504   \n",
       "\n",
       "       TOTUSJZ_quadratic_weighted_average  SAVNCPP_mean  SAVNCPP_var  \\\n",
       "0                                0.930350      0.925501     0.918802   \n",
       "1                                0.886192      0.854969     0.878886   \n",
       "2                                0.894595      0.882857     0.887249   \n",
       "3                                0.941771      0.899735     0.917619   \n",
       "4                                0.894419      0.882598     0.886480   \n",
       "...                                   ...           ...          ...   \n",
       "73487                            0.849670      0.835645     0.861763   \n",
       "73488                            0.887663      0.863003     0.888197   \n",
       "73489                            0.956056      0.910589     0.934832   \n",
       "73490                            0.949976      0.904882     0.916292   \n",
       "73491                            0.823789      0.811892     0.839582   \n",
       "\n",
       "       SAVNCPP_linear_weighted_average  SAVNCPP_quadratic_weighted_average  \\\n",
       "0                             0.925125                            0.925156   \n",
       "1                             0.853792                            0.854869   \n",
       "2                             0.882917                            0.883033   \n",
       "3                             0.897710                            0.897125   \n",
       "4                             0.882251                            0.881813   \n",
       "...                                ...                                 ...   \n",
       "73487                         0.838511                            0.840411   \n",
       "73488                         0.859556                            0.856926   \n",
       "73489                         0.912872                            0.913686   \n",
       "73490                         0.904619                            0.903956   \n",
       "73491                         0.808945                            0.807987   \n",
       "\n",
       "       USFLUX_last_value  R_VALUE_min  ...  TOTUSJH_max  \\\n",
       "0               0.951962     0.000000  ...     0.275990   \n",
       "1               0.923537     0.000000  ...     0.123894   \n",
       "2               0.927167     0.000000  ...     0.141522   \n",
       "3               0.966551     0.708787  ...     0.328616   \n",
       "4               0.929630     0.000000  ...     0.140699   \n",
       "...                  ...          ...  ...          ...   \n",
       "73487           0.893933     0.000000  ...     0.066916   \n",
       "73488           0.921493     0.000000  ...     0.134423   \n",
       "73489           0.971788     0.747176  ...     0.429922   \n",
       "73490           0.972461     0.673997  ...     0.395401   \n",
       "73491           0.881571     0.000000  ...     0.041070   \n",
       "\n",
       "       TOTUSJH_average_absolute_derivative_change  TOTUSJH_last_value  \\\n",
       "0                                        0.233363            0.271143   \n",
       "1                                        0.116892            0.108247   \n",
       "2                                        0.176855            0.120471   \n",
       "3                                        0.228363            0.320618   \n",
       "4                                        0.147472            0.124584   \n",
       "...                                           ...                 ...   \n",
       "73487                                    0.132699            0.047050   \n",
       "73488                                    0.165114            0.113201   \n",
       "73489                                    0.289465            0.416328   \n",
       "73490                                    0.250973            0.366209   \n",
       "73491                                    0.083222            0.033708   \n",
       "\n",
       "       ABSNJZH_gderivative_stddev  ABSNJZH_last_value  \\\n",
       "0                        0.166540            0.113146   \n",
       "1                        0.108461            0.008834   \n",
       "2                        0.118634            0.050708   \n",
       "3                        0.167205            0.136556   \n",
       "4                        0.115037            0.095203   \n",
       "...                           ...                 ...   \n",
       "73487                    0.070496            0.004870   \n",
       "73488                    0.133634            0.058917   \n",
       "73489                    0.219139            0.031731   \n",
       "73490                    0.205729            0.071047   \n",
       "73491                    0.060583            0.003694   \n",
       "\n",
       "       ABSNJZH_slope_of_longest_mono_increase  \\\n",
       "0                                    0.153355   \n",
       "1                                    0.090031   \n",
       "2                                    0.106157   \n",
       "3                                    0.121103   \n",
       "4                                    0.095102   \n",
       "...                                       ...   \n",
       "73487                                0.063659   \n",
       "73488                                0.114678   \n",
       "73489                                0.185848   \n",
       "73490                                0.167420   \n",
       "73491                                0.049404   \n",
       "\n",
       "       ABSNJZH_avg_mono_increase_slope  SHRGT45_last_value        id  \\\n",
       "0                             0.147190            0.292429  0.294529   \n",
       "1                             0.101681            0.247061  0.398784   \n",
       "2                             0.108718            0.205895  0.280851   \n",
       "3                             0.147689            0.470788  0.330699   \n",
       "4                             0.104563            0.538216  0.305167   \n",
       "...                                ...                 ...       ...   \n",
       "73487                         0.068696            0.197497  0.055015   \n",
       "73488                         0.122110            0.512786  0.323100   \n",
       "73489                         0.195310            0.519211  0.410030   \n",
       "73490                         0.204119            0.228974  0.145593   \n",
       "73491                         0.056536            0.000000  0.434650   \n",
       "\n",
       "       SAVNCPP_max  \n",
       "0         0.073789  \n",
       "1         0.009592  \n",
       "2         0.017374  \n",
       "3         0.037189  \n",
       "4         0.018851  \n",
       "...            ...  \n",
       "73487     0.005171  \n",
       "73488     0.014201  \n",
       "73489     0.057426  \n",
       "73490     0.043784  \n",
       "73491     0.002329  \n",
       "\n",
       "[73492 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrees_labels = abt_cpy['lab']\n",
    "xtrees_features = abt_cpy.drop(['lab'], axis = 1)\n",
    "xtrees_model_features = SelectFromModel(max_features = numFeat, estimator = ExtraTreesClassifier(n_estimators = 75)).fit(xtrees_features, xtrees_labels)\n",
    "xtrees_selected_features = xtrees_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_train = pd.concat([xtrees_labels, xtrees_selected_features], axis = 1)\n",
    "xtrees_train\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_quadratic_weighted_average</th>\n",
       "      <th>TOTUSJZ_linear_weighted_average</th>\n",
       "      <th>TOTUSJZ_quadratic_weighted_average</th>\n",
       "      <th>SAVNCPP_mean</th>\n",
       "      <th>SAVNCPP_var</th>\n",
       "      <th>SAVNCPP_linear_weighted_average</th>\n",
       "      <th>SAVNCPP_quadratic_weighted_average</th>\n",
       "      <th>USFLUX_last_value</th>\n",
       "      <th>R_VALUE_min</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTUSJH_max</th>\n",
       "      <th>TOTUSJH_average_absolute_derivative_change</th>\n",
       "      <th>TOTUSJH_last_value</th>\n",
       "      <th>ABSNJZH_gderivative_stddev</th>\n",
       "      <th>ABSNJZH_last_value</th>\n",
       "      <th>ABSNJZH_slope_of_longest_mono_increase</th>\n",
       "      <th>ABSNJZH_avg_mono_increase_slope</th>\n",
       "      <th>SHRGT45_last_value</th>\n",
       "      <th>id</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.910168</td>\n",
       "      <td>0.932139</td>\n",
       "      <td>0.931387</td>\n",
       "      <td>0.920598</td>\n",
       "      <td>0.924507</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.923319</td>\n",
       "      <td>0.950452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264179</td>\n",
       "      <td>0.230203</td>\n",
       "      <td>0.253303</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.165156</td>\n",
       "      <td>0.175954</td>\n",
       "      <td>0.716087</td>\n",
       "      <td>0.578116</td>\n",
       "      <td>0.063282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.854401</td>\n",
       "      <td>0.906851</td>\n",
       "      <td>0.907144</td>\n",
       "      <td>0.852694</td>\n",
       "      <td>0.887692</td>\n",
       "      <td>0.850472</td>\n",
       "      <td>0.847213</td>\n",
       "      <td>0.938672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161652</td>\n",
       "      <td>0.154830</td>\n",
       "      <td>0.152454</td>\n",
       "      <td>0.118672</td>\n",
       "      <td>0.051744</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.121236</td>\n",
       "      <td>0.214311</td>\n",
       "      <td>0.736474</td>\n",
       "      <td>0.011076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.861191</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.906249</td>\n",
       "      <td>0.892299</td>\n",
       "      <td>0.911761</td>\n",
       "      <td>0.895781</td>\n",
       "      <td>0.897702</td>\n",
       "      <td>0.933693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174518</td>\n",
       "      <td>0.215340</td>\n",
       "      <td>0.163801</td>\n",
       "      <td>0.163447</td>\n",
       "      <td>0.116043</td>\n",
       "      <td>0.140554</td>\n",
       "      <td>0.163217</td>\n",
       "      <td>0.304359</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.035432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.909206</td>\n",
       "      <td>0.932530</td>\n",
       "      <td>0.931970</td>\n",
       "      <td>0.903374</td>\n",
       "      <td>0.918006</td>\n",
       "      <td>0.904310</td>\n",
       "      <td>0.905689</td>\n",
       "      <td>0.958722</td>\n",
       "      <td>0.705388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292981</td>\n",
       "      <td>0.253225</td>\n",
       "      <td>0.260010</td>\n",
       "      <td>0.161562</td>\n",
       "      <td>0.141853</td>\n",
       "      <td>0.132879</td>\n",
       "      <td>0.138083</td>\n",
       "      <td>0.400342</td>\n",
       "      <td>0.858359</td>\n",
       "      <td>0.038205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.838926</td>\n",
       "      <td>0.877464</td>\n",
       "      <td>0.877113</td>\n",
       "      <td>0.851310</td>\n",
       "      <td>0.875186</td>\n",
       "      <td>0.850989</td>\n",
       "      <td>0.850975</td>\n",
       "      <td>0.920081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104345</td>\n",
       "      <td>0.152074</td>\n",
       "      <td>0.094275</td>\n",
       "      <td>0.092659</td>\n",
       "      <td>0.053446</td>\n",
       "      <td>0.088042</td>\n",
       "      <td>0.089760</td>\n",
       "      <td>0.305536</td>\n",
       "      <td>0.589970</td>\n",
       "      <td>0.008969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88552</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.863351</td>\n",
       "      <td>0.914217</td>\n",
       "      <td>0.913351</td>\n",
       "      <td>0.895049</td>\n",
       "      <td>0.921903</td>\n",
       "      <td>0.899760</td>\n",
       "      <td>0.901948</td>\n",
       "      <td>0.940105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205758</td>\n",
       "      <td>0.181885</td>\n",
       "      <td>0.180158</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>0.064933</td>\n",
       "      <td>0.112496</td>\n",
       "      <td>0.138010</td>\n",
       "      <td>0.254829</td>\n",
       "      <td>0.941337</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88553</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.785622</td>\n",
       "      <td>0.849331</td>\n",
       "      <td>0.849530</td>\n",
       "      <td>0.822867</td>\n",
       "      <td>0.855199</td>\n",
       "      <td>0.821317</td>\n",
       "      <td>0.819851</td>\n",
       "      <td>0.903115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058479</td>\n",
       "      <td>0.093526</td>\n",
       "      <td>0.057382</td>\n",
       "      <td>0.077821</td>\n",
       "      <td>0.013280</td>\n",
       "      <td>0.055907</td>\n",
       "      <td>0.073404</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.658663</td>\n",
       "      <td>0.005150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88554</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.775761</td>\n",
       "      <td>0.846496</td>\n",
       "      <td>0.845727</td>\n",
       "      <td>0.846153</td>\n",
       "      <td>0.867941</td>\n",
       "      <td>0.849616</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.893574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.134978</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.076017</td>\n",
       "      <td>0.033815</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.134326</td>\n",
       "      <td>0.800304</td>\n",
       "      <td>0.006847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88555</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.831255</td>\n",
       "      <td>0.885174</td>\n",
       "      <td>0.884993</td>\n",
       "      <td>0.875727</td>\n",
       "      <td>0.892356</td>\n",
       "      <td>0.876233</td>\n",
       "      <td>0.875336</td>\n",
       "      <td>0.925681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115870</td>\n",
       "      <td>0.137341</td>\n",
       "      <td>0.105768</td>\n",
       "      <td>0.099209</td>\n",
       "      <td>0.070360</td>\n",
       "      <td>0.094470</td>\n",
       "      <td>0.097608</td>\n",
       "      <td>0.210722</td>\n",
       "      <td>0.881155</td>\n",
       "      <td>0.020035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88556</th>\n",
       "      <td>B</td>\n",
       "      <td>0.916336</td>\n",
       "      <td>0.948403</td>\n",
       "      <td>0.948583</td>\n",
       "      <td>0.888428</td>\n",
       "      <td>0.916207</td>\n",
       "      <td>0.886562</td>\n",
       "      <td>0.885521</td>\n",
       "      <td>0.964984</td>\n",
       "      <td>0.667372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365650</td>\n",
       "      <td>0.300972</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>0.188376</td>\n",
       "      <td>0.166899</td>\n",
       "      <td>0.147654</td>\n",
       "      <td>0.184904</td>\n",
       "      <td>0.478505</td>\n",
       "      <td>0.947720</td>\n",
       "      <td>0.033437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88557 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTBSQ_quadratic_weighted_average  TOTUSJZ_linear_weighted_average  \\\n",
       "0      NF                           0.910168                         0.932139   \n",
       "1      NF                           0.854401                         0.906851   \n",
       "2      NF                           0.861191                         0.906075   \n",
       "3      NF                           0.909206                         0.932530   \n",
       "4      NF                           0.838926                         0.877464   \n",
       "...    ..                                ...                              ...   \n",
       "88552  NF                           0.863351                         0.914217   \n",
       "88553  NF                           0.785622                         0.849331   \n",
       "88554  NF                           0.775761                         0.846496   \n",
       "88555  NF                           0.831255                         0.885174   \n",
       "88556   B                           0.916336                         0.948403   \n",
       "\n",
       "       TOTUSJZ_quadratic_weighted_average  SAVNCPP_mean  SAVNCPP_var  \\\n",
       "0                                0.931387      0.920598     0.924507   \n",
       "1                                0.907144      0.852694     0.887692   \n",
       "2                                0.906249      0.892299     0.911761   \n",
       "3                                0.931970      0.903374     0.918006   \n",
       "4                                0.877113      0.851310     0.875186   \n",
       "...                                   ...           ...          ...   \n",
       "88552                            0.913351      0.895049     0.921903   \n",
       "88553                            0.849530      0.822867     0.855199   \n",
       "88554                            0.845727      0.846153     0.867941   \n",
       "88555                            0.884993      0.875727     0.892356   \n",
       "88556                            0.948583      0.888428     0.916207   \n",
       "\n",
       "       SAVNCPP_linear_weighted_average  SAVNCPP_quadratic_weighted_average  \\\n",
       "0                             0.922513                            0.923319   \n",
       "1                             0.850472                            0.847213   \n",
       "2                             0.895781                            0.897702   \n",
       "3                             0.904310                            0.905689   \n",
       "4                             0.850989                            0.850975   \n",
       "...                                ...                                 ...   \n",
       "88552                         0.899760                            0.901948   \n",
       "88553                         0.821317                            0.819851   \n",
       "88554                         0.849616                            0.852041   \n",
       "88555                         0.876233                            0.875336   \n",
       "88556                         0.886562                            0.885521   \n",
       "\n",
       "       USFLUX_last_value  R_VALUE_min  ...  TOTUSJH_max  \\\n",
       "0               0.950452     0.000000  ...     0.264179   \n",
       "1               0.938672     0.000000  ...     0.161652   \n",
       "2               0.933693     0.000000  ...     0.174518   \n",
       "3               0.958722     0.705388  ...     0.292981   \n",
       "4               0.920081     0.000000  ...     0.104345   \n",
       "...                  ...          ...  ...          ...   \n",
       "88552           0.940105     0.000000  ...     0.205758   \n",
       "88553           0.903115     0.000000  ...     0.058479   \n",
       "88554           0.893574     0.000000  ...     0.062495   \n",
       "88555           0.925681     0.000000  ...     0.115870   \n",
       "88556           0.964984     0.667372  ...     0.365650   \n",
       "\n",
       "       TOTUSJH_average_absolute_derivative_change  TOTUSJH_last_value  \\\n",
       "0                                        0.230203            0.253303   \n",
       "1                                        0.154830            0.152454   \n",
       "2                                        0.215340            0.163801   \n",
       "3                                        0.253225            0.260010   \n",
       "4                                        0.152074            0.094275   \n",
       "...                                           ...                 ...   \n",
       "88552                                    0.181885            0.180158   \n",
       "88553                                    0.093526            0.057382   \n",
       "88554                                    0.134978            0.053345   \n",
       "88555                                    0.137341            0.105768   \n",
       "88556                                    0.300972            0.366120   \n",
       "\n",
       "       ABSNJZH_gderivative_stddev  ABSNJZH_last_value  \\\n",
       "0                        0.169989            0.235000   \n",
       "1                        0.118672            0.051744   \n",
       "2                        0.163447            0.116043   \n",
       "3                        0.161562            0.141853   \n",
       "4                        0.092659            0.053446   \n",
       "...                           ...                 ...   \n",
       "88552                    0.148995            0.064933   \n",
       "88553                    0.077821            0.013280   \n",
       "88554                    0.076017            0.033815   \n",
       "88555                    0.099209            0.070360   \n",
       "88556                    0.188376            0.166899   \n",
       "\n",
       "       ABSNJZH_slope_of_longest_mono_increase  \\\n",
       "0                                    0.165156   \n",
       "1                                    0.092857   \n",
       "2                                    0.140554   \n",
       "3                                    0.132879   \n",
       "4                                    0.088042   \n",
       "...                                       ...   \n",
       "88552                                0.112496   \n",
       "88553                                0.055907   \n",
       "88554                                0.063175   \n",
       "88555                                0.094470   \n",
       "88556                                0.147654   \n",
       "\n",
       "       ABSNJZH_avg_mono_increase_slope  SHRGT45_last_value        id  \\\n",
       "0                             0.175954            0.716087  0.578116   \n",
       "1                             0.121236            0.214311  0.736474   \n",
       "2                             0.163217            0.304359  0.942857   \n",
       "3                             0.138083            0.400342  0.858359   \n",
       "4                             0.089760            0.305536  0.589970   \n",
       "...                                ...                 ...       ...   \n",
       "88552                         0.138010            0.254829  0.941337   \n",
       "88553                         0.073404            0.140722  0.658663   \n",
       "88554                         0.079521            0.134326  0.800304   \n",
       "88555                         0.097608            0.210722  0.881155   \n",
       "88556                         0.184904            0.478505  0.947720   \n",
       "\n",
       "       SAVNCPP_max  \n",
       "0         0.063282  \n",
       "1         0.011076  \n",
       "2         0.035432  \n",
       "3         0.038205  \n",
       "4         0.008969  \n",
       "...            ...  \n",
       "88552     0.034200  \n",
       "88553     0.005150  \n",
       "88554     0.006847  \n",
       "88555     0.020035  \n",
       "88556     0.033437  \n",
       "\n",
       "[88557 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrees_test_labels = abt2_cpy['lab']\n",
    "xtrees_test_features = abt2_cpy.drop(['lab'], axis = 1)\n",
    "xtrees_test_selected = xtrees_test_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_test = pd.concat([xtrees_test_labels, xtrees_test_selected], axis = 1)\n",
    "xtrees_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3 (5 points)\n",
    "\n",
    "Now that you have three different datasets, you need to convert them each to a binary classification problem datase or dichotomize the training and testing data. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed with the feature selected training and testing data from the exmpale, Q1, and Q2.\n",
    "\n",
    "**Note:** You might want to put the training and testing tuples you get from the call to the dichotomize method into seperate training and testing lists. Then you can loop over them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "train = [df_train_set1.copy(), lasso_train_set.copy(), xtrees_train.copy()]\n",
    "test = [df_test_set1.copy(), lasso_test_set.copy(), xtrees_test.copy()]\n",
    "di_train_x1 = []\n",
    "di_train_y1 = []\n",
    "di_test_x2 = []\n",
    "di_test_y2 = []\n",
    "\n",
    "for set1 in train:\n",
    "    x1, y1 = dichotomize_X_y(set1)\n",
    "    di_train_x1.append(x1)\n",
    "    di_train_y1.append(y1)\n",
    "\n",
    "for set2 in test:\n",
    "    x2, y2 = dichotomize_X_y(set2)\n",
    "    di_test_x2.append(x2)\n",
    "    di_test_y2.append(y2)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4 (10 points)\n",
    "\n",
    "Now that you have your data setup, you will be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. For this exercise default regularization parameter `C` value of 1.0, the default `kernel` type of `rbf`, and the default setting of the kernel coefficient `gamma` for the `rbf` kernel. You should, however, set the `class_weight` to `balanced` when you construct your models. This way the regularization parameter is adjusted for each class in proportion the occurrence of that class in the dataset.\n",
    "\n",
    "You should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out.\n",
    "\n",
    "**Note:** for more information on what the `C` and `gamma` parameters do on the `rbf` kernel see the [RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) documentation. We won't be tuning these values in this question, but it is genearally accepted that tuning should be done to find the best performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['F-Val', 'FromLasso', 'FromForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=76129\tFP=11027\tFN=76\tTP=1325\n",
      "F-Val\n",
      "TSS: 0.8192327710296818\n",
      "HSS: 0.1690723900997794\n",
      "TN=63672\tFP=8659\tFN=1009\tTP=152\n",
      "FromLasso\n",
      "TSS: 0.011208080147288246\n",
      "HSS: 0.0026423608313362124\n",
      "TN=84801\tFP=2355\tFN=672\tTP=729\n",
      "FromForest\n",
      "TSS: 0.4933220974809697\n",
      "HSS: 0.3100729329414291\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "for ind in range(len(selected_labels)):\n",
    "     classifier = SVC(class_weight = 'balanced')\n",
    "     classifier.fit(di_train_x1[ind],di_train_y1[ind])\n",
    "     y_pred = classifier.predict(di_test_x2[ind])\n",
    "     \n",
    "     tss_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "     hss_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "\n",
    "     print(selected_labels[ind])\n",
    "     print(f\"TSS: {tss_score}\")\n",
    "     print(f\"HSS: {hss_score}\")\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5 (15 points)\n",
    "\n",
    "After training and testing the SVC model on the above dataset, you will likely see that this process is quite time consuming. This is because the algorithm needs evaluate all the instances in the training dataset to find instances that can be used as points in a separating hyperplane between the samples of different classes.  \n",
    "\n",
    "In order to speed this process, lets reduce the number of samples in the dataset through undersampling the classes like we did before. Unlike was done before, where we just pick some random sample of instances in the various classes, we will be performing some data informed under sampling using the [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) clustering algorithm.\n",
    "\n",
    "So, what I want you to do is construct a function (I have started it below) that performs the following:\n",
    "\n",
    "* Groups the data by the `lab` column and finds the size of the smallest group.\n",
    "\n",
    "* For every group of samples in the dataset that is not the smallest group, you will use the KMeans algorithm to cluster the samples into `K` clusters. The size of `K` is the size of the smallest group.  **Note:** You need to make sure you are only using the descriptive features when doing this, so drop the label column from each group.\n",
    "\n",
    "* Once you have the `K` clusters, you will get the cluster centers by using the `cluster_centers_` attribute of your Kmeans object. This attribute is a `(n_clusters, n_features)` array of values. These will be the new set of samples of descriptive features for the class you are processing. You should construct a DataFrame with these and add a label column with your class label for each one of these samples.\n",
    "\n",
    "* The samples for the smalles class group from the original dataset will be the samples you return for that class. \n",
    "\n",
    "* You will need to concatenate all of the results into one DataFrame and return it at the end of the function.\n",
    "\n",
    "Once you have completed the function, you need to apply it to each of your three training sets (the ones that have not had the dicotimize process applied, I hope you kept a copy). Then you will apply the dicotomize process to the sampled training sets and place them into a list for use in the next problem.\n",
    "\n",
    "**Note:** By training our models on representations of the real data instead of the acutal measurements, we are building a type of surrogate model. By doing so, we can approximate how our model might behave when trained with the true data, but can test several different settings much faster than what we otherwise would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_under_sample_clust(data:DataFrame)->DataFrame:\n",
    "    #----------------------------------------------\n",
    "    temp = data.copy()\n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    freq = pd.DataFrame(temp['lab'].value_counts())\n",
    "    indices = freq.index.tolist()\n",
    "    min_val_ind = freq.idxmin()[0]\n",
    "    indices.remove(min_val_ind)\n",
    "    min_v = freq['lab'][min_val_ind]\n",
    "\n",
    "    desc_feats = temp.columns[1:]\n",
    "\n",
    "    res = pd.concat([res, temp[temp['lab'] == min_val_ind]])\n",
    "\n",
    "    for ind in indices:\n",
    "        classifier = KMeans(n_clusters=min_v)\n",
    "        classifier.fit(temp[temp['lab'] == ind].drop(['lab'], axis = 1))\n",
    "        temp2 = pd.DataFrame(classifier.cluster_centers_, columns=desc_feats)\n",
    "        temp2['lab'] = ind\n",
    "        res = pd.concat([res, temp2])\n",
    "    \n",
    "    res.reset_index(inplace=True, drop=True)\n",
    "    return res\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "for frame in train:\n",
    "     frame = perform_under_sample_clust(frame)\n",
    "\n",
    "us_di_train_x1 = []\n",
    "us_di_train_y1 = []\n",
    "\n",
    "for frame in train:\n",
    "     x1, y1 = dichotomize_X_y(frame)\n",
    "     us_di_train_x1.append(x1)\n",
    "     us_di_train_y1.append(y1)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6 (10 points)\n",
    "\n",
    "In question 5 we produced datasets that approxumate what our real data looks like. By training our models on these representations of the real data instead of the acutal measurements, we are building a type of surrogate model. In doing so, we can approximate how our model might behave when trained with the true data, but we obtain a major advantage in that we can test several different settings much faster than what we otherwise would if using the true dataset. We can then use these surrogate results to find a range of the hyperparameters that we might wish to investigate using the true input data.\n",
    "\n",
    "For this question, you will again train your models on the three different feature selected data. However, instead of the full partition 1 training datasets, you will be using the sampling with KMeans training datasets you constructed in Q5. \n",
    "\n",
    "You will again be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. Like before, you should set the `class_weight` to `balanced` when you construct your models. Unlike the previous question where you traind the models, this time you will be asked to evaluate several different settings for the `kernel`, the regularization parameter `C`, and kernel coefficient `gamma`. **Note:** The `gamma` paramter is only utilized on the ‘rbf’, ‘poly’ and ‘sigmoid’ kernels, so there is no reason to evaluate multiple settings for the `linear` kernel. I have listed the settings of each parameter in a code block below. \n",
    "\n",
    "For each of the settings, you should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['F-Val', 'FromLasso', 'FromForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['linear', 'poly', 'rbf']\n",
    "c_vals = [ 0.5, 1.0]\n",
    "gamma_vals = [0.5, 1, 10]\n",
    "temp = [kernel, c_vals, gamma_vals]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Val *-*-**-*-**-*-**-*-**-*-*\n",
      "TN=77673\tFP=9483\tFN=88\tTP=1313\n",
      "TSS: 0.8283828214991299\n",
      "HSS: 0.19268908613638142\n",
      "TN=77673\tFP=9483\tFN=88\tTP=1313\n",
      "TSS: 0.8283828214991299\n",
      "HSS: 0.19268908613638142\n",
      "TN=77673\tFP=9483\tFN=88\tTP=1313\n",
      "TSS: 0.8283828214991299\n",
      "HSS: 0.19268908613638142\n",
      "TN=77696\tFP=9460\tFN=86\tTP=1315\n",
      "TSS: 0.8300742678736093\n",
      "HSS: 0.19341429433144303\n",
      "TN=77696\tFP=9460\tFN=86\tTP=1315\n",
      "TSS: 0.8300742678736093\n",
      "HSS: 0.19341429433144303\n",
      "TN=77696\tFP=9460\tFN=86\tTP=1315\n",
      "TSS: 0.8300742678736093\n",
      "HSS: 0.19341429433144303\n",
      "TN=76996\tFP=10160\tFN=123\tTP=1278\n",
      "TSS: 0.7956329849560654\n",
      "HSS: 0.17584978626330627\n",
      "TN=76373\tFP=10783\tFN=83\tTP=1318\n",
      "TSS: 0.8170359176776526\n",
      "HSS: 0.17174301434535463\n",
      "TN=79324\tFP=7832\tFN=291\tTP=1110\n",
      "TSS: 0.7024293636564743\n",
      "HSS: 0.1925503029266028\n",
      "TN=76868\tFP=10288\tFN=110\tTP=1291\n",
      "TSS: 0.8034434403623698\n",
      "HSS: 0.17565383748955804\n",
      "TN=76352\tFP=10804\tFN=81\tTP=1320\n",
      "TSS: 0.8182225221594339\n",
      "HSS: 0.17170094568229738\n",
      "TN=79643\tFP=7513\tFN=306\tTP=1095\n",
      "TSS: 0.6953828292629043\n",
      "HSS: 0.19695068328515927\n",
      "TN=75633\tFP=11523\tFN=68\tTP=1333\n",
      "TSS: 0.8192520330524518\n",
      "HSS: 0.16311846723656773\n",
      "TN=75925\tFP=11231\tFN=72\tTP=1329\n",
      "TSS: 0.819747243933765\n",
      "HSS: 0.16666627246149338\n",
      "TN=79921\tFP=7235\tFN=464\tTP=937\n",
      "TSS: 0.5857959239790858\n",
      "HSS: 0.17343328956617524\n",
      "TN=75908\tFP=11248\tFN=73\tTP=1328\n",
      "TSS: 0.818838415509938\n",
      "HSS: 0.16629153919626086\n",
      "TN=76141\tFP=11015\tFN=74\tTP=1327\n",
      "TSS: 0.8207980069309868\n",
      "HSS: 0.1695182720398389\n",
      "TN=80683\tFP=6473\tFN=567\tTP=834\n",
      "TSS: 0.5210199526055965\n",
      "HSS: 0.1694981056259245\n",
      "FromLasso *-*-**-*-**-*-**-*-**-*-*\n",
      "TN=64121\tFP=8210\tFN=1014\tTP=147\n",
      "TSS: 0.013109021449875649\n",
      "HSS: 0.0032373477665669954\n",
      "TN=64121\tFP=8210\tFN=1014\tTP=147\n",
      "TSS: 0.013109021449875649\n",
      "HSS: 0.0032373477665669954\n",
      "TN=64121\tFP=8210\tFN=1014\tTP=147\n",
      "TSS: 0.013109021449875649\n",
      "HSS: 0.0032373477665669954\n",
      "TN=64114\tFP=8217\tFN=1012\tTP=149\n",
      "TSS: 0.014734897019921978\n",
      "HSS: 0.003635443066417614\n",
      "TN=64114\tFP=8217\tFN=1012\tTP=149\n",
      "TSS: 0.014734897019921978\n",
      "HSS: 0.003635443066417614\n",
      "TN=64114\tFP=8217\tFN=1012\tTP=149\n",
      "TSS: 0.014734897019921978\n",
      "HSS: 0.003635443066417614\n",
      "TN=63693\tFP=8638\tFN=1010\tTP=151\n",
      "TSS: 0.010637085650758257\n",
      "HSS: 0.0025132697657293305\n",
      "TN=63701\tFP=8630\tFN=1011\tTP=150\n",
      "TSS: 0.009886361854204773\n",
      "HSS: 0.0023379997937811076\n",
      "TN=62650\tFP=9681\tFN=988\tTP=173\n",
      "TSS: 0.015166447396444321\n",
      "HSS: 0.0032381590025357073\n",
      "TN=63693\tFP=8638\tFN=1009\tTP=152\n",
      "TSS: 0.011498412093480057\n",
      "HSS: 0.0027165070714471906\n",
      "TN=63675\tFP=8656\tFN=1011\tTP=150\n",
      "TSS: 0.009526903254157768\n",
      "HSS: 0.002247137448320282\n",
      "TN=62892\tFP=9439\tFN=1000\tTP=161\n",
      "TSS: 0.008176260130374174\n",
      "HSS: 0.0017867580517324606\n",
      "TN=63603\tFP=8728\tFN=1008\tTP=153\n",
      "TSS: 0.011115458766808375\n",
      "HSS: 0.0026023265705888447\n",
      "TN=63461\tFP=8870\tFN=1005\tTP=156\n",
      "TSS: 0.011736241125486241\n",
      "HSS: 0.002708697888293777\n",
      "TN=63747\tFP=8584\tFN=1011\tTP=150\n",
      "TSS: 0.010522327069672543\n",
      "HSS: 0.0024999215190330093\n",
      "TN=63576\tFP=8755\tFN=1008\tTP=153\n",
      "TSS: 0.010742174835990326\n",
      "HSS: 0.0025082156485565937\n",
      "TN=63365\tFP=8966\tFN=1004\tTP=157\n",
      "TSS: 0.011270335814188306\n",
      "HSS: 0.0025767234846880717\n",
      "TN=63931\tFP=8400\tFN=1015\tTP=146\n",
      "TSS: 0.009620882160656527\n",
      "HSS: 0.002329851105763831\n",
      "FromForest *-*-**-*-**-*-**-*-**-*-*\n",
      "TN=83022\tFP=4134\tFN=386\tTP=1015\n",
      "TSS: 0.6770503219362106\n",
      "HSS: 0.29232163456378374\n",
      "TN=83022\tFP=4134\tFN=386\tTP=1015\n",
      "TSS: 0.6770503219362106\n",
      "HSS: 0.29232163456378374\n",
      "TN=83022\tFP=4134\tFN=386\tTP=1015\n",
      "TSS: 0.6770503219362106\n",
      "HSS: 0.29232163456378374\n",
      "TN=83123\tFP=4033\tFN=379\tTP=1022\n",
      "TSS: 0.6832055946741686\n",
      "HSS: 0.2992439079395601\n",
      "TN=83123\tFP=4033\tFN=379\tTP=1022\n",
      "TSS: 0.6832055946741686\n",
      "HSS: 0.2992439079395601\n",
      "TN=83123\tFP=4033\tFN=379\tTP=1022\n",
      "TSS: 0.6832055946741686\n",
      "HSS: 0.2992439079395601\n",
      "TN=84801\tFP=2355\tFN=850\tTP=551\n",
      "TSS: 0.36626999184214026\n",
      "HSS: 0.2396298390001187\n",
      "TN=84922\tFP=2234\tFN=803\tTP=598\n",
      "TSS: 0.4012057731427061\n",
      "HSS: 0.2670259669458558\n",
      "TN=84677\tFP=2479\tFN=999\tTP=402\n",
      "TSS: 0.25849465031714036\n",
      "HSS: 0.1700954358365223\n",
      "TN=84895\tFP=2261\tFN=824\tTP=577\n",
      "TSS: 0.38590669043757514\n",
      "HSS: 0.25648387073096923\n",
      "TN=85020\tFP=2136\tFN=853\tTP=548\n",
      "TSS: 0.3666414000031251\n",
      "HSS: 0.25276429592343497\n",
      "TN=84501\tFP=2655\tFN=1010\tTP=391\n",
      "TSS: 0.24862374812821783\n",
      "HSS: 0.15759183833364496\n",
      "TN=84775\tFP=2381\tFN=670\tTP=731\n",
      "TSS: 0.49445133356585347\n",
      "HSS: 0.3088738767695434\n",
      "TN=85858\tFP=1298\tFN=884\tTP=517\n",
      "TSS: 0.3541292912175102\n",
      "HSS: 0.30918154371869444\n",
      "TN=86652\tFP=504\tFN=1156\tTP=245\n",
      "TSS: 0.16909235481471457\n",
      "HSS: 0.21930156005412077\n",
      "TN=85545\tFP=1611\tFN=807\tTP=594\n",
      "TSS: 0.405498771898635\n",
      "HSS: 0.3162214098275436\n",
      "TN=86162\tFP=994\tFN=939\tTP=462\n",
      "TSS: 0.3183596166582297\n",
      "HSS: 0.31232753798245394\n",
      "TN=86688\tFP=468\tFN=1174\tTP=227\n",
      "TSS: 0.15665744153361866\n",
      "HSS: 0.20829687762514992\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "for ind in range(len(selected_labels)):\n",
    "     print(selected_labels[ind], \"*-*-*\"*5)\n",
    "     for kernel, c_value, gamma_value in params:\n",
    "         classifier = SVC(class_weight = 'balanced', gamma=gamma_value, C=c_value, kernel=kernel)\n",
    "         classifier.fit(us_di_train_x1[ind],di_train_y1[ind])\n",
    "         y_pred = classifier.predict(di_test_x2[ind])\n",
    "         tss_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "         hss_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "         print(f\"TSS: {tss_score}\")\n",
    "         print(f\"HSS: {hss_score}\")\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7 (10 points)\n",
    "\n",
    "Results above were able to find some combinations of hyperparamters and datasets that work fairly well for our problem. But the question remains, can we do better?\n",
    "\n",
    "Maybe one way to improve our results would be to elinate the easy to classify instances from our dataset and only focus our efforts on the more difficult ones. If you recall from our data preparation there was a feature in our dataset that we could use to easily distinguish between a rather large percentage of `flare` and `non-flare` data. This feature was `R_VALUE_median`, but we don't know what value to use to filter off part of our data.\n",
    "\n",
    "So, for this question, let's plot and see where a good cutoff might be. To do this, let's use the seaborn [ecdfplot](https://seaborn.pydata.org/generated/seaborn.ecdfplot.html#seaborn.ecdfplot) or the cumulative distribution function plot.  Your input will be the original analytics base table of partition one.  You should set the `x` axis to `R_VALUE_median`, and set the `hue` to `lab`.\n",
    "\n",
    "After plotting this, you will see that around 0.5 we begin to see some instances of the `M` and around 0.7 we begin to see some instances of the `X` class flares in our dataset. So, use 0.5 as a threshold value to filter out all of the instances that fall below this threshold from our training data. Construct a copy of the original partition 1 data with this applied.\n",
    "\n",
    "You can then verify this using the seaborn [kdeplot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html#seaborn.kdeplot) using the new filtered data as input, setting `x` to `R_VALUE_median` again, and setting `hue` to `lab` again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='R_VALUE_median', ylabel='Proportion'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "sns.ecdfplot(abt, hue='lab', x='R_VALUE_median')\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_abt = abt[abt['R_VALUE_median'] >= 0.5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='R_VALUE_median', ylabel='Density'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-----------------------------------------------------\n",
    "sns.kdeplot(data=filter_abt, hue='lab', x='R_VALUE_median')\n",
    "#-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8 (10 points)\n",
    "\n",
    "For this question, you will utilize the filtered analytics base table you constructed in the previous question.  You should:\n",
    "\n",
    "* Repeat the feature selection I did for you in the  example using the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), and scoring function [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif).\n",
    "\n",
    "* Repeat the feature selection from Q1 using the [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), and utilizing the [LassoLars](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars) as your `estimator`.  You should also set the `max_features` to the number of features we are going to select (20 features).\n",
    "\n",
    "* Repeat the feature selection from Q2 using [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). With a random forest model called [ExtraTressClssifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) as the `estimator`. The `n_estimators` of the random forest algorithm should be set to `75` when you construct it. The `max_features` of the SelectFromModel should be set to the number of features we are going to select (20 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "\n",
    "# Split the target and descriptive features for Partition 1 into two \n",
    "# different DataFrame objects\n",
    "df_labels = filter_abt['lab'].copy()\n",
    "df_feats = filter_abt.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Split the target and descriptive features for Partition 2 inot two\n",
    "# different DataFrame Objects\n",
    "df_test_labels = abt2['lab'].copy()\n",
    "df_test_feats = abt2.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Do feature selection\n",
    "feats1 = SelectKBest(f_classif, k=numFeat).fit(df_feats, df_labels)\n",
    "# Construct a training dataset from Partition 1 with only the selected descriptive \n",
    "# features and the target feature\n",
    "df_selected_feats1 = df_feats.loc[:, feats1.get_support()]\n",
    "df_train_set1 = pd.concat([df_labels, df_selected_feats1], axis=1)\n",
    "\n",
    "# Construct a testing dataset from Partition 2 with only the selected descriptive\n",
    "# features and the target feature\n",
    "df_test_selected_feats1 = df_test_feats.loc[:, feats1.get_support()]\n",
    "df_test_set1 = pd.concat([df_test_labels, df_test_selected_feats1], axis=1)\n",
    "#----------------------------------------------\n",
    "lasso_labs = filter_abt['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso_feats = filter_abt.drop(['lab'], axis =1)\n",
    "lasso_df_feats = SelectFromModel(max_features = numFeat, estimator = LassoLars( alpha = 0, eps =1)).fit(lasso_feats, lasso_labs)\n",
    "lasso_train = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_train_set = pd.concat([filter_abt['lab'], lasso_train], axis = 1, join = 'inner')\n",
    "#----------------------------------------------\n",
    "lasso2_labs = abt2_cpy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso2_feats = abt2_cpy.drop(['lab'], axis =1)\n",
    "lasso_df_test_feats = lasso2_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test_set = pd.concat([abt2['lab'], lasso_test], axis = 1, join = 'inner')\n",
    "#----------------------------------------------\n",
    "xtrees_labels = filter_abt['lab'].copy()\n",
    "xtrees_features = filter_abt.copy().drop(['lab'], axis = 1)\n",
    "xtrees_model_features = SelectFromModel(max_features = numFeat, estimator = ExtraTreesClassifier(n_estimators = 75)).fit(xtrees_features, xtrees_labels)\n",
    "xtrees_selected_features = xtrees_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_train = pd.concat([xtrees_labels, xtrees_selected_features], axis = 1)\n",
    "#----------------------------------------------\n",
    "xtrees_test_labels = abt2_cpy['lab']\n",
    "xtrees_test_features = abt2_cpy.drop(['lab'], axis = 1)\n",
    "xtrees_test_selected = xtrees_test_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_test = pd.concat([xtrees_test_labels, xtrees_test_selected], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q9 (10 points)\n",
    "\n",
    "Using the training and testing datsets you constructed in the previous question after performing feature selection on the filtered partition 1 analytics base table. You now need to perform the sampling on the training data using the function you made in Q5.\n",
    "\n",
    "Then you should convert each of the new training and testing datasets to a binary classification problem datase or dichotomize the training and testing data like you did in Q3. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed with the feature selected training and testing data.\n",
    "\n",
    "**Note:** You might want to put the training and testing tuples you get from the call to the dichotomize method into seperate training and testing lists. Then you can loop over them later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "train = [df_train_set1.copy(), lasso_train_set.copy(), xtrees_train.copy()]\n",
    "test = [df_test_set1.copy(), lasso_test_set.copy(), xtrees_test.copy()]\n",
    "\n",
    "#undersampling\n",
    "for frame in train:\n",
    "     frame = perform_under_sample_clust(frame)\n",
    "\n",
    "di_test_x1 = []\n",
    "di_test_y1 = []\n",
    "di_train_x2 = []\n",
    "di_train_y2 = []\n",
    "\n",
    "for frame in train:\n",
    "     x1, y1 = dichotomize_X_y(frame)\n",
    "     di_train_x1.append(x1)\n",
    "     di_train_y1.append(y1)\n",
    "    \n",
    "\n",
    "for frame in test:\n",
    "     x2, y2 = dichotomize_X_y(frame)\n",
    "     di_test_x2.append(x2)\n",
    "     di_test_y2.append(y2)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q10 (20 points)\n",
    "\n",
    "Like in Q6, this question will be utilizing the filtered and sampled datasets constructed in the previous question. For this question, you will again train your models on the three different feature selected data that had the instances below our thrshold filtered out and then had sampling by clustering performed on them. \n",
    "\n",
    "You will again be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. Like before, you should set the `class_weight` to `balanced` when you construct your models. You will again evaluate several different settings for the `kernel`, the regularization parameter `C`, and kernel coefficient `gamma`. **Note:** The `gamma` paramter is only utilized on the ‘rbf’, ‘poly’ and ‘sigmoid’ kernels, so there is no reason to evaluate multiple settings for the `linear` kernel. I have listed the settings of each parameter in a code block below. \n",
    "\n",
    "For each of the settings, you should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out. **Note:** The testing data has the samples in it that are below our threshold value, so you will first need to filter those out of the data you plan to pass to your model for testing. However, you still want those instances included in the calculation of the TSS and HSSS. So, your groud truth `lab` data should include all the instances in partition 2. You will need to concatenate a vector with all zeros in it to the match the labels you partitioned from the model testing data. \n",
    "\n",
    "Let's give you a representation of that:\n",
    "    \n",
    "    labels_from_data = [labels for samples > threshold] + [labels for samples <= threshold]\n",
    "    predict_labels = [labels from the model on > thrshold samples] + [0s the length of samples <= threshold]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "kernel = ['linear', 'poly', 'rbf']\n",
    "c_vals = [ 0.5, 1.0]\n",
    "gamma_vals = [0.5, 1, 10]\n",
    "temp = [kernel, c_vals, gamma_vals]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Val -_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_-\n",
      "\n",
      "TN=77673\tFP=9483\tFN=88\tTP=1313\n",
      "TSS: 0.8283828214991299\n",
      "HSS: 0.19268908613638142\n",
      "TN=77673\tFP=9483\tFN=88\tTP=1313\n",
      "TSS: 0.8283828214991299\n",
      "HSS: 0.19268908613638142\n",
      "TN=77673\tFP=9483\tFN=88\tTP=1313\n",
      "TSS: 0.8283828214991299\n",
      "HSS: 0.19268908613638142\n",
      "TN=77696\tFP=9460\tFN=86\tTP=1315\n",
      "TSS: 0.8300742678736093\n",
      "HSS: 0.19341429433144303\n",
      "TN=77696\tFP=9460\tFN=86\tTP=1315\n",
      "TSS: 0.8300742678736093\n",
      "HSS: 0.19341429433144303\n",
      "TN=77696\tFP=9460\tFN=86\tTP=1315\n",
      "TSS: 0.8300742678736093\n",
      "HSS: 0.19341429433144303\n",
      "TN=76996\tFP=10160\tFN=123\tTP=1278\n",
      "TSS: 0.7956329849560654\n",
      "HSS: 0.17584978626330627\n",
      "TN=76373\tFP=10783\tFN=83\tTP=1318\n",
      "TSS: 0.8170359176776526\n",
      "HSS: 0.17174301434535463\n",
      "TN=79324\tFP=7832\tFN=291\tTP=1110\n",
      "TSS: 0.7024293636564743\n",
      "HSS: 0.1925503029266028\n",
      "TN=76868\tFP=10288\tFN=110\tTP=1291\n",
      "TSS: 0.8034434403623698\n",
      "HSS: 0.17565383748955804\n",
      "TN=76352\tFP=10804\tFN=81\tTP=1320\n",
      "TSS: 0.8182225221594339\n",
      "HSS: 0.17170094568229738\n",
      "TN=79643\tFP=7513\tFN=306\tTP=1095\n",
      "TSS: 0.6953828292629043\n",
      "HSS: 0.19695068328515927\n",
      "TN=75633\tFP=11523\tFN=68\tTP=1333\n",
      "TSS: 0.8192520330524518\n",
      "HSS: 0.16311846723656773\n",
      "TN=75925\tFP=11231\tFN=72\tTP=1329\n",
      "TSS: 0.819747243933765\n",
      "HSS: 0.16666627246149338\n",
      "TN=79921\tFP=7235\tFN=464\tTP=937\n",
      "TSS: 0.5857959239790858\n",
      "HSS: 0.17343328956617524\n",
      "TN=75908\tFP=11248\tFN=73\tTP=1328\n",
      "TSS: 0.818838415509938\n",
      "HSS: 0.16629153919626086\n",
      "TN=76141\tFP=11015\tFN=74\tTP=1327\n",
      "TSS: 0.8207980069309868\n",
      "HSS: 0.1695182720398389\n",
      "TN=80683\tFP=6473\tFN=567\tTP=834\n",
      "TSS: 0.5210199526055965\n",
      "HSS: 0.1694981056259245\n",
      "FromLasso -_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_-\n",
      "\n",
      "TN=64121\tFP=8210\tFN=1014\tTP=147\n",
      "TSS: 0.013109021449875649\n",
      "HSS: 0.0032373477665669954\n",
      "TN=64121\tFP=8210\tFN=1014\tTP=147\n",
      "TSS: 0.013109021449875649\n",
      "HSS: 0.0032373477665669954\n",
      "TN=64121\tFP=8210\tFN=1014\tTP=147\n",
      "TSS: 0.013109021449875649\n",
      "HSS: 0.0032373477665669954\n",
      "TN=64114\tFP=8217\tFN=1012\tTP=149\n",
      "TSS: 0.014734897019921978\n",
      "HSS: 0.003635443066417614\n",
      "TN=64114\tFP=8217\tFN=1012\tTP=149\n",
      "TSS: 0.014734897019921978\n",
      "HSS: 0.003635443066417614\n",
      "TN=64114\tFP=8217\tFN=1012\tTP=149\n",
      "TSS: 0.014734897019921978\n",
      "HSS: 0.003635443066417614\n",
      "TN=63693\tFP=8638\tFN=1010\tTP=151\n",
      "TSS: 0.010637085650758257\n",
      "HSS: 0.0025132697657293305\n",
      "TN=63701\tFP=8630\tFN=1011\tTP=150\n",
      "TSS: 0.009886361854204773\n",
      "HSS: 0.0023379997937811076\n",
      "TN=62650\tFP=9681\tFN=988\tTP=173\n",
      "TSS: 0.015166447396444321\n",
      "HSS: 0.0032381590025357073\n",
      "TN=63693\tFP=8638\tFN=1009\tTP=152\n",
      "TSS: 0.011498412093480057\n",
      "HSS: 0.0027165070714471906\n",
      "TN=63675\tFP=8656\tFN=1011\tTP=150\n",
      "TSS: 0.009526903254157768\n",
      "HSS: 0.002247137448320282\n",
      "TN=62892\tFP=9439\tFN=1000\tTP=161\n",
      "TSS: 0.008176260130374174\n",
      "HSS: 0.0017867580517324606\n",
      "TN=63603\tFP=8728\tFN=1008\tTP=153\n",
      "TSS: 0.011115458766808375\n",
      "HSS: 0.0026023265705888447\n",
      "TN=63461\tFP=8870\tFN=1005\tTP=156\n",
      "TSS: 0.011736241125486241\n",
      "HSS: 0.002708697888293777\n",
      "TN=63747\tFP=8584\tFN=1011\tTP=150\n",
      "TSS: 0.010522327069672543\n",
      "HSS: 0.0024999215190330093\n",
      "TN=63576\tFP=8755\tFN=1008\tTP=153\n",
      "TSS: 0.010742174835990326\n",
      "HSS: 0.0025082156485565937\n",
      "TN=63365\tFP=8966\tFN=1004\tTP=157\n",
      "TSS: 0.011270335814188306\n",
      "HSS: 0.0025767234846880717\n",
      "TN=63931\tFP=8400\tFN=1015\tTP=146\n",
      "TSS: 0.009620882160656527\n",
      "HSS: 0.002329851105763831\n",
      "FromForest -_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_-\n",
      "\n",
      "TN=83022\tFP=4134\tFN=386\tTP=1015\n",
      "TSS: 0.6770503219362106\n",
      "HSS: 0.29232163456378374\n",
      "TN=83022\tFP=4134\tFN=386\tTP=1015\n",
      "TSS: 0.6770503219362106\n",
      "HSS: 0.29232163456378374\n",
      "TN=83022\tFP=4134\tFN=386\tTP=1015\n",
      "TSS: 0.6770503219362106\n",
      "HSS: 0.29232163456378374\n",
      "TN=83123\tFP=4033\tFN=379\tTP=1022\n",
      "TSS: 0.6832055946741686\n",
      "HSS: 0.2992439079395601\n",
      "TN=83123\tFP=4033\tFN=379\tTP=1022\n",
      "TSS: 0.6832055946741686\n",
      "HSS: 0.2992439079395601\n",
      "TN=83123\tFP=4033\tFN=379\tTP=1022\n",
      "TSS: 0.6832055946741686\n",
      "HSS: 0.2992439079395601\n",
      "TN=84801\tFP=2355\tFN=850\tTP=551\n",
      "TSS: 0.36626999184214026\n",
      "HSS: 0.2396298390001187\n",
      "TN=84922\tFP=2234\tFN=803\tTP=598\n",
      "TSS: 0.4012057731427061\n",
      "HSS: 0.2670259669458558\n",
      "TN=84677\tFP=2479\tFN=999\tTP=402\n",
      "TSS: 0.25849465031714036\n",
      "HSS: 0.1700954358365223\n",
      "TN=84895\tFP=2261\tFN=824\tTP=577\n",
      "TSS: 0.38590669043757514\n",
      "HSS: 0.25648387073096923\n",
      "TN=85020\tFP=2136\tFN=853\tTP=548\n",
      "TSS: 0.3666414000031251\n",
      "HSS: 0.25276429592343497\n",
      "TN=84501\tFP=2655\tFN=1010\tTP=391\n",
      "TSS: 0.24862374812821783\n",
      "HSS: 0.15759183833364496\n",
      "TN=84775\tFP=2381\tFN=670\tTP=731\n",
      "TSS: 0.49445133356585347\n",
      "HSS: 0.3088738767695434\n",
      "TN=85858\tFP=1298\tFN=884\tTP=517\n",
      "TSS: 0.3541292912175102\n",
      "HSS: 0.30918154371869444\n",
      "TN=86652\tFP=504\tFN=1156\tTP=245\n",
      "TSS: 0.16909235481471457\n",
      "HSS: 0.21930156005412077\n",
      "TN=85545\tFP=1611\tFN=807\tTP=594\n",
      "TSS: 0.405498771898635\n",
      "HSS: 0.3162214098275436\n",
      "TN=86162\tFP=994\tFN=939\tTP=462\n",
      "TSS: 0.3183596166582297\n",
      "HSS: 0.31232753798245394\n",
      "TN=86688\tFP=468\tFN=1174\tTP=227\n",
      "TSS: 0.15665744153361866\n",
      "HSS: 0.20829687762514992\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "for ind in range(len(selected_labels)):\n",
    "     print(selected_labels[ind], \"-_-_-_-_-_-_-\"*5, end='\\n\\n')\n",
    "     for kernel, c_value, gamma_value in params:\n",
    "         classifier = SVC(class_weight = 'balanced', gamma=gamma_value, C=c_value, kernel=kernel)\n",
    "         classifier.fit(di_train_x1[ind],di_train_y1[ind])\n",
    "         y_pred = classifier.predict(di_test_x2[ind])\n",
    "         tss_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "         hss_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "         print(f\"TSS: {tss_score}\")\n",
    "         print(f\"HSS: {hss_score}\")\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these results are getting unruely, we should maybe be saving them to do analysis on them too? Maybe I'll ask you to do that for the extra credit assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
