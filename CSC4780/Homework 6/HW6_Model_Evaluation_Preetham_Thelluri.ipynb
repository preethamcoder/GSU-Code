{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 6: Model Evaluation 2\n",
    "As in the previous assignments, in this homework assignment you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "This assignment will continue to utilize a copy of the extracted feature dataset we used in Homework 5. Recall that the dataset has been processed by performing log, z-score and range scaling. We continuing to use more than one partition worth of data, so for the scaling, the mean, standard deviation, minimum, and maximum were calculated using data from both partitions so that a global scaling can be performed on each partition. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This assignment will continue to use [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB) for a training set and [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD) as a testing set. \n",
    "\n",
    "---\n",
    "\n",
    "For this assignment, cleaning, transforming, and normalization of the data has been completed using both partitions to find the various minimum, maximum, standard deviation, and mean values needed to perform these operations. Recall from lecture that we should not perform these operations on each partition individually, but as a whole as there may(will) be different values for these in different partitions. \n",
    "\n",
    "For example, if we perform simple range scaling on each partition individually and we see a range of 0 to 100 in one partition and 0 to 10 in another. After individual scaling the values with 100 in the first would be mapped to 1 just like the values that had 10 in the second. This can cause serious performance problems in your model, so I have made sure that the normalization was treated properly for you. \n",
    "\n",
    "Below you will find the full partitions and `toy` sampled data from each partition, where only 20 samples from each of our 5 classes have been included in the data.  \n",
    "\n",
    "#### Full\n",
    "- [Full Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition1ExtractedFeatures.csv)\n",
    "- [Full Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "#### Toy\n",
    "- [Toy Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition1ExtractedFeatures.csv)\n",
    "- [Toy Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the two files, you should load each into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "As was done in Homework 5, for each of the models we evaluate in this assignmnet, you will calculate the True Skill Statistic score using the test data from Partition 2 to determine which model performs the best for classifying the positive flaring class.\n",
    "\n",
    "    True skill statistic (TSS) = TPR + TNR - 1 = TPR - (1-TNR) = TPR - FPR\n",
    "\n",
    "Where:\n",
    "\n",
    "    True positive rate (TPR) = TP/(TP+FN) Also known as recall or sensitivity\n",
    "    True negative rate (TNR) = TN/(TN+FP) Also known as specificity or selectivity\n",
    "    False positive rate (FPR) = FP/(FP+TN) = (1-TNR) Also known as fall-out or false alarm ratio\n",
    "\n",
    "\n",
    "**Recall**\n",
    "\n",
    "    True positive (TP)\n",
    "    True negative (TN)\n",
    "    False positive (FP)\n",
    "    False negative (FN)\n",
    "    \n",
    "See [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for more information.\n",
    "\n",
    "Below is a function implemented to provide your score for each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tss(y_true, y_predict):\n",
    "    '''\n",
    "    Calculates the true skill score for binary classification based on the output of the confusion\n",
    "    table function\n",
    "    \n",
    "        Parameters:\n",
    "            y_true   : A vector/list of values that represent the true class label of the data being evaluated.\n",
    "            y_predict: A vector/list of values that represent the predicted class label for the data being evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            tss_value (float): A floating point value (-1.0,1.0) indicating the TSS of the input data\n",
    "    '''\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    \n",
    "    return tp_rate - fp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In addition to the TSS, you will be asked to also calculate the Heidke Skill Score (HSS) to see how much better your model performs than a random forecast.  \n",
    "\n",
    "Below is a function implemented to provide your score fore each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hss(y_true, y_predict):\n",
    "    '''\n",
    "    Calculates the Heidke Skill Score for binary classification based on the output of the confusion\n",
    "    table function.\n",
    "    \n",
    "    The HSS measures the fractional improvement of the forecast over the standard forecast.\n",
    "    The \"standard forecast\" is usually the number correct by chance or the proportion \n",
    "    correct by chance.\n",
    "    \n",
    "        Parameters:\n",
    "            y_true   : A vector/list of values that represent the true class label of the data being evaluated.\n",
    "            y_predict: A vector/list of values that represent the predicted class label for the data being evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            hss_value (float): A floating point value (-inf,1.0) indicating the HSS of the input data. \n",
    "                Negative values indicate that the chance forecast is better, 0 means no skill, and a perfect forecast obtains a HSS of 1.\n",
    "    '''\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    #print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    P = float(TP + FN)\n",
    "    N = float(FP + TN)\n",
    "    numerator = 2*((TP * TN) - (FN * FP))\n",
    "    denominator = P*(FN + TN) + N*(TP + FP)\n",
    "    \n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As in the previous assignment, we will be utilizing a binary classification of our 5 class dataset. So, below is the helper function to change our class labels from the 5 class target feature to the binary target feature. The function is implemented to take a dataframe (e.g. our `abt`) and prepares it for a binary classification by merging the `X`- and `M`-class samples into one group, and the rest (`NF`, `B`, and `C`) into another group, labeled with `1`s and `0`s, respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomize_X_y(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    dichotomizes the dataset and split it into the features (X) and the labels (y).\n",
    "    \n",
    "    :return: two np.ndarray objects X and y.\n",
    "    \"\"\"\n",
    "    data_dich = data.copy()\n",
    "    data_dich['lab'] = data_dich['lab'].map({'NF': 0, 'B': 0, 'C': 0, 'M': 1, 'X': 1})\n",
    "    y = data_dich['lab']\n",
    "    X = data_dich.drop(['lab'], axis=1)\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reading the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/FDS'\n",
    "data_file = \"toy_normalized_partition1ExtractedFeatures.csv\"\n",
    "data_file2 = \"toy_normalized_partition2ExtractedFeatures.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file).replace('\\\\', '/'))\n",
    "abt2 = pd.read_csv(os.path.join(data_dir, data_file2).replace('\\\\', '/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Run Feature Selection\n",
    "\n",
    "Below you have code to perform feature selction using [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). The scoring function being used is [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif).\n",
    "\n",
    "Once feature selection is done with this one method, a set of training and testing dataframes are constructed by doing the following:\n",
    "\n",
    "* Utilizing the `get_support` function of the feature selection object we get a mask of the features we will select from our original analytics base table DataFrame.  \n",
    "\n",
    "* The mask of selected features is then paired with the `loc` function on our datframe containing only the descriptive features to get our selected featrues on all rows in our feature dataframe.\n",
    "\n",
    "* The set of selected features are concatenated with our labels to construct a training dataset.\n",
    "\n",
    "* This process was then repeated to construct the testing set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "\n",
    "# Split the target and descriptive features for Partition 1 into two \n",
    "# different DataFrame objects\n",
    "df_labels = abt['lab'].copy()\n",
    "df_feats = abt.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Split the target and descriptive features for Partition 2 inot two\n",
    "# different DataFrame Objects\n",
    "df_test_labels = abt2['lab'].copy()\n",
    "df_test_feats = abt2.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Do feature selection\n",
    "feats1 = SelectKBest(f_classif, k=numFeat).fit(df_feats, df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTPOT_max</th>\n",
       "      <th>TOTPOT_median</th>\n",
       "      <th>TOTPOT_linear_weighted_average</th>\n",
       "      <th>TOTPOT_quadratic_weighted_average</th>\n",
       "      <th>TOTPOT_last_value</th>\n",
       "      <th>ABSNJZH_var</th>\n",
       "      <th>ABSNJZH_difference_of_vars</th>\n",
       "      <th>SAVNCPP_min</th>\n",
       "      <th>SAVNCPP_median</th>\n",
       "      <th>...</th>\n",
       "      <th>SAVNCPP_var</th>\n",
       "      <th>SAVNCPP_linear_weighted_average</th>\n",
       "      <th>SAVNCPP_quadratic_weighted_average</th>\n",
       "      <th>SAVNCPP_last_value</th>\n",
       "      <th>R_VALUE_mean</th>\n",
       "      <th>R_VALUE_linear_weighted_average</th>\n",
       "      <th>R_VALUE_quadratic_weighted_average</th>\n",
       "      <th>ABSNJZH_max</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>ABSNJZH_difference_of_means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.888176</td>\n",
       "      <td>0.910607</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.897118</td>\n",
       "      <td>0.889232</td>\n",
       "      <td>0.544861</td>\n",
       "      <td>0.446953</td>\n",
       "      <td>0.826002</td>\n",
       "      <td>0.875710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905469</td>\n",
       "      <td>0.877613</td>\n",
       "      <td>0.879440</td>\n",
       "      <td>0.884478</td>\n",
       "      <td>0.679219</td>\n",
       "      <td>0.679430</td>\n",
       "      <td>0.679142</td>\n",
       "      <td>0.101098</td>\n",
       "      <td>0.158041</td>\n",
       "      <td>0.066318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.897998</td>\n",
       "      <td>0.923272</td>\n",
       "      <td>0.912721</td>\n",
       "      <td>0.910271</td>\n",
       "      <td>0.902113</td>\n",
       "      <td>0.622084</td>\n",
       "      <td>0.583644</td>\n",
       "      <td>0.884346</td>\n",
       "      <td>0.911267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921677</td>\n",
       "      <td>0.910235</td>\n",
       "      <td>0.909503</td>\n",
       "      <td>0.901160</td>\n",
       "      <td>0.694106</td>\n",
       "      <td>0.690313</td>\n",
       "      <td>0.689117</td>\n",
       "      <td>0.169857</td>\n",
       "      <td>0.216134</td>\n",
       "      <td>0.046944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.910680</td>\n",
       "      <td>0.936391</td>\n",
       "      <td>0.925860</td>\n",
       "      <td>0.923360</td>\n",
       "      <td>0.915168</td>\n",
       "      <td>0.722335</td>\n",
       "      <td>0.669964</td>\n",
       "      <td>0.927232</td>\n",
       "      <td>0.935240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946928</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>0.940307</td>\n",
       "      <td>0.942041</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.629308</td>\n",
       "      <td>0.634254</td>\n",
       "      <td>0.373180</td>\n",
       "      <td>0.324493</td>\n",
       "      <td>0.278712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.903096</td>\n",
       "      <td>0.928696</td>\n",
       "      <td>0.918240</td>\n",
       "      <td>0.915727</td>\n",
       "      <td>0.907472</td>\n",
       "      <td>0.585993</td>\n",
       "      <td>0.582444</td>\n",
       "      <td>0.846077</td>\n",
       "      <td>0.887077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916147</td>\n",
       "      <td>0.887299</td>\n",
       "      <td>0.887924</td>\n",
       "      <td>0.863439</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.726870</td>\n",
       "      <td>0.725750</td>\n",
       "      <td>0.125847</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>0.097641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.878469</td>\n",
       "      <td>0.901278</td>\n",
       "      <td>0.891138</td>\n",
       "      <td>0.888651</td>\n",
       "      <td>0.881540</td>\n",
       "      <td>0.606894</td>\n",
       "      <td>0.560271</td>\n",
       "      <td>0.891314</td>\n",
       "      <td>0.908367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914248</td>\n",
       "      <td>0.907830</td>\n",
       "      <td>0.907221</td>\n",
       "      <td>0.908226</td>\n",
       "      <td>0.380567</td>\n",
       "      <td>0.324239</td>\n",
       "      <td>0.274938</td>\n",
       "      <td>0.142287</td>\n",
       "      <td>0.203227</td>\n",
       "      <td>0.174156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>X</td>\n",
       "      <td>0.944975</td>\n",
       "      <td>0.972000</td>\n",
       "      <td>0.961066</td>\n",
       "      <td>0.958503</td>\n",
       "      <td>0.950173</td>\n",
       "      <td>0.771674</td>\n",
       "      <td>0.708003</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964465</td>\n",
       "      <td>0.987145</td>\n",
       "      <td>0.986907</td>\n",
       "      <td>0.987173</td>\n",
       "      <td>0.872821</td>\n",
       "      <td>0.869824</td>\n",
       "      <td>0.868661</td>\n",
       "      <td>0.769927</td>\n",
       "      <td>0.396334</td>\n",
       "      <td>0.346541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>X</td>\n",
       "      <td>0.934951</td>\n",
       "      <td>0.961286</td>\n",
       "      <td>0.950650</td>\n",
       "      <td>0.948183</td>\n",
       "      <td>0.940155</td>\n",
       "      <td>0.826597</td>\n",
       "      <td>0.851072</td>\n",
       "      <td>0.966643</td>\n",
       "      <td>0.973606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972932</td>\n",
       "      <td>0.973942</td>\n",
       "      <td>0.974619</td>\n",
       "      <td>0.975578</td>\n",
       "      <td>0.918191</td>\n",
       "      <td>0.919130</td>\n",
       "      <td>0.919843</td>\n",
       "      <td>0.579843</td>\n",
       "      <td>0.495160</td>\n",
       "      <td>0.337517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>X</td>\n",
       "      <td>0.938538</td>\n",
       "      <td>0.965245</td>\n",
       "      <td>0.954360</td>\n",
       "      <td>0.951779</td>\n",
       "      <td>0.943762</td>\n",
       "      <td>0.882727</td>\n",
       "      <td>0.831361</td>\n",
       "      <td>0.981816</td>\n",
       "      <td>0.985870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979513</td>\n",
       "      <td>0.987029</td>\n",
       "      <td>0.987651</td>\n",
       "      <td>0.990187</td>\n",
       "      <td>0.952882</td>\n",
       "      <td>0.951576</td>\n",
       "      <td>0.950977</td>\n",
       "      <td>0.862346</td>\n",
       "      <td>0.621664</td>\n",
       "      <td>0.583486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>X</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.949557</td>\n",
       "      <td>0.947220</td>\n",
       "      <td>0.939333</td>\n",
       "      <td>0.859894</td>\n",
       "      <td>0.791927</td>\n",
       "      <td>0.965378</td>\n",
       "      <td>0.972216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968952</td>\n",
       "      <td>0.970228</td>\n",
       "      <td>0.969830</td>\n",
       "      <td>0.966993</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>0.910553</td>\n",
       "      <td>0.910627</td>\n",
       "      <td>0.579843</td>\n",
       "      <td>0.566709</td>\n",
       "      <td>0.239493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>X</td>\n",
       "      <td>0.932186</td>\n",
       "      <td>0.958334</td>\n",
       "      <td>0.947737</td>\n",
       "      <td>0.945306</td>\n",
       "      <td>0.937340</td>\n",
       "      <td>0.717814</td>\n",
       "      <td>0.603444</td>\n",
       "      <td>0.972312</td>\n",
       "      <td>0.973636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949732</td>\n",
       "      <td>0.973804</td>\n",
       "      <td>0.973998</td>\n",
       "      <td>0.972545</td>\n",
       "      <td>0.879228</td>\n",
       "      <td>0.880102</td>\n",
       "      <td>0.882134</td>\n",
       "      <td>0.582789</td>\n",
       "      <td>0.318601</td>\n",
       "      <td>0.129982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab  TOTPOT_max  TOTPOT_median  TOTPOT_linear_weighted_average  \\\n",
       "0    B    0.888176       0.910607                        0.899937   \n",
       "1    B    0.897998       0.923272                        0.912721   \n",
       "2    B    0.910680       0.936391                        0.925860   \n",
       "3    B    0.903096       0.928696                        0.918240   \n",
       "4    B    0.878469       0.901278                        0.891138   \n",
       "..  ..         ...            ...                             ...   \n",
       "95   X    0.944975       0.972000                        0.961066   \n",
       "96   X    0.934951       0.961286                        0.950650   \n",
       "97   X    0.938538       0.965245                        0.954360   \n",
       "98   X    0.934400       0.959772                        0.949557   \n",
       "99   X    0.932186       0.958334                        0.947737   \n",
       "\n",
       "    TOTPOT_quadratic_weighted_average  TOTPOT_last_value  ABSNJZH_var  \\\n",
       "0                            0.897118           0.889232     0.544861   \n",
       "1                            0.910271           0.902113     0.622084   \n",
       "2                            0.923360           0.915168     0.722335   \n",
       "3                            0.915727           0.907472     0.585993   \n",
       "4                            0.888651           0.881540     0.606894   \n",
       "..                                ...                ...          ...   \n",
       "95                           0.958503           0.950173     0.771674   \n",
       "96                           0.948183           0.940155     0.826597   \n",
       "97                           0.951779           0.943762     0.882727   \n",
       "98                           0.947220           0.939333     0.859894   \n",
       "99                           0.945306           0.937340     0.717814   \n",
       "\n",
       "    ABSNJZH_difference_of_vars  SAVNCPP_min  SAVNCPP_median  ...  SAVNCPP_var  \\\n",
       "0                     0.446953     0.826002        0.875710  ...     0.905469   \n",
       "1                     0.583644     0.884346        0.911267  ...     0.921677   \n",
       "2                     0.669964     0.927232        0.935240  ...     0.946928   \n",
       "3                     0.582444     0.846077        0.887077  ...     0.916147   \n",
       "4                     0.560271     0.891314        0.908367  ...     0.914248   \n",
       "..                         ...          ...             ...  ...          ...   \n",
       "95                    0.708003     0.986181        0.987464  ...     0.964465   \n",
       "96                    0.851072     0.966643        0.973606  ...     0.972932   \n",
       "97                    0.831361     0.981816        0.985870  ...     0.979513   \n",
       "98                    0.791927     0.965378        0.972216  ...     0.968952   \n",
       "99                    0.603444     0.972312        0.973636  ...     0.949732   \n",
       "\n",
       "    SAVNCPP_linear_weighted_average  SAVNCPP_quadratic_weighted_average  \\\n",
       "0                          0.877613                            0.879440   \n",
       "1                          0.910235                            0.909503   \n",
       "2                          0.938836                            0.940307   \n",
       "3                          0.887299                            0.887924   \n",
       "4                          0.907830                            0.907221   \n",
       "..                              ...                                 ...   \n",
       "95                         0.987145                            0.986907   \n",
       "96                         0.973942                            0.974619   \n",
       "97                         0.987029                            0.987651   \n",
       "98                         0.970228                            0.969830   \n",
       "99                         0.973804                            0.973998   \n",
       "\n",
       "    SAVNCPP_last_value  R_VALUE_mean  R_VALUE_linear_weighted_average  \\\n",
       "0             0.884478      0.679219                         0.679430   \n",
       "1             0.901160      0.694106                         0.690313   \n",
       "2             0.942041      0.621892                         0.629308   \n",
       "3             0.863439      0.730323                         0.726870   \n",
       "4             0.908226      0.380567                         0.324239   \n",
       "..                 ...           ...                              ...   \n",
       "95            0.987173      0.872821                         0.869824   \n",
       "96            0.975578      0.918191                         0.919130   \n",
       "97            0.990187      0.952882                         0.951576   \n",
       "98            0.966993      0.909167                         0.910553   \n",
       "99            0.972545      0.879228                         0.880102   \n",
       "\n",
       "    R_VALUE_quadratic_weighted_average  ABSNJZH_max  ABSNJZH_stddev  \\\n",
       "0                             0.679142     0.101098        0.158041   \n",
       "1                             0.689117     0.169857        0.216134   \n",
       "2                             0.634254     0.373180        0.324493   \n",
       "3                             0.725750     0.125847        0.186718   \n",
       "4                             0.274938     0.142287        0.203227   \n",
       "..                                 ...          ...             ...   \n",
       "95                            0.868661     0.769927        0.396334   \n",
       "96                            0.919843     0.579843        0.495160   \n",
       "97                            0.950977     0.862346        0.621664   \n",
       "98                            0.910627     0.579843        0.566709   \n",
       "99                            0.882134     0.582789        0.318601   \n",
       "\n",
       "    ABSNJZH_difference_of_means  \n",
       "0                      0.066318  \n",
       "1                      0.046944  \n",
       "2                      0.278712  \n",
       "3                      0.097641  \n",
       "4                      0.174156  \n",
       "..                          ...  \n",
       "95                     0.346541  \n",
       "96                     0.337517  \n",
       "97                     0.583486  \n",
       "98                     0.239493  \n",
       "99                     0.129982  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a training dataset from Partition 1 with only the selected descriptive \n",
    "# features and the target feature\n",
    "df_selected_feats1 = df_feats.loc[:, feats1.get_support()]\n",
    "df_train_set1 = pd.concat([df_labels, df_selected_feats1], axis=1)\n",
    "\n",
    "# Construct a testing dataset from Partition 2 with only the selected descriptive\n",
    "# features and the target feature\n",
    "df_test_selected_feats1 = df_test_feats.loc[:, feats1.get_support()]\n",
    "df_test_set1 = pd.concat([df_test_labels, df_test_selected_feats1], axis=1)\n",
    "df_test_set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q1 (5 points)\n",
    "\n",
    "Using the feature selection task above as a template, you will now perform feature selection again to produce a second set of training and testing data. This time you will use the [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). \n",
    "\n",
    "Instead of using the [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) that is shown in the example documentation linked above, you will be utilizing the [LassoLars](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars) as your `estimator`.  You should also set the `max_features` to the number of features we are going to select (20 features). \n",
    "\n",
    "**Note:** The LassoLars class is a regression model and will not work on our string class lables. So, you need to map the `NF`, `B`, `C`, `M`, `X` labels to a range between `-1` and `1` before you attempt to fit the feature selection model to them. You can try evenly spacing them, or forcing the classes that will be used as the negative class into a tight range and those that will be used as the postive class into another tight range.  Maybe this another location for you to do hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "abt_cpy = abt.copy()\n",
    "abt2_cpy = abt2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_dderivative_mean</th>\n",
       "      <th>TOTUSJZ_gderivative_mean</th>\n",
       "      <th>SAVNCPP_dderivative_mean</th>\n",
       "      <th>MEANJZD_kurtosis</th>\n",
       "      <th>TOTFX_kurtosis</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANGAM_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANALP_min</th>\n",
       "      <th>...</th>\n",
       "      <th>MEANPOT_difference_of_maxs</th>\n",
       "      <th>MEANPOT_stddev</th>\n",
       "      <th>MEANPOT_last_value</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_increase</th>\n",
       "      <th>MEANPOT_difference_of_means</th>\n",
       "      <th>MEANPOT_avg_mono_decrease_slope</th>\n",
       "      <th>MEANPOT_difference_of_stds</th>\n",
       "      <th>MEANPOT_dderivative_mean</th>\n",
       "      <th>MEANPOT_gderivative_stddev</th>\n",
       "      <th>MEANPOT_gderivative_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.979171</td>\n",
       "      <td>0.979249</td>\n",
       "      <td>0.976164</td>\n",
       "      <td>0.037609</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.999789</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.995406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>8.113658e-06</td>\n",
       "      <td>0.351491</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.322541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.980098</td>\n",
       "      <td>0.979847</td>\n",
       "      <td>0.978972</td>\n",
       "      <td>0.037284</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.999506</td>\n",
       "      <td>0.998619</td>\n",
       "      <td>0.995912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>2.846932e-05</td>\n",
       "      <td>0.351517</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.322561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.979165</td>\n",
       "      <td>0.979242</td>\n",
       "      <td>0.979138</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.040059</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>0.996573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>5.385364e-06</td>\n",
       "      <td>0.351505</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.322553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.977933</td>\n",
       "      <td>0.978422</td>\n",
       "      <td>0.973676</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.999843</td>\n",
       "      <td>0.996432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>7.232174e-06</td>\n",
       "      <td>0.351488</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.322538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>0.979897</td>\n",
       "      <td>0.979740</td>\n",
       "      <td>0.976483</td>\n",
       "      <td>0.034652</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.998720</td>\n",
       "      <td>0.996480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>7.075361e-07</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.322549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>X</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.982453</td>\n",
       "      <td>0.981571</td>\n",
       "      <td>0.981398</td>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.034746</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>1.695746e-05</td>\n",
       "      <td>0.351499</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.322547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>X</td>\n",
       "      <td>0.998882</td>\n",
       "      <td>0.980923</td>\n",
       "      <td>0.980462</td>\n",
       "      <td>0.981911</td>\n",
       "      <td>0.045091</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.999484</td>\n",
       "      <td>0.998356</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.999884</td>\n",
       "      <td>3.298372e-05</td>\n",
       "      <td>0.351538</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.322586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>X</td>\n",
       "      <td>0.998882</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>0.981080</td>\n",
       "      <td>0.979000</td>\n",
       "      <td>0.036840</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>2.544730e-06</td>\n",
       "      <td>0.351536</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>X</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.982395</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>0.979225</td>\n",
       "      <td>0.053946</td>\n",
       "      <td>0.027424</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>2.842715e-06</td>\n",
       "      <td>0.351485</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.322534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>X</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.978730</td>\n",
       "      <td>0.978976</td>\n",
       "      <td>0.979790</td>\n",
       "      <td>0.049728</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.999130</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.996816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>3.112438e-06</td>\n",
       "      <td>0.351504</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.322550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab  TOTBSQ_slope_of_longest_mono_decrease  TOTUSJZ_dderivative_mean  \\\n",
       "0    B                               0.999990                  0.979171   \n",
       "1    B                               0.999980                  0.980098   \n",
       "2    B                               0.999849                  0.979165   \n",
       "3    B                               0.999559                  0.977933   \n",
       "4    B                               0.999913                  0.979897   \n",
       "..  ..                                    ...                       ...   \n",
       "95   X                               0.999694                  0.982453   \n",
       "96   X                               0.998882                  0.980923   \n",
       "97   X                               0.998882                  0.981685   \n",
       "98   X                               0.999694                  0.982395   \n",
       "99   X                               0.999244                  0.978730   \n",
       "\n",
       "    TOTUSJZ_gderivative_mean  SAVNCPP_dderivative_mean  MEANJZD_kurtosis  \\\n",
       "0                   0.979249                  0.976164          0.037609   \n",
       "1                   0.979847                  0.978972          0.037284   \n",
       "2                   0.979242                  0.979138          0.040767   \n",
       "3                   0.978422                  0.973676          0.040877   \n",
       "4                   0.979740                  0.976483          0.034652   \n",
       "..                       ...                       ...               ...   \n",
       "95                  0.981571                  0.981398          0.050595   \n",
       "96                  0.980462                  0.981911          0.045091   \n",
       "97                  0.981080                  0.979000          0.036840   \n",
       "98                  0.981400                  0.979225          0.053946   \n",
       "99                  0.978976                  0.979790          0.049728   \n",
       "\n",
       "    TOTFX_kurtosis  MEANPOT_slope_of_longest_mono_decrease  \\\n",
       "0         0.036443                                0.999789   \n",
       "1         0.033039                                0.999506   \n",
       "2         0.040059                                0.999771   \n",
       "3         0.035849                                0.999849   \n",
       "4         0.027600                                0.999652   \n",
       "..             ...                                     ...   \n",
       "95        0.034746                                0.999897   \n",
       "96        0.027407                                0.999484   \n",
       "97        0.027419                                0.998950   \n",
       "98        0.027424                                0.999897   \n",
       "99        0.029754                                0.999130   \n",
       "\n",
       "    MEANGAM_slope_of_longest_mono_decrease  MEANALP_min  ...  \\\n",
       "0                                 0.997375     0.995406  ...   \n",
       "1                                 0.998619     0.995912  ...   \n",
       "2                                 0.999498     0.996573  ...   \n",
       "3                                 0.999843     0.996432  ...   \n",
       "4                                 0.998720     0.996480  ...   \n",
       "..                                     ...          ...  ...   \n",
       "95                                0.998922     0.995805  ...   \n",
       "96                                0.998356     0.995951  ...   \n",
       "97                                0.997506     0.995951  ...   \n",
       "98                                0.998922     0.995805  ...   \n",
       "99                                0.999872     0.996816  ...   \n",
       "\n",
       "    MEANPOT_difference_of_maxs  MEANPOT_stddev  MEANPOT_last_value  \\\n",
       "0                     0.000003        0.000023            0.000072   \n",
       "1                     0.000031        0.000073            0.000113   \n",
       "2                     0.000002        0.000017            0.000058   \n",
       "3                     0.000004        0.000018            0.000082   \n",
       "4                     0.000015        0.000051            0.000188   \n",
       "..                         ...             ...                 ...   \n",
       "95                    0.000004        0.000038            0.000259   \n",
       "96                    0.000008        0.000095            0.000351   \n",
       "97                    0.000028        0.000098            0.000339   \n",
       "98                    0.000017        0.000042            0.000253   \n",
       "99                    0.000002        0.000049            0.000196   \n",
       "\n",
       "    MEANPOT_slope_of_longest_mono_increase  MEANPOT_difference_of_means  \\\n",
       "0                                 0.000007                     0.000043   \n",
       "1                                 0.000013                     0.000096   \n",
       "2                                 0.000002                     0.000017   \n",
       "3                                 0.000016                     0.000041   \n",
       "4                                 0.000019                     0.000125   \n",
       "..                                     ...                          ...   \n",
       "95                                0.000008                     0.000024   \n",
       "96                                0.000035                     0.000212   \n",
       "97                                0.000035                     0.000248   \n",
       "98                                0.000009                     0.000059   \n",
       "99                                0.000019                     0.000035   \n",
       "\n",
       "    MEANPOT_avg_mono_decrease_slope  MEANPOT_difference_of_stds  \\\n",
       "0                          0.999974                8.113658e-06   \n",
       "1                          0.999944                2.846932e-05   \n",
       "2                          0.999989                5.385364e-06   \n",
       "3                          0.999983                7.232174e-06   \n",
       "4                          0.999952                7.075361e-07   \n",
       "..                              ...                         ...   \n",
       "95                         0.999957                1.695746e-05   \n",
       "96                         0.999884                3.298372e-05   \n",
       "97                         0.999889                2.544730e-06   \n",
       "98                         0.999952                2.842715e-06   \n",
       "99                         0.999960                3.112438e-06   \n",
       "\n",
       "    MEANPOT_dderivative_mean  MEANPOT_gderivative_stddev  \\\n",
       "0                   0.351491                    0.000014   \n",
       "1                   0.351517                    0.000032   \n",
       "2                   0.351505                    0.000007   \n",
       "3                   0.351488                    0.000014   \n",
       "4                   0.351500                    0.000021   \n",
       "..                       ...                         ...   \n",
       "95                  0.351499                    0.000021   \n",
       "96                  0.351538                    0.000051   \n",
       "97                  0.351536                    0.000051   \n",
       "98                  0.351485                    0.000023   \n",
       "99                  0.351504                    0.000033   \n",
       "\n",
       "    MEANPOT_gderivative_mean  \n",
       "0                   0.322541  \n",
       "1                   0.322561  \n",
       "2                   0.322553  \n",
       "3                   0.322538  \n",
       "4                   0.322549  \n",
       "..                       ...  \n",
       "95                  0.322547  \n",
       "96                  0.322586  \n",
       "97                  0.322581  \n",
       "98                  0.322534  \n",
       "99                  0.322550  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "lasso_labs = abt_cpy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso_feats = abt_cpy.drop(['lab'], axis=1)\n",
    "lasso_df_feats = SelectFromModel(max_features = numFeat, estimator = LassoLars( alpha = 0, eps =1)).fit(lasso_feats, lasso_labs)\n",
    "lasso_train = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_train_set = pd.concat([abt['lab'], lasso_train], axis = 1)\n",
    "lasso_train_set\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_dderivative_mean</th>\n",
       "      <th>TOTUSJZ_gderivative_mean</th>\n",
       "      <th>SAVNCPP_dderivative_mean</th>\n",
       "      <th>MEANJZD_kurtosis</th>\n",
       "      <th>TOTFX_kurtosis</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANGAM_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANALP_min</th>\n",
       "      <th>...</th>\n",
       "      <th>MEANPOT_difference_of_maxs</th>\n",
       "      <th>MEANPOT_stddev</th>\n",
       "      <th>MEANPOT_last_value</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_increase</th>\n",
       "      <th>MEANPOT_difference_of_means</th>\n",
       "      <th>MEANPOT_avg_mono_decrease_slope</th>\n",
       "      <th>MEANPOT_difference_of_stds</th>\n",
       "      <th>MEANPOT_dderivative_mean</th>\n",
       "      <th>MEANPOT_gderivative_stddev</th>\n",
       "      <th>MEANPOT_gderivative_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.979171</td>\n",
       "      <td>0.979249</td>\n",
       "      <td>0.976164</td>\n",
       "      <td>0.037609</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.999789</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>0.995406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>8.113658e-06</td>\n",
       "      <td>0.351491</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.322541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.980098</td>\n",
       "      <td>0.979847</td>\n",
       "      <td>0.978972</td>\n",
       "      <td>0.037284</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.999506</td>\n",
       "      <td>0.998619</td>\n",
       "      <td>0.995912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>2.846932e-05</td>\n",
       "      <td>0.351517</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.322561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.979165</td>\n",
       "      <td>0.979242</td>\n",
       "      <td>0.979138</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.040059</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>0.996573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>5.385364e-06</td>\n",
       "      <td>0.351505</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.322553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.977933</td>\n",
       "      <td>0.978422</td>\n",
       "      <td>0.973676</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.999843</td>\n",
       "      <td>0.996432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>7.232174e-06</td>\n",
       "      <td>0.351488</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.322538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>0.979897</td>\n",
       "      <td>0.979740</td>\n",
       "      <td>0.976483</td>\n",
       "      <td>0.034652</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.998720</td>\n",
       "      <td>0.996480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>7.075361e-07</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.322549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>X</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.982453</td>\n",
       "      <td>0.981571</td>\n",
       "      <td>0.981398</td>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.034746</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>1.695746e-05</td>\n",
       "      <td>0.351499</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.322547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>X</td>\n",
       "      <td>0.998882</td>\n",
       "      <td>0.980923</td>\n",
       "      <td>0.980462</td>\n",
       "      <td>0.981911</td>\n",
       "      <td>0.045091</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.999484</td>\n",
       "      <td>0.998356</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.999884</td>\n",
       "      <td>3.298372e-05</td>\n",
       "      <td>0.351538</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.322586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>X</td>\n",
       "      <td>0.998882</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>0.981080</td>\n",
       "      <td>0.979000</td>\n",
       "      <td>0.036840</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>2.544730e-06</td>\n",
       "      <td>0.351536</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>X</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.982395</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>0.979225</td>\n",
       "      <td>0.053946</td>\n",
       "      <td>0.027424</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>2.842715e-06</td>\n",
       "      <td>0.351485</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.322534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>X</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.978730</td>\n",
       "      <td>0.978976</td>\n",
       "      <td>0.979790</td>\n",
       "      <td>0.049728</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.999130</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.996816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>3.112438e-06</td>\n",
       "      <td>0.351504</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.322550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab  TOTBSQ_slope_of_longest_mono_decrease  TOTUSJZ_dderivative_mean  \\\n",
       "0    B                               0.999990                  0.979171   \n",
       "1    B                               0.999980                  0.980098   \n",
       "2    B                               0.999849                  0.979165   \n",
       "3    B                               0.999559                  0.977933   \n",
       "4    B                               0.999913                  0.979897   \n",
       "..  ..                                    ...                       ...   \n",
       "95   X                               0.999694                  0.982453   \n",
       "96   X                               0.998882                  0.980923   \n",
       "97   X                               0.998882                  0.981685   \n",
       "98   X                               0.999694                  0.982395   \n",
       "99   X                               0.999244                  0.978730   \n",
       "\n",
       "    TOTUSJZ_gderivative_mean  SAVNCPP_dderivative_mean  MEANJZD_kurtosis  \\\n",
       "0                   0.979249                  0.976164          0.037609   \n",
       "1                   0.979847                  0.978972          0.037284   \n",
       "2                   0.979242                  0.979138          0.040767   \n",
       "3                   0.978422                  0.973676          0.040877   \n",
       "4                   0.979740                  0.976483          0.034652   \n",
       "..                       ...                       ...               ...   \n",
       "95                  0.981571                  0.981398          0.050595   \n",
       "96                  0.980462                  0.981911          0.045091   \n",
       "97                  0.981080                  0.979000          0.036840   \n",
       "98                  0.981400                  0.979225          0.053946   \n",
       "99                  0.978976                  0.979790          0.049728   \n",
       "\n",
       "    TOTFX_kurtosis  MEANPOT_slope_of_longest_mono_decrease  \\\n",
       "0         0.036443                                0.999789   \n",
       "1         0.033039                                0.999506   \n",
       "2         0.040059                                0.999771   \n",
       "3         0.035849                                0.999849   \n",
       "4         0.027600                                0.999652   \n",
       "..             ...                                     ...   \n",
       "95        0.034746                                0.999897   \n",
       "96        0.027407                                0.999484   \n",
       "97        0.027419                                0.998950   \n",
       "98        0.027424                                0.999897   \n",
       "99        0.029754                                0.999130   \n",
       "\n",
       "    MEANGAM_slope_of_longest_mono_decrease  MEANALP_min  ...  \\\n",
       "0                                 0.997375     0.995406  ...   \n",
       "1                                 0.998619     0.995912  ...   \n",
       "2                                 0.999498     0.996573  ...   \n",
       "3                                 0.999843     0.996432  ...   \n",
       "4                                 0.998720     0.996480  ...   \n",
       "..                                     ...          ...  ...   \n",
       "95                                0.998922     0.995805  ...   \n",
       "96                                0.998356     0.995951  ...   \n",
       "97                                0.997506     0.995951  ...   \n",
       "98                                0.998922     0.995805  ...   \n",
       "99                                0.999872     0.996816  ...   \n",
       "\n",
       "    MEANPOT_difference_of_maxs  MEANPOT_stddev  MEANPOT_last_value  \\\n",
       "0                     0.000003        0.000023            0.000072   \n",
       "1                     0.000031        0.000073            0.000113   \n",
       "2                     0.000002        0.000017            0.000058   \n",
       "3                     0.000004        0.000018            0.000082   \n",
       "4                     0.000015        0.000051            0.000188   \n",
       "..                         ...             ...                 ...   \n",
       "95                    0.000004        0.000038            0.000259   \n",
       "96                    0.000008        0.000095            0.000351   \n",
       "97                    0.000028        0.000098            0.000339   \n",
       "98                    0.000017        0.000042            0.000253   \n",
       "99                    0.000002        0.000049            0.000196   \n",
       "\n",
       "    MEANPOT_slope_of_longest_mono_increase  MEANPOT_difference_of_means  \\\n",
       "0                                 0.000007                     0.000043   \n",
       "1                                 0.000013                     0.000096   \n",
       "2                                 0.000002                     0.000017   \n",
       "3                                 0.000016                     0.000041   \n",
       "4                                 0.000019                     0.000125   \n",
       "..                                     ...                          ...   \n",
       "95                                0.000008                     0.000024   \n",
       "96                                0.000035                     0.000212   \n",
       "97                                0.000035                     0.000248   \n",
       "98                                0.000009                     0.000059   \n",
       "99                                0.000019                     0.000035   \n",
       "\n",
       "    MEANPOT_avg_mono_decrease_slope  MEANPOT_difference_of_stds  \\\n",
       "0                          0.999974                8.113658e-06   \n",
       "1                          0.999944                2.846932e-05   \n",
       "2                          0.999989                5.385364e-06   \n",
       "3                          0.999983                7.232174e-06   \n",
       "4                          0.999952                7.075361e-07   \n",
       "..                              ...                         ...   \n",
       "95                         0.999957                1.695746e-05   \n",
       "96                         0.999884                3.298372e-05   \n",
       "97                         0.999889                2.544730e-06   \n",
       "98                         0.999952                2.842715e-06   \n",
       "99                         0.999960                3.112438e-06   \n",
       "\n",
       "    MEANPOT_dderivative_mean  MEANPOT_gderivative_stddev  \\\n",
       "0                   0.351491                    0.000014   \n",
       "1                   0.351517                    0.000032   \n",
       "2                   0.351505                    0.000007   \n",
       "3                   0.351488                    0.000014   \n",
       "4                   0.351500                    0.000021   \n",
       "..                       ...                         ...   \n",
       "95                  0.351499                    0.000021   \n",
       "96                  0.351538                    0.000051   \n",
       "97                  0.351536                    0.000051   \n",
       "98                  0.351485                    0.000023   \n",
       "99                  0.351504                    0.000033   \n",
       "\n",
       "    MEANPOT_gderivative_mean  \n",
       "0                   0.322541  \n",
       "1                   0.322561  \n",
       "2                   0.322553  \n",
       "3                   0.322538  \n",
       "4                   0.322549  \n",
       "..                       ...  \n",
       "95                  0.322547  \n",
       "96                  0.322586  \n",
       "97                  0.322581  \n",
       "98                  0.322534  \n",
       "99                  0.322550  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso2_labs = abt2_cpy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso2_feats = abt2_cpy.drop(['lab'], axis =1)\n",
    "lasso_df_test_feats = lasso2_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test_set = pd.concat([abt2['lab'], lasso_test], axis = 1)\n",
    "lasso_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2 (5 points)\n",
    "\n",
    "In this question, you will again perform the feature selection task [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). However, this time, you will be utililizing a random forest model called [ExtraTressClssifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) as the `estimator`. \n",
    "\n",
    "You need set the `max_features` of the SelectFromModel object to the number of features we are going to select (20 features). \n",
    "\n",
    "You also need to set the `n_estimators` of the random forest algorithm to `75` when you construct it.\n",
    "\n",
    "**Note:** This method allows you to utilize our string class labels, so you don't need to map the lables to any other values. You can use the labels that were used in the original example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_linear_weighted_average</th>\n",
       "      <th>TOTPOT_mean</th>\n",
       "      <th>ABSNJZH_var</th>\n",
       "      <th>SAVNCPP_min</th>\n",
       "      <th>SAVNCPP_median</th>\n",
       "      <th>SAVNCPP_var</th>\n",
       "      <th>SAVNCPP_difference_of_medians</th>\n",
       "      <th>SAVNCPP_quadratic_weighted_average</th>\n",
       "      <th>SAVNCPP_last_value</th>\n",
       "      <th>...</th>\n",
       "      <th>USFLUX_skewness</th>\n",
       "      <th>TOTFX_kurtosis</th>\n",
       "      <th>R_VALUE_mean</th>\n",
       "      <th>R_VALUE_last_value</th>\n",
       "      <th>TOTUSJH_min</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>ABSNJZH_difference_of_maxs</th>\n",
       "      <th>MEANSHR_linear_weighted_average</th>\n",
       "      <th>MEANGBH_average_absolute_derivative_change</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.857535</td>\n",
       "      <td>0.888655</td>\n",
       "      <td>0.600188</td>\n",
       "      <td>0.867919</td>\n",
       "      <td>0.893334</td>\n",
       "      <td>0.901466</td>\n",
       "      <td>0.886079</td>\n",
       "      <td>0.890066</td>\n",
       "      <td>0.882146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466460</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.535726</td>\n",
       "      <td>0.497707</td>\n",
       "      <td>0.145829</td>\n",
       "      <td>0.197777</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>0.707192</td>\n",
       "      <td>0.294229</td>\n",
       "      <td>0.027841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.843869</td>\n",
       "      <td>0.882247</td>\n",
       "      <td>0.680944</td>\n",
       "      <td>0.825839</td>\n",
       "      <td>0.897194</td>\n",
       "      <td>0.928598</td>\n",
       "      <td>0.933825</td>\n",
       "      <td>0.903081</td>\n",
       "      <td>0.907012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471575</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.458838</td>\n",
       "      <td>0.593018</td>\n",
       "      <td>0.101915</td>\n",
       "      <td>0.274373</td>\n",
       "      <td>0.230947</td>\n",
       "      <td>0.843157</td>\n",
       "      <td>0.442534</td>\n",
       "      <td>0.036982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.906763</td>\n",
       "      <td>0.911793</td>\n",
       "      <td>0.625489</td>\n",
       "      <td>0.895776</td>\n",
       "      <td>0.911852</td>\n",
       "      <td>0.925663</td>\n",
       "      <td>0.911748</td>\n",
       "      <td>0.914137</td>\n",
       "      <td>0.920578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526223</td>\n",
       "      <td>0.040059</td>\n",
       "      <td>0.575241</td>\n",
       "      <td>0.497019</td>\n",
       "      <td>0.279344</td>\n",
       "      <td>0.219138</td>\n",
       "      <td>0.074247</td>\n",
       "      <td>0.688024</td>\n",
       "      <td>0.177295</td>\n",
       "      <td>0.059151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.932182</td>\n",
       "      <td>0.541169</td>\n",
       "      <td>0.924288</td>\n",
       "      <td>0.931340</td>\n",
       "      <td>0.929003</td>\n",
       "      <td>0.918959</td>\n",
       "      <td>0.928458</td>\n",
       "      <td>0.921168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495887</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.879914</td>\n",
       "      <td>0.890073</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.155693</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.674857</td>\n",
       "      <td>0.168761</td>\n",
       "      <td>0.095279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.926448</td>\n",
       "      <td>0.929374</td>\n",
       "      <td>0.593376</td>\n",
       "      <td>0.813905</td>\n",
       "      <td>0.890793</td>\n",
       "      <td>0.920634</td>\n",
       "      <td>0.911303</td>\n",
       "      <td>0.888427</td>\n",
       "      <td>0.897998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439050</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.802363</td>\n",
       "      <td>0.780744</td>\n",
       "      <td>0.317649</td>\n",
       "      <td>0.192390</td>\n",
       "      <td>0.138257</td>\n",
       "      <td>0.857433</td>\n",
       "      <td>0.185685</td>\n",
       "      <td>0.037962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>X</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.948648</td>\n",
       "      <td>0.817282</td>\n",
       "      <td>0.971766</td>\n",
       "      <td>0.976454</td>\n",
       "      <td>0.961077</td>\n",
       "      <td>0.950802</td>\n",
       "      <td>0.976761</td>\n",
       "      <td>0.975462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490344</td>\n",
       "      <td>0.034746</td>\n",
       "      <td>0.921229</td>\n",
       "      <td>0.927977</td>\n",
       "      <td>0.518692</td>\n",
       "      <td>0.476814</td>\n",
       "      <td>0.365403</td>\n",
       "      <td>0.849314</td>\n",
       "      <td>0.171691</td>\n",
       "      <td>0.429985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>X</td>\n",
       "      <td>0.986462</td>\n",
       "      <td>0.960412</td>\n",
       "      <td>0.981117</td>\n",
       "      <td>0.903731</td>\n",
       "      <td>0.956255</td>\n",
       "      <td>0.987593</td>\n",
       "      <td>0.949146</td>\n",
       "      <td>0.956666</td>\n",
       "      <td>0.960427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425749</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.976325</td>\n",
       "      <td>0.959694</td>\n",
       "      <td>0.733205</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.531832</td>\n",
       "      <td>0.849787</td>\n",
       "      <td>0.212918</td>\n",
       "      <td>0.320873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>X</td>\n",
       "      <td>0.985814</td>\n",
       "      <td>0.959869</td>\n",
       "      <td>0.979179</td>\n",
       "      <td>0.903731</td>\n",
       "      <td>0.954739</td>\n",
       "      <td>0.988355</td>\n",
       "      <td>0.904570</td>\n",
       "      <td>0.954578</td>\n",
       "      <td>0.958772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451961</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.979155</td>\n",
       "      <td>0.962338</td>\n",
       "      <td>0.733205</td>\n",
       "      <td>0.919066</td>\n",
       "      <td>0.531832</td>\n",
       "      <td>0.847687</td>\n",
       "      <td>0.213173</td>\n",
       "      <td>0.320873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>X</td>\n",
       "      <td>0.966694</td>\n",
       "      <td>0.948479</td>\n",
       "      <td>0.802219</td>\n",
       "      <td>0.971766</td>\n",
       "      <td>0.976107</td>\n",
       "      <td>0.961561</td>\n",
       "      <td>0.956618</td>\n",
       "      <td>0.976678</td>\n",
       "      <td>0.975806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511945</td>\n",
       "      <td>0.027424</td>\n",
       "      <td>0.920955</td>\n",
       "      <td>0.930951</td>\n",
       "      <td>0.513776</td>\n",
       "      <td>0.448571</td>\n",
       "      <td>0.350938</td>\n",
       "      <td>0.850064</td>\n",
       "      <td>0.180257</td>\n",
       "      <td>0.429985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>X</td>\n",
       "      <td>0.933507</td>\n",
       "      <td>0.934946</td>\n",
       "      <td>0.691016</td>\n",
       "      <td>0.933320</td>\n",
       "      <td>0.941395</td>\n",
       "      <td>0.941069</td>\n",
       "      <td>0.921602</td>\n",
       "      <td>0.943121</td>\n",
       "      <td>0.942193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499880</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.770904</td>\n",
       "      <td>0.756027</td>\n",
       "      <td>0.424058</td>\n",
       "      <td>0.285805</td>\n",
       "      <td>0.249056</td>\n",
       "      <td>0.850931</td>\n",
       "      <td>0.176458</td>\n",
       "      <td>0.132776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab  TOTBSQ_linear_weighted_average  TOTPOT_mean  ABSNJZH_var  SAVNCPP_min  \\\n",
       "0    B                        0.857535     0.888655     0.600188     0.867919   \n",
       "1    B                        0.843869     0.882247     0.680944     0.825839   \n",
       "2    B                        0.906763     0.911793     0.625489     0.895776   \n",
       "3    B                        0.944800     0.932182     0.541169     0.924288   \n",
       "4    B                        0.926448     0.929374     0.593376     0.813905   \n",
       "..  ..                             ...          ...          ...          ...   \n",
       "95   X                        0.967341     0.948648     0.817282     0.971766   \n",
       "96   X                        0.986462     0.960412     0.981117     0.903731   \n",
       "97   X                        0.985814     0.959869     0.979179     0.903731   \n",
       "98   X                        0.966694     0.948479     0.802219     0.971766   \n",
       "99   X                        0.933507     0.934946     0.691016     0.933320   \n",
       "\n",
       "    SAVNCPP_median  SAVNCPP_var  SAVNCPP_difference_of_medians  \\\n",
       "0         0.893334     0.901466                       0.886079   \n",
       "1         0.897194     0.928598                       0.933825   \n",
       "2         0.911852     0.925663                       0.911748   \n",
       "3         0.931340     0.929003                       0.918959   \n",
       "4         0.890793     0.920634                       0.911303   \n",
       "..             ...          ...                            ...   \n",
       "95        0.976454     0.961077                       0.950802   \n",
       "96        0.956255     0.987593                       0.949146   \n",
       "97        0.954739     0.988355                       0.904570   \n",
       "98        0.976107     0.961561                       0.956618   \n",
       "99        0.941395     0.941069                       0.921602   \n",
       "\n",
       "    SAVNCPP_quadratic_weighted_average  SAVNCPP_last_value  ...  \\\n",
       "0                             0.890066            0.882146  ...   \n",
       "1                             0.903081            0.907012  ...   \n",
       "2                             0.914137            0.920578  ...   \n",
       "3                             0.928458            0.921168  ...   \n",
       "4                             0.888427            0.897998  ...   \n",
       "..                                 ...                 ...  ...   \n",
       "95                            0.976761            0.975462  ...   \n",
       "96                            0.956666            0.960427  ...   \n",
       "97                            0.954578            0.958772  ...   \n",
       "98                            0.976678            0.975806  ...   \n",
       "99                            0.943121            0.942193  ...   \n",
       "\n",
       "    USFLUX_skewness  TOTFX_kurtosis  R_VALUE_mean  R_VALUE_last_value  \\\n",
       "0          0.466460        0.036443      0.535726            0.497707   \n",
       "1          0.471575        0.033039      0.458838            0.593018   \n",
       "2          0.526223        0.040059      0.575241            0.497019   \n",
       "3          0.495887        0.035849      0.879914            0.890073   \n",
       "4          0.439050        0.027600      0.802363            0.780744   \n",
       "..              ...             ...           ...                 ...   \n",
       "95         0.490344        0.034746      0.921229            0.927977   \n",
       "96         0.425749        0.027407      0.976325            0.959694   \n",
       "97         0.451961        0.027419      0.979155            0.962338   \n",
       "98         0.511945        0.027424      0.920955            0.930951   \n",
       "99         0.499880        0.029754      0.770904            0.756027   \n",
       "\n",
       "    TOTUSJH_min  ABSNJZH_stddev  ABSNJZH_difference_of_maxs  \\\n",
       "0      0.145829        0.197777                    0.060871   \n",
       "1      0.101915        0.274373                    0.230947   \n",
       "2      0.279344        0.219138                    0.074247   \n",
       "3      0.379392        0.155693                    0.066391   \n",
       "4      0.317649        0.192390                    0.138257   \n",
       "..          ...             ...                         ...   \n",
       "95     0.518692        0.476814                    0.365403   \n",
       "96     0.733205        0.926316                    0.531832   \n",
       "97     0.733205        0.919066                    0.531832   \n",
       "98     0.513776        0.448571                    0.350938   \n",
       "99     0.424058        0.285805                    0.249056   \n",
       "\n",
       "    MEANSHR_linear_weighted_average  \\\n",
       "0                          0.707192   \n",
       "1                          0.843157   \n",
       "2                          0.688024   \n",
       "3                          0.674857   \n",
       "4                          0.857433   \n",
       "..                              ...   \n",
       "95                         0.849314   \n",
       "96                         0.849787   \n",
       "97                         0.847687   \n",
       "98                         0.850064   \n",
       "99                         0.850931   \n",
       "\n",
       "    MEANGBH_average_absolute_derivative_change  SAVNCPP_max  \n",
       "0                                     0.294229     0.027841  \n",
       "1                                     0.442534     0.036982  \n",
       "2                                     0.177295     0.059151  \n",
       "3                                     0.168761     0.095279  \n",
       "4                                     0.185685     0.037962  \n",
       "..                                         ...          ...  \n",
       "95                                    0.171691     0.429985  \n",
       "96                                    0.212918     0.320873  \n",
       "97                                    0.213173     0.320873  \n",
       "98                                    0.180257     0.429985  \n",
       "99                                    0.176458     0.132776  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrees_labels = abt_cpy['lab']\n",
    "xtrees_features = abt_cpy.drop(['lab'], axis = 1)\n",
    "xtrees_model_features = SelectFromModel(max_features = numFeat, estimator = ExtraTreesClassifier(n_estimators = 75)).fit(xtrees_features, xtrees_labels)\n",
    "xtrees_selected_features = xtrees_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_train = pd.concat([xtrees_labels, xtrees_selected_features], axis = 1)\n",
    "xtrees_train\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_linear_weighted_average</th>\n",
       "      <th>TOTPOT_mean</th>\n",
       "      <th>ABSNJZH_var</th>\n",
       "      <th>SAVNCPP_min</th>\n",
       "      <th>SAVNCPP_median</th>\n",
       "      <th>SAVNCPP_var</th>\n",
       "      <th>SAVNCPP_difference_of_medians</th>\n",
       "      <th>SAVNCPP_quadratic_weighted_average</th>\n",
       "      <th>SAVNCPP_last_value</th>\n",
       "      <th>...</th>\n",
       "      <th>USFLUX_skewness</th>\n",
       "      <th>TOTFX_kurtosis</th>\n",
       "      <th>R_VALUE_mean</th>\n",
       "      <th>R_VALUE_last_value</th>\n",
       "      <th>TOTUSJH_min</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>ABSNJZH_difference_of_maxs</th>\n",
       "      <th>MEANSHR_linear_weighted_average</th>\n",
       "      <th>MEANGBH_average_absolute_derivative_change</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.891868</td>\n",
       "      <td>0.904676</td>\n",
       "      <td>0.544861</td>\n",
       "      <td>0.826002</td>\n",
       "      <td>0.875710</td>\n",
       "      <td>0.905469</td>\n",
       "      <td>0.871146</td>\n",
       "      <td>0.879440</td>\n",
       "      <td>0.884478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487745</td>\n",
       "      <td>0.028188</td>\n",
       "      <td>0.679219</td>\n",
       "      <td>0.677843</td>\n",
       "      <td>0.212863</td>\n",
       "      <td>0.158041</td>\n",
       "      <td>0.024713</td>\n",
       "      <td>0.649360</td>\n",
       "      <td>0.146881</td>\n",
       "      <td>0.021409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.907375</td>\n",
       "      <td>0.916737</td>\n",
       "      <td>0.622084</td>\n",
       "      <td>0.884346</td>\n",
       "      <td>0.911267</td>\n",
       "      <td>0.921677</td>\n",
       "      <td>0.852440</td>\n",
       "      <td>0.909503</td>\n",
       "      <td>0.901160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441909</td>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.694106</td>\n",
       "      <td>0.682567</td>\n",
       "      <td>0.277444</td>\n",
       "      <td>0.216134</td>\n",
       "      <td>0.056961</td>\n",
       "      <td>0.722890</td>\n",
       "      <td>0.148309</td>\n",
       "      <td>0.047650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.930710</td>\n",
       "      <td>0.929911</td>\n",
       "      <td>0.722335</td>\n",
       "      <td>0.927232</td>\n",
       "      <td>0.935240</td>\n",
       "      <td>0.946928</td>\n",
       "      <td>0.944892</td>\n",
       "      <td>0.940307</td>\n",
       "      <td>0.942041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465092</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.621892</td>\n",
       "      <td>0.631455</td>\n",
       "      <td>0.365649</td>\n",
       "      <td>0.324493</td>\n",
       "      <td>0.287077</td>\n",
       "      <td>0.828676</td>\n",
       "      <td>0.183252</td>\n",
       "      <td>0.123288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.916118</td>\n",
       "      <td>0.922231</td>\n",
       "      <td>0.585993</td>\n",
       "      <td>0.846077</td>\n",
       "      <td>0.887077</td>\n",
       "      <td>0.916147</td>\n",
       "      <td>0.897691</td>\n",
       "      <td>0.887924</td>\n",
       "      <td>0.863439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454938</td>\n",
       "      <td>0.033735</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.721790</td>\n",
       "      <td>0.269404</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>0.087503</td>\n",
       "      <td>0.757868</td>\n",
       "      <td>0.149620</td>\n",
       "      <td>0.027869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.879534</td>\n",
       "      <td>0.895181</td>\n",
       "      <td>0.606894</td>\n",
       "      <td>0.891314</td>\n",
       "      <td>0.908367</td>\n",
       "      <td>0.914248</td>\n",
       "      <td>0.757929</td>\n",
       "      <td>0.907221</td>\n",
       "      <td>0.908226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536629</td>\n",
       "      <td>0.038919</td>\n",
       "      <td>0.380567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229202</td>\n",
       "      <td>0.203227</td>\n",
       "      <td>0.120954</td>\n",
       "      <td>0.652698</td>\n",
       "      <td>0.212713</td>\n",
       "      <td>0.045467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>X</td>\n",
       "      <td>0.995461</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.771674</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.964465</td>\n",
       "      <td>0.934043</td>\n",
       "      <td>0.986907</td>\n",
       "      <td>0.987173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621232</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>0.872821</td>\n",
       "      <td>0.871368</td>\n",
       "      <td>0.885666</td>\n",
       "      <td>0.396334</td>\n",
       "      <td>0.326953</td>\n",
       "      <td>0.881194</td>\n",
       "      <td>0.185888</td>\n",
       "      <td>0.682312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>X</td>\n",
       "      <td>0.977947</td>\n",
       "      <td>0.954518</td>\n",
       "      <td>0.826597</td>\n",
       "      <td>0.966643</td>\n",
       "      <td>0.973606</td>\n",
       "      <td>0.972932</td>\n",
       "      <td>0.968006</td>\n",
       "      <td>0.974619</td>\n",
       "      <td>0.975578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412339</td>\n",
       "      <td>0.033952</td>\n",
       "      <td>0.918191</td>\n",
       "      <td>0.917902</td>\n",
       "      <td>0.634410</td>\n",
       "      <td>0.495160</td>\n",
       "      <td>0.228014</td>\n",
       "      <td>0.781405</td>\n",
       "      <td>0.141181</td>\n",
       "      <td>0.405530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>X</td>\n",
       "      <td>0.984968</td>\n",
       "      <td>0.958541</td>\n",
       "      <td>0.882727</td>\n",
       "      <td>0.981816</td>\n",
       "      <td>0.985870</td>\n",
       "      <td>0.979513</td>\n",
       "      <td>0.976734</td>\n",
       "      <td>0.987651</td>\n",
       "      <td>0.990187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508683</td>\n",
       "      <td>0.046716</td>\n",
       "      <td>0.952882</td>\n",
       "      <td>0.948928</td>\n",
       "      <td>0.870969</td>\n",
       "      <td>0.621664</td>\n",
       "      <td>0.574351</td>\n",
       "      <td>0.812522</td>\n",
       "      <td>0.132968</td>\n",
       "      <td>0.658544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>X</td>\n",
       "      <td>0.975771</td>\n",
       "      <td>0.953294</td>\n",
       "      <td>0.859894</td>\n",
       "      <td>0.965378</td>\n",
       "      <td>0.972216</td>\n",
       "      <td>0.968952</td>\n",
       "      <td>0.952973</td>\n",
       "      <td>0.969830</td>\n",
       "      <td>0.966993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521756</td>\n",
       "      <td>0.034015</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>0.906332</td>\n",
       "      <td>0.629267</td>\n",
       "      <td>0.566709</td>\n",
       "      <td>0.132315</td>\n",
       "      <td>0.784807</td>\n",
       "      <td>0.138522</td>\n",
       "      <td>0.386208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>X</td>\n",
       "      <td>0.972842</td>\n",
       "      <td>0.951657</td>\n",
       "      <td>0.717814</td>\n",
       "      <td>0.972312</td>\n",
       "      <td>0.973636</td>\n",
       "      <td>0.949732</td>\n",
       "      <td>0.927473</td>\n",
       "      <td>0.973998</td>\n",
       "      <td>0.972545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501455</td>\n",
       "      <td>0.027303</td>\n",
       "      <td>0.879228</td>\n",
       "      <td>0.886554</td>\n",
       "      <td>0.625602</td>\n",
       "      <td>0.318601</td>\n",
       "      <td>0.197372</td>\n",
       "      <td>0.797780</td>\n",
       "      <td>0.134234</td>\n",
       "      <td>0.419809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lab  TOTBSQ_linear_weighted_average  TOTPOT_mean  ABSNJZH_var  SAVNCPP_min  \\\n",
       "0    B                        0.891868     0.904676     0.544861     0.826002   \n",
       "1    B                        0.907375     0.916737     0.622084     0.884346   \n",
       "2    B                        0.930710     0.929911     0.722335     0.927232   \n",
       "3    B                        0.916118     0.922231     0.585993     0.846077   \n",
       "4    B                        0.879534     0.895181     0.606894     0.891314   \n",
       "..  ..                             ...          ...          ...          ...   \n",
       "95   X                        0.995461     0.965111     0.771674     0.986181   \n",
       "96   X                        0.977947     0.954518     0.826597     0.966643   \n",
       "97   X                        0.984968     0.958541     0.882727     0.981816   \n",
       "98   X                        0.975771     0.953294     0.859894     0.965378   \n",
       "99   X                        0.972842     0.951657     0.717814     0.972312   \n",
       "\n",
       "    SAVNCPP_median  SAVNCPP_var  SAVNCPP_difference_of_medians  \\\n",
       "0         0.875710     0.905469                       0.871146   \n",
       "1         0.911267     0.921677                       0.852440   \n",
       "2         0.935240     0.946928                       0.944892   \n",
       "3         0.887077     0.916147                       0.897691   \n",
       "4         0.908367     0.914248                       0.757929   \n",
       "..             ...          ...                            ...   \n",
       "95        0.987464     0.964465                       0.934043   \n",
       "96        0.973606     0.972932                       0.968006   \n",
       "97        0.985870     0.979513                       0.976734   \n",
       "98        0.972216     0.968952                       0.952973   \n",
       "99        0.973636     0.949732                       0.927473   \n",
       "\n",
       "    SAVNCPP_quadratic_weighted_average  SAVNCPP_last_value  ...  \\\n",
       "0                             0.879440            0.884478  ...   \n",
       "1                             0.909503            0.901160  ...   \n",
       "2                             0.940307            0.942041  ...   \n",
       "3                             0.887924            0.863439  ...   \n",
       "4                             0.907221            0.908226  ...   \n",
       "..                                 ...                 ...  ...   \n",
       "95                            0.986907            0.987173  ...   \n",
       "96                            0.974619            0.975578  ...   \n",
       "97                            0.987651            0.990187  ...   \n",
       "98                            0.969830            0.966993  ...   \n",
       "99                            0.973998            0.972545  ...   \n",
       "\n",
       "    USFLUX_skewness  TOTFX_kurtosis  R_VALUE_mean  R_VALUE_last_value  \\\n",
       "0          0.487745        0.028188      0.679219            0.677843   \n",
       "1          0.441909        0.027338      0.694106            0.682567   \n",
       "2          0.465092        0.029043      0.621892            0.631455   \n",
       "3          0.454938        0.033735      0.730323            0.721790   \n",
       "4          0.536629        0.038919      0.380567            0.000000   \n",
       "..              ...             ...           ...                 ...   \n",
       "95         0.621232        0.030347      0.872821            0.871368   \n",
       "96         0.412339        0.033952      0.918191            0.917902   \n",
       "97         0.508683        0.046716      0.952882            0.948928   \n",
       "98         0.521756        0.034015      0.909167            0.906332   \n",
       "99         0.501455        0.027303      0.879228            0.886554   \n",
       "\n",
       "    TOTUSJH_min  ABSNJZH_stddev  ABSNJZH_difference_of_maxs  \\\n",
       "0      0.212863        0.158041                    0.024713   \n",
       "1      0.277444        0.216134                    0.056961   \n",
       "2      0.365649        0.324493                    0.287077   \n",
       "3      0.269404        0.186718                    0.087503   \n",
       "4      0.229202        0.203227                    0.120954   \n",
       "..          ...             ...                         ...   \n",
       "95     0.885666        0.396334                    0.326953   \n",
       "96     0.634410        0.495160                    0.228014   \n",
       "97     0.870969        0.621664                    0.574351   \n",
       "98     0.629267        0.566709                    0.132315   \n",
       "99     0.625602        0.318601                    0.197372   \n",
       "\n",
       "    MEANSHR_linear_weighted_average  \\\n",
       "0                          0.649360   \n",
       "1                          0.722890   \n",
       "2                          0.828676   \n",
       "3                          0.757868   \n",
       "4                          0.652698   \n",
       "..                              ...   \n",
       "95                         0.881194   \n",
       "96                         0.781405   \n",
       "97                         0.812522   \n",
       "98                         0.784807   \n",
       "99                         0.797780   \n",
       "\n",
       "    MEANGBH_average_absolute_derivative_change  SAVNCPP_max  \n",
       "0                                     0.146881     0.021409  \n",
       "1                                     0.148309     0.047650  \n",
       "2                                     0.183252     0.123288  \n",
       "3                                     0.149620     0.027869  \n",
       "4                                     0.212713     0.045467  \n",
       "..                                         ...          ...  \n",
       "95                                    0.185888     0.682312  \n",
       "96                                    0.141181     0.405530  \n",
       "97                                    0.132968     0.658544  \n",
       "98                                    0.138522     0.386208  \n",
       "99                                    0.134234     0.419809  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrees_test_labels = abt2_cpy['lab']\n",
    "xtrees_test_features = abt2_cpy.drop(['lab'], axis = 1)\n",
    "xtrees_test_selected = xtrees_test_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_test = pd.concat([xtrees_test_labels, xtrees_test_selected], axis = 1)\n",
    "xtrees_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3 (5 points)\n",
    "\n",
    "Now that you have three different datasets, you need to convert them each to a binary classification problem datase or dichotomize the training and testing data. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed with the feature selected training and testing data from the exmpale, Q1, and Q2.\n",
    "\n",
    "**Note:** You might want to put the training and testing tuples you get from the call to the dichotomize method into seperate training and testing lists. Then you can loop over them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "train = [df_train_set1.copy(), lasso_train_set.copy(), xtrees_train.copy()]\n",
    "test = [df_test_set1.copy(), lasso_test_set.copy(), xtrees_test.copy()]\n",
    "di_train_x1 = []\n",
    "di_train_y1 = []\n",
    "di_test_x2 = []\n",
    "di_test_y2 = []\n",
    "\n",
    "for set1 in train:\n",
    "    x1, y1 = dichotomize_X_y(set1)\n",
    "    di_train_x1.append(x1)\n",
    "    di_train_y1.append(y1)\n",
    "\n",
    "for set2 in test:\n",
    "    x2, y2 = dichotomize_X_y(set2)\n",
    "    di_test_x2.append(x2)\n",
    "    di_test_y2.append(y2)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4 (10 points)\n",
    "\n",
    "Now that you have your data setup, you will be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. For this exercise default regularization parameter `C` value of 1.0, the default `kernel` type of `rbf`, and the default setting of the kernel coefficient `gamma` for the `rbf` kernel. You should, however, set the `class_weight` to `balanced` when you construct your models. This way the regularization parameter is adjusted for each class in proportion the occurrence of that class in the dataset.\n",
    "\n",
    "You should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out.\n",
    "\n",
    "**Note:** for more information on what the `C` and `gamma` parameters do on the `rbf` kernel see the [RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) documentation. We won't be tuning these values in this question, but it is genearally accepted that tuning should be done to find the best performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['F-Val', 'FromLasso', 'FromForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "F-Val\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "FromLasso\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=44\tFP=16\tFN=1\tTP=39\n",
      "FromForest\n",
      "TSS: 0.7083333333333333\n",
      "HSS: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "for ind in range(len(selected_labels)):\n",
    "     classifier = SVC(class_weight = 'balanced')\n",
    "     classifier.fit(di_train_x1[ind],di_train_y1[ind])\n",
    "     y_pred = classifier.predict(di_test_x2[ind])\n",
    "     \n",
    "     tss_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "     hss_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "\n",
    "     print(selected_labels[ind])\n",
    "     print(f\"TSS: {tss_score}\")\n",
    "     print(f\"HSS: {hss_score}\")\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5 (15 points)\n",
    "\n",
    "After training and testing the SVC model on the above dataset, you will likely see that this process is quite time consuming. This is because the algorithm needs evaluate all the instances in the training dataset to find instances that can be used as points in a separating hyperplane between the samples of different classes.  \n",
    "\n",
    "In order to speed this process, lets reduce the number of samples in the dataset through undersampling the classes like we did before. Unlike was done before, where we just pick some random sample of instances in the various classes, we will be performing some data informed under sampling using the [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) clustering algorithm.\n",
    "\n",
    "So, what I want you to do is construct a function (I have started it below) that performs the following:\n",
    "\n",
    "* Groups the data by the `lab` column and finds the size of the smallest group.\n",
    "\n",
    "* For every group of samples in the dataset that is not the smallest group, you will use the KMeans algorithm to cluster the samples into `K` clusters. The size of `K` is the size of the smallest group.  **Note:** You need to make sure you are only using the descriptive features when doing this, so drop the label column from each group.\n",
    "\n",
    "* Once you have the `K` clusters, you will get the cluster centers by using the `cluster_centers_` attribute of your Kmeans object. This attribute is a `(n_clusters, n_features)` array of values. These will be the new set of samples of descriptive features for the class you are processing. You should construct a DataFrame with these and add a label column with your class label for each one of these samples.\n",
    "\n",
    "* The samples for the smalles class group from the original dataset will be the samples you return for that class. \n",
    "\n",
    "* You will need to concatenate all of the results into one DataFrame and return it at the end of the function.\n",
    "\n",
    "Once you have completed the function, you need to apply it to each of your three training sets (the ones that have not had the dicotimize process applied, I hope you kept a copy). Then you will apply the dicotomize process to the sampled training sets and place them into a list for use in the next problem.\n",
    "\n",
    "**Note:** By training our models on representations of the real data instead of the acutal measurements, we are building a type of surrogate model. By doing so, we can approximate how our model might behave when trained with the true data, but can test several different settings much faster than what we otherwise would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_under_sample_clust(data:DataFrame)->DataFrame:\n",
    "    #----------------------------------------------\n",
    "    temp = data.copy()\n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    freq = pd.DataFrame(temp['lab'].value_counts())\n",
    "    indices = freq.index.tolist()\n",
    "    min_val_ind = freq.idxmin()[0]\n",
    "    indices.remove(min_val_ind)\n",
    "    min_v = freq['lab'][min_val_ind]\n",
    "\n",
    "    desc_feats = temp.columns[1:]\n",
    "\n",
    "    res = pd.concat([res, temp[temp['lab'] == min_val_ind]])\n",
    "\n",
    "    for ind in indices:\n",
    "        classifier = KMeans(n_clusters=min_v)\n",
    "        classifier.fit(temp[temp['lab'] == ind].drop(['lab'], axis = 1))\n",
    "        temp2 = pd.DataFrame(classifier.cluster_centers_, columns=desc_feats)\n",
    "        temp2['lab'] = ind\n",
    "        res = pd.concat([res, temp2])\n",
    "    \n",
    "    res.reset_index(inplace=True, drop=True)\n",
    "    return res\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "for frame in train:\n",
    "     frame = perform_under_sample_clust(frame)\n",
    "\n",
    "us_di_train_x1 = []\n",
    "us_di_train_y1 = []\n",
    "\n",
    "for frame in train:\n",
    "     x1, y1 = dichotomize_X_y(frame)\n",
    "     us_di_train_x1.append(x1)\n",
    "     us_di_train_y1.append(y1)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6 (10 points)\n",
    "\n",
    "In question 5 we produced datasets that approxumate what our real data looks like. By training our models on these representations of the real data instead of the acutal measurements, we are building a type of surrogate model. In doing so, we can approximate how our model might behave when trained with the true data, but we obtain a major advantage in that we can test several different settings much faster than what we otherwise would if using the true dataset. We can then use these surrogate results to find a range of the hyperparameters that we might wish to investigate using the true input data.\n",
    "\n",
    "For this question, you will again train your models on the three different feature selected data. However, instead of the full partition 1 training datasets, you will be using the sampling with KMeans training datasets you constructed in Q5. \n",
    "\n",
    "You will again be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. Like before, you should set the `class_weight` to `balanced` when you construct your models. Unlike the previous question where you traind the models, this time you will be asked to evaluate several different settings for the `kernel`, the regularization parameter `C`, and kernel coefficient `gamma`. **Note:** The `gamma` paramter is only utilized on the ‘rbf’, ‘poly’ and ‘sigmoid’ kernels, so there is no reason to evaluate multiple settings for the `linear` kernel. I have listed the settings of each parameter in a code block below. \n",
    "\n",
    "For each of the settings, you should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['F-Val', 'FromLasso', 'FromForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['linear', 'poly', 'rbf']\n",
    "c_vals = [ 0.5, 1.0]\n",
    "gamma_vals = [0.5, 1, 10]\n",
    "temp = [kernel, c_vals, gamma_vals]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Val *-*-**-*-**-*-**-*-**-*-*\n",
      "TN=48\tFP=12\tFN=2\tTP=38\n",
      "TSS: 0.75\n",
      "HSS: 0.72\n",
      "TN=48\tFP=12\tFN=2\tTP=38\n",
      "TSS: 0.75\n",
      "HSS: 0.72\n",
      "TN=48\tFP=12\tFN=2\tTP=38\n",
      "TSS: 0.75\n",
      "HSS: 0.72\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=51\tFP=9\tFN=1\tTP=39\n",
      "TSS: 0.825\n",
      "HSS: 0.7983870967741935\n",
      "TN=51\tFP=9\tFN=1\tTP=39\n",
      "TSS: 0.825\n",
      "HSS: 0.7983870967741935\n",
      "TN=50\tFP=10\tFN=20\tTP=20\n",
      "TSS: 0.33333333333333337\n",
      "HSS: 0.34782608695652173\n",
      "TN=51\tFP=9\tFN=1\tTP=39\n",
      "TSS: 0.825\n",
      "HSS: 0.7983870967741935\n",
      "TN=49\tFP=11\tFN=1\tTP=39\n",
      "TSS: 0.7916666666666666\n",
      "HSS: 0.76\n",
      "TN=49\tFP=11\tFN=20\tTP=20\n",
      "TSS: 0.31666666666666665\n",
      "HSS: 0.329004329004329\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=49\tFP=11\tFN=1\tTP=39\n",
      "TSS: 0.7916666666666666\n",
      "HSS: 0.76\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=49\tFP=11\tFN=1\tTP=39\n",
      "TSS: 0.7916666666666666\n",
      "HSS: 0.76\n",
      "FromLasso *-*-**-*-**-*-**-*-**-*-*\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=39\tTP=1\n",
      "TSS: 0.025\n",
      "HSS: 0.029850746268656716\n",
      "TN=46\tFP=14\tFN=5\tTP=35\n",
      "TSS: 0.6416666666666666\n",
      "HSS: 0.6184738955823293\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=58\tFP=2\tFN=38\tTP=2\n",
      "TSS: 0.01666666666666667\n",
      "HSS: 0.0196078431372549\n",
      "TN=48\tFP=12\tFN=6\tTP=34\n",
      "TSS: 0.6499999999999999\n",
      "HSS: 0.6341463414634146\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "FromForest *-*-**-*-**-*-**-*-**-*-*\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=51\tFP=9\tFN=2\tTP=38\n",
      "TSS: 0.7999999999999999\n",
      "HSS: 0.7773279352226721\n",
      "TN=52\tFP=8\tFN=3\tTP=37\n",
      "TSS: 0.7916666666666667\n",
      "HSS: 0.7755102040816326\n",
      "TN=43\tFP=17\tFN=19\tTP=21\n",
      "TSS: 0.2416666666666667\n",
      "HSS: 0.24369747899159663\n",
      "TN=50\tFP=10\tFN=2\tTP=38\n",
      "TSS: 0.7833333333333333\n",
      "HSS: 0.7580645161290323\n",
      "TN=51\tFP=9\tFN=5\tTP=35\n",
      "TSS: 0.725\n",
      "HSS: 0.7131147540983607\n",
      "TN=43\tFP=17\tFN=19\tTP=21\n",
      "TSS: 0.2416666666666667\n",
      "HSS: 0.24369747899159663\n",
      "TN=43\tFP=17\tFN=2\tTP=38\n",
      "TSS: 0.6666666666666666\n",
      "HSS: 0.6274509803921569\n",
      "TN=43\tFP=17\tFN=1\tTP=39\n",
      "TSS: 0.6916666666666667\n",
      "HSS: 0.6484375\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=49\tFP=11\tFN=2\tTP=38\n",
      "TSS: 0.7666666666666666\n",
      "HSS: 0.7389558232931727\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "for ind in range(len(selected_labels)):\n",
    "     print(selected_labels[ind], \"*-*-*\"*5)\n",
    "     for kernel, c_value, gamma_value in params:\n",
    "         classifier = SVC(class_weight = 'balanced', gamma=gamma_value, C=c_value, kernel=kernel)\n",
    "         classifier.fit(us_di_train_x1[ind],di_train_y1[ind])\n",
    "         y_pred = classifier.predict(di_test_x2[ind])\n",
    "         tss_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "         hss_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "         print(f\"TSS: {tss_score}\")\n",
    "         print(f\"HSS: {hss_score}\")\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7 (10 points)\n",
    "\n",
    "Results above were able to find some combinations of hyperparamters and datasets that work fairly well for our problem. But the question remains, can we do better?\n",
    "\n",
    "Maybe one way to improve our results would be to elinate the easy to classify instances from our dataset and only focus our efforts on the more difficult ones. If you recall from our data preparation there was a feature in our dataset that we could use to easily distinguish between a rather large percentage of `flare` and `non-flare` data. This feature was `R_VALUE_median`, but we don't know what value to use to filter off part of our data.\n",
    "\n",
    "So, for this question, let's plot and see where a good cutoff might be. To do this, let's use the seaborn [ecdfplot](https://seaborn.pydata.org/generated/seaborn.ecdfplot.html#seaborn.ecdfplot) or the cumulative distribution function plot.  Your input will be the original analytics base table of partition one.  You should set the `x` axis to `R_VALUE_median`, and set the `hue` to `lab`.\n",
    "\n",
    "After plotting this, you will see that around 0.5 we begin to see some instances of the `M` and around 0.7 we begin to see some instances of the `X` class flares in our dataset. So, use 0.5 as a threshold value to filter out all of the instances that fall below this threshold from our training data. Construct a copy of the original partition 1 data with this applied.\n",
    "\n",
    "You can then verify this using the seaborn [kdeplot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html#seaborn.kdeplot) using the new filtered data as input, setting `x` to `R_VALUE_median` again, and setting `hue` to `lab` again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='R_VALUE_median', ylabel='Proportion'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "sns.ecdfplot(abt, hue='lab', x='R_VALUE_median')\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_abt = abt[abt['R_VALUE_median'] >= 0.5].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8 (10 points)\n",
    "\n",
    "For this question, you will utilize the filtered analytics base table you constructed in the previous question.  You should:\n",
    "\n",
    "* Repeat the feature selection I did for you in the  example using the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), and scoring function [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif).\n",
    "\n",
    "* Repeat the feature selection from Q1 using the [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), and utilizing the [LassoLars](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars) as your `estimator`.  You should also set the `max_features` to the number of features we are going to select (20 features).\n",
    "\n",
    "* Repeat the feature selection from Q2 using [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). With a random forest model called [ExtraTressClssifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) as the `estimator`. The `n_estimators` of the random forest algorithm should be set to `75` when you construct it. The `max_features` of the SelectFromModel should be set to the number of features we are going to select (20 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sthelluri1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [435] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "C:\\Users\\sthelluri1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "numFeat = 20\n",
    "\n",
    "# Split the target and descriptive features for Partition 1 into two \n",
    "# different DataFrame objects\n",
    "df_labels = filter_abt['lab'].copy()\n",
    "df_feats = filter_abt.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Split the target and descriptive features for Partition 2 inot two\n",
    "# different DataFrame Objects\n",
    "df_test_labels = abt2['lab'].copy()\n",
    "df_test_feats = abt2.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Do feature selection\n",
    "feats1 = SelectKBest(f_classif, k=numFeat).fit(df_feats, df_labels)\n",
    "# Construct a training dataset from Partition 1 with only the selected descriptive \n",
    "# features and the target feature\n",
    "df_selected_feats1 = df_feats.loc[:, feats1.get_support()]\n",
    "df_train_set1 = pd.concat([df_labels, df_selected_feats1], axis=1)\n",
    "\n",
    "# Construct a testing dataset from Partition 2 with only the selected descriptive\n",
    "# features and the target feature\n",
    "df_test_selected_feats1 = df_test_feats.loc[:, feats1.get_support()]\n",
    "df_test_set1 = pd.concat([df_test_labels, df_test_selected_feats1], axis=1)\n",
    "#----------------------------------------------\n",
    "lasso_labs = filter_abt['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso_feats = filter_abt.drop(['lab'], axis =1)\n",
    "lasso_df_feats = SelectFromModel(max_features = numFeat, estimator = LassoLars( alpha = 0, eps =1)).fit(lasso_feats, lasso_labs)\n",
    "lasso_train = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_train_set = pd.concat([filter_abt['lab'], lasso_train], axis = 1)\n",
    "#----------------------------------------------\n",
    "lasso2_labs = abt2_cpy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "lasso2_feats = abt2_cpy.drop(['lab'], axis =1)\n",
    "lasso_df_test_feats = lasso2_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test = lasso_feats.loc[:, lasso_df_feats.get_support()]\n",
    "lasso_test_set = pd.concat([abt2['lab'], lasso_test], axis = 1)\n",
    "#----------------------------------------------\n",
    "xtrees_labels = filter_abt['lab'].copy()\n",
    "xtrees_features = filter_abt.copy().drop(['lab'], axis = 1)\n",
    "xtrees_model_features = SelectFromModel(max_features = numFeat, estimator = ExtraTreesClassifier(n_estimators = 75)).fit(xtrees_features, xtrees_labels)\n",
    "xtrees_selected_features = xtrees_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_train = pd.concat([xtrees_labels, xtrees_selected_features], axis = 1)\n",
    "#----------------------------------------------\n",
    "xtrees_test_labels = abt2_cpy['lab']\n",
    "xtrees_test_features = abt2_cpy.drop(['lab'], axis = 1)\n",
    "xtrees_test_selected = xtrees_test_features.loc[:, xtrees_model_features.get_support()]\n",
    "xtrees_test = pd.concat([xtrees_test_labels, xtrees_test_selected], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q9 (10 points)\n",
    "\n",
    "Using the training and testing datsets you constructed in the previous question after performing feature selection on the filtered partition 1 analytics base table. You now need to perform the sampling on the training data using the function you made in Q5.\n",
    "\n",
    "Then you should convert each of the new training and testing datasets to a binary classification problem datase or dichotomize the training and testing data like you did in Q3. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed with the feature selected training and testing data.\n",
    "\n",
    "**Note:** You might want to put the training and testing tuples you get from the call to the dichotomize method into seperate training and testing lists. Then you can loop over them later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "train = [df_train_set1.copy(), lasso_train_set.copy(), xtrees_train.copy()]\n",
    "test = [df_test_set1.copy(), lasso_test_set.copy(), xtrees_test.copy()]\n",
    "\n",
    "#undersampling\n",
    "for frame in train:\n",
    "     frame = perform_under_sample_clust(frame)\n",
    "\n",
    "di_test_x1 = []\n",
    "di_test_y1 = []\n",
    "di_train_x2 = []\n",
    "di_train_y2 = []\n",
    "\n",
    "for frame in train:\n",
    "     x1, y1 = dichotomize_X_y(frame)\n",
    "     di_train_x1.append(x1)\n",
    "     di_train_y1.append(y1)\n",
    "    \n",
    "\n",
    "for frame in test:\n",
    "     x2, y2 = dichotomize_X_y(frame)\n",
    "     di_test_x2.append(x2)\n",
    "     di_test_y2.append(y2)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q10 (20 points)\n",
    "\n",
    "Like in Q6, this question will be utilizing the filtered and sampled datasets constructed in the previous question. For this question, you will again train your models on the three different feature selected data that had the instances below our thrshold filtered out and then had sampling by clustering performed on them. \n",
    "\n",
    "You will again be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. Like before, you should set the `class_weight` to `balanced` when you construct your models. You will again evaluate several different settings for the `kernel`, the regularization parameter `C`, and kernel coefficient `gamma`. **Note:** The `gamma` paramter is only utilized on the ‘rbf’, ‘poly’ and ‘sigmoid’ kernels, so there is no reason to evaluate multiple settings for the `linear` kernel. I have listed the settings of each parameter in a code block below. \n",
    "\n",
    "For each of the settings, you should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out. **Note:** The testing data has the samples in it that are below our threshold value, so you will first need to filter those out of the data you plan to pass to your model for testing. However, you still want those instances included in the calculation of the TSS and HSSS. So, your groud truth `lab` data should include all the instances in partition 2. You will need to concatenate a vector with all zeros in it to the match the labels you partitioned from the model testing data. \n",
    "\n",
    "Let's give you a representation of that:\n",
    "    \n",
    "    labels_from_data = [labels for samples > threshold] + [labels for samples <= threshold]\n",
    "    predict_labels = [labels from the model on > thrshold samples] + [0s the length of samples <= threshold]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "kernel = ['linear', 'poly', 'rbf']\n",
    "c_vals = [ 0.5, 1.0]\n",
    "gamma_vals = [0.5, 1, 10]\n",
    "temp = [kernel, c_vals, gamma_vals]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Val -_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_-\n",
      "\n",
      "TN=48\tFP=12\tFN=2\tTP=38\n",
      "TSS: 0.75\n",
      "HSS: 0.72\n",
      "TN=48\tFP=12\tFN=2\tTP=38\n",
      "TSS: 0.75\n",
      "HSS: 0.72\n",
      "TN=48\tFP=12\tFN=2\tTP=38\n",
      "TSS: 0.75\n",
      "HSS: 0.72\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=51\tFP=9\tFN=1\tTP=39\n",
      "TSS: 0.825\n",
      "HSS: 0.7983870967741935\n",
      "TN=51\tFP=9\tFN=1\tTP=39\n",
      "TSS: 0.825\n",
      "HSS: 0.7983870967741935\n",
      "TN=50\tFP=10\tFN=20\tTP=20\n",
      "TSS: 0.33333333333333337\n",
      "HSS: 0.34782608695652173\n",
      "TN=51\tFP=9\tFN=1\tTP=39\n",
      "TSS: 0.825\n",
      "HSS: 0.7983870967741935\n",
      "TN=49\tFP=11\tFN=1\tTP=39\n",
      "TSS: 0.7916666666666666\n",
      "HSS: 0.76\n",
      "TN=49\tFP=11\tFN=20\tTP=20\n",
      "TSS: 0.31666666666666665\n",
      "HSS: 0.329004329004329\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=49\tFP=11\tFN=1\tTP=39\n",
      "TSS: 0.7916666666666666\n",
      "HSS: 0.76\n",
      "TN=47\tFP=13\tFN=1\tTP=39\n",
      "TSS: 0.7583333333333333\n",
      "HSS: 0.7222222222222222\n",
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=49\tFP=11\tFN=1\tTP=39\n",
      "TSS: 0.7916666666666666\n",
      "HSS: 0.76\n",
      "FromLasso -_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_-\n",
      "\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=39\tTP=1\n",
      "TSS: 0.025\n",
      "HSS: 0.029850746268656716\n",
      "TN=46\tFP=14\tFN=5\tTP=35\n",
      "TSS: 0.6416666666666666\n",
      "HSS: 0.6184738955823293\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=58\tFP=2\tFN=38\tTP=2\n",
      "TSS: 0.01666666666666667\n",
      "HSS: 0.0196078431372549\n",
      "TN=48\tFP=12\tFN=6\tTP=34\n",
      "TSS: 0.6499999999999999\n",
      "HSS: 0.6341463414634146\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "TN=60\tFP=0\tFN=40\tTP=0\n",
      "TSS: 0\n",
      "HSS: 0.0\n",
      "FromForest -_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_--_-_-_-_-_-_-\n",
      "\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=51\tFP=9\tFN=2\tTP=38\n",
      "TSS: 0.7999999999999999\n",
      "HSS: 0.7773279352226721\n",
      "TN=52\tFP=8\tFN=3\tTP=37\n",
      "TSS: 0.7916666666666667\n",
      "HSS: 0.7755102040816326\n",
      "TN=43\tFP=17\tFN=19\tTP=21\n",
      "TSS: 0.2416666666666667\n",
      "HSS: 0.24369747899159663\n",
      "TN=50\tFP=10\tFN=2\tTP=38\n",
      "TSS: 0.7833333333333333\n",
      "HSS: 0.7580645161290323\n",
      "TN=51\tFP=9\tFN=5\tTP=35\n",
      "TSS: 0.725\n",
      "HSS: 0.7131147540983607\n",
      "TN=43\tFP=17\tFN=19\tTP=21\n",
      "TSS: 0.2416666666666667\n",
      "HSS: 0.24369747899159663\n",
      "TN=43\tFP=17\tFN=2\tTP=38\n",
      "TSS: 0.6666666666666666\n",
      "HSS: 0.6274509803921569\n",
      "TN=43\tFP=17\tFN=1\tTP=39\n",
      "TSS: 0.6916666666666667\n",
      "HSS: 0.6484375\n",
      "TN=47\tFP=13\tFN=2\tTP=38\n",
      "TSS: 0.7333333333333333\n",
      "HSS: 0.701195219123506\n",
      "TN=46\tFP=14\tFN=2\tTP=38\n",
      "TSS: 0.7166666666666666\n",
      "HSS: 0.6825396825396826\n",
      "TN=46\tFP=14\tFN=1\tTP=39\n",
      "TSS: 0.7416666666666667\n",
      "HSS: 0.7035573122529645\n",
      "TN=49\tFP=11\tFN=2\tTP=38\n",
      "TSS: 0.7666666666666666\n",
      "HSS: 0.7389558232931727\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "for ind in range(len(selected_labels)):\n",
    "     print(selected_labels[ind], \"-_-_-_-_-_-_-\"*5, end='\\n\\n')\n",
    "     for kernel, c_value, gamma_value in params:\n",
    "         classifier = SVC(class_weight = 'balanced', gamma=gamma_value, C=c_value, kernel=kernel)\n",
    "         classifier.fit(di_train_x1[ind],di_train_y1[ind])\n",
    "         y_pred = classifier.predict(di_test_x2[ind])\n",
    "         tss_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "         hss_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "         print(f\"TSS: {tss_score}\")\n",
    "         print(f\"HSS: {hss_score}\")\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these results are getting unruely, we should maybe be saving them to do analysis on them too? Maybe I'll ask you to do that for the extra credit assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
