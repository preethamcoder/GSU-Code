{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 5: Model Evaluation\n",
    "As in the previous assignments, in this homework assignment you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "\n",
    "This assignment will utilize a copy of the extracted feature dataset we have been working with. The dataset has been processed by performing outlier clipping, z-score and range scaling, and forward feature selection to select 20 features. We are now going to utilize more than one partition worth of data, so for the z-score and range scaling, the mean, standard deviation, minimum, and maximum were calculated using data from both partitions so that a global scaling can be performed on each partition. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This assignment will continue to only use [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB) and will add the use of [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD) as a testing set. \n",
    "\n",
    "---\n",
    "\n",
    "For this assignment, cleaning, transforming, and normalization of the data has been completed using both partitions to find the various minimum, maximum, standard deviation, and mean values needed to perform these operations. Recall from lecture that we should not perform these operations on each partition individually, but as a whole as there may(will) be different values for these in different partitions. \n",
    "\n",
    "For example, if we perform simple range scaling on each partition individually and we see a range of 0 to 100 in one partition and 0 to 10 in another. After individual scaling the values with 100 in the first would be mapped to 1 just like the values that had 10 in the second. This can cause serious performance problems in your model, so I have made sure that the normalization was treated properly for you. \n",
    "\n",
    "Below you will find the full partitions and `toy` sampled data from each partition, where only 20 samples from each of our 5 classes have been included in the data.  \n",
    "\n",
    "#### Full\n",
    "- [Full Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition1ExtractedFeatures.csv)\n",
    "- [Full Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "#### Toy\n",
    "- [Toy Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition1ExtractedFeatures.csv)\n",
    "- [Toy Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the two files, you should load each into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "For each of the models we evaluate in this assignmnet, you will calculate the True Skill Statistic score using the test data from Partition 2 to determine which model performs the best for classifying the positive flaring class.\n",
    "\n",
    "    True skill statistic (TSS) = TPR + TNR - 1 = TPR - (1-TNR) = TPR - FPR\n",
    "\n",
    "Where:\n",
    "\n",
    "    True positive rate (TPR) = TP/(TP+FN) Also known as recall or sensitivity\n",
    "    True negative rate (TNR) = TN/(TN+FP) Also known as specificity or selectivity\n",
    "    False positive rate (FPR) = FP/(FP+TN) = (1-TNR) Also known as fall-out or false alarm ratio\n",
    "\n",
    "\n",
    "**Recall**\n",
    "\n",
    "    True positive (TP)\n",
    "    True negative (TN)\n",
    "    False positive (FP)\n",
    "    False negative (FN)\n",
    "    \n",
    "See [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for more information.\n",
    "\n",
    "Below is a function implemented to provide your score for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tss(y_true=None, y_predict=None):\n",
    "    \"\"\"\n",
    "    Calculates the true skill score for binary classification based on the output of the confusion\n",
    "    table function\n",
    "    \"\"\"\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    \n",
    "    return tp_rate - fp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we will be utilizing a binary classification of our 5 class dataset. So, below is the helper function to change our class labels from the 5 class target feature to the binary target feature. The function is implemented to take a dataframe (e.g. our `abt`) and prepares it for a binary classification by merging the `X`- and `M`-class samples into one group, and the rest (`NF`, `B`, and `C`) into another group, labeled with `1`s and `0`s, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomize_X_y(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    dichotomizes the dataset and split it into the features (X) and the labels (y).\n",
    "    \n",
    "    :return: two np.ndarray objects X and y.\n",
    "    \"\"\"\n",
    "    data_dich = data.copy()\n",
    "    data_dich['lab'] = data_dich['lab'].map({'NF': 0, 'B': 0, 'C': 0, 'M': 1, 'X': 1})\n",
    "    y = data_dich['lab'].copy()\n",
    "    X = data_dich.copy().drop(['lab'], axis=1)\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/FDS'\n",
    "data_file = \"normalized_partition1ExtractedFeatures.csv\"\n",
    "data_file2 = \"normalized_partition2ExtractedFeatures.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file).replace('\\\\', '/'))\n",
    "abt2 = pd.read_csv(os.path.join(data_dir, data_file2).replace('\\\\', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTUSJH_var</th>\n",
       "      <th>TOTUSJH_difference_of_vars</th>\n",
       "      <th>TOTBSQ_min</th>\n",
       "      <th>TOTBSQ_max</th>\n",
       "      <th>TOTBSQ_median</th>\n",
       "      <th>TOTBSQ_mean</th>\n",
       "      <th>TOTBSQ_var</th>\n",
       "      <th>TOTBSQ_difference_of_mins</th>\n",
       "      <th>TOTBSQ_difference_of_maxs</th>\n",
       "      <th>...</th>\n",
       "      <th>TOTUSJZ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_gderivative_stddev</th>\n",
       "      <th>MEANPOT_max</th>\n",
       "      <th>MEANPOT_gderivative_mean</th>\n",
       "      <th>TOTFX_stddev</th>\n",
       "      <th>SAVNCPP_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTPOT_avg_mono_decrease_slope</th>\n",
       "      <th>USFLUX_stddev</th>\n",
       "      <th>TOTBSQ_dderivative_stddev</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.703435</td>\n",
       "      <td>0.691661</td>\n",
       "      <td>0.878752</td>\n",
       "      <td>0.886776</td>\n",
       "      <td>0.880269</td>\n",
       "      <td>0.880795</td>\n",
       "      <td>0.838756</td>\n",
       "      <td>0.731461</td>\n",
       "      <td>0.849843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988157</td>\n",
       "      <td>0.019057</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.322544</td>\n",
       "      <td>0.049426</td>\n",
       "      <td>0.932794</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>0.051082</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.536687</td>\n",
       "      <td>0.369924</td>\n",
       "      <td>0.827467</td>\n",
       "      <td>0.837925</td>\n",
       "      <td>0.832165</td>\n",
       "      <td>0.832613</td>\n",
       "      <td>0.805319</td>\n",
       "      <td>0.805999</td>\n",
       "      <td>0.814957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974288</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.322542</td>\n",
       "      <td>0.014624</td>\n",
       "      <td>0.987151</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>0.551995</td>\n",
       "      <td>0.831203</td>\n",
       "      <td>0.843844</td>\n",
       "      <td>0.835858</td>\n",
       "      <td>0.837174</td>\n",
       "      <td>0.819294</td>\n",
       "      <td>0.806446</td>\n",
       "      <td>0.828374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974871</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.322542</td>\n",
       "      <td>0.030668</td>\n",
       "      <td>0.949625</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.027575</td>\n",
       "      <td>0.006493</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.646995</td>\n",
       "      <td>0.467533</td>\n",
       "      <td>0.924507</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.924840</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.843487</td>\n",
       "      <td>0.825663</td>\n",
       "      <td>0.754150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.322539</td>\n",
       "      <td>0.055006</td>\n",
       "      <td>0.938782</td>\n",
       "      <td>0.999863</td>\n",
       "      <td>0.052563</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.508972</td>\n",
       "      <td>0.470260</td>\n",
       "      <td>0.863690</td>\n",
       "      <td>0.867887</td>\n",
       "      <td>0.866987</td>\n",
       "      <td>0.866551</td>\n",
       "      <td>0.803660</td>\n",
       "      <td>0.806323</td>\n",
       "      <td>0.791700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997272</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.322533</td>\n",
       "      <td>0.014136</td>\n",
       "      <td>0.985678</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>0.005817</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73487</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.462685</td>\n",
       "      <td>0.411532</td>\n",
       "      <td>0.753679</td>\n",
       "      <td>0.792289</td>\n",
       "      <td>0.777925</td>\n",
       "      <td>0.780560</td>\n",
       "      <td>0.774680</td>\n",
       "      <td>0.789393</td>\n",
       "      <td>0.793877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999178</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>0.010079</td>\n",
       "      <td>0.998463</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.829836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73488</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.606655</td>\n",
       "      <td>0.637593</td>\n",
       "      <td>0.809834</td>\n",
       "      <td>0.846074</td>\n",
       "      <td>0.843249</td>\n",
       "      <td>0.839888</td>\n",
       "      <td>0.841479</td>\n",
       "      <td>0.867723</td>\n",
       "      <td>0.790040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981707</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.322560</td>\n",
       "      <td>0.016209</td>\n",
       "      <td>0.924634</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.829848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73489</th>\n",
       "      <td>C</td>\n",
       "      <td>0.711481</td>\n",
       "      <td>0.737709</td>\n",
       "      <td>0.939336</td>\n",
       "      <td>0.943771</td>\n",
       "      <td>0.941188</td>\n",
       "      <td>0.941403</td>\n",
       "      <td>0.905944</td>\n",
       "      <td>0.895820</td>\n",
       "      <td>0.900880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910137</td>\n",
       "      <td>0.021655</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.322558</td>\n",
       "      <td>0.176565</td>\n",
       "      <td>0.999444</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>0.201647</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.829859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73490</th>\n",
       "      <td>B</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.611349</td>\n",
       "      <td>0.926691</td>\n",
       "      <td>0.929611</td>\n",
       "      <td>0.928393</td>\n",
       "      <td>0.927970</td>\n",
       "      <td>0.879584</td>\n",
       "      <td>0.886586</td>\n",
       "      <td>0.874680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962007</td>\n",
       "      <td>0.012777</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.322546</td>\n",
       "      <td>0.051507</td>\n",
       "      <td>0.915419</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.110994</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>0.829870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73491</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.326030</td>\n",
       "      <td>0.274840</td>\n",
       "      <td>0.732502</td>\n",
       "      <td>0.752903</td>\n",
       "      <td>0.742266</td>\n",
       "      <td>0.742535</td>\n",
       "      <td>0.711129</td>\n",
       "      <td>0.712962</td>\n",
       "      <td>0.653274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994676</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.322544</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.988084</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.829882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73492 rows × 722 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTUSJH_var  TOTUSJH_difference_of_vars  TOTBSQ_min  TOTBSQ_max  \\\n",
       "0      NF     0.703435                    0.691661    0.878752    0.886776   \n",
       "1      NF     0.536687                    0.369924    0.827467    0.837925   \n",
       "2      NF     0.593047                    0.551995    0.831203    0.843844   \n",
       "3      NF     0.646995                    0.467533    0.924507    0.925464   \n",
       "4      NF     0.508972                    0.470260    0.863690    0.867887   \n",
       "...    ..          ...                         ...         ...         ...   \n",
       "73487  NF     0.462685                    0.411532    0.753679    0.792289   \n",
       "73488  NF     0.606655                    0.637593    0.809834    0.846074   \n",
       "73489   C     0.711481                    0.737709    0.939336    0.943771   \n",
       "73490   B     0.732800                    0.611349    0.926691    0.929611   \n",
       "73491  NF     0.326030                    0.274840    0.732502    0.752903   \n",
       "\n",
       "       TOTBSQ_median  TOTBSQ_mean  TOTBSQ_var  TOTBSQ_difference_of_mins  \\\n",
       "0           0.880269     0.880795    0.838756                   0.731461   \n",
       "1           0.832165     0.832613    0.805319                   0.805999   \n",
       "2           0.835858     0.837174    0.819294                   0.806446   \n",
       "3           0.924840     0.924978    0.843487                   0.825663   \n",
       "4           0.866987     0.866551    0.803660                   0.806323   \n",
       "...              ...          ...         ...                        ...   \n",
       "73487       0.777925     0.780560    0.774680                   0.789393   \n",
       "73488       0.843249     0.839888    0.841479                   0.867723   \n",
       "73489       0.941188     0.941403    0.905944                   0.895820   \n",
       "73490       0.928393     0.927970    0.879584                   0.886586   \n",
       "73491       0.742266     0.742535    0.711129                   0.712962   \n",
       "\n",
       "       TOTBSQ_difference_of_maxs  ...  TOTUSJZ_slope_of_longest_mono_decrease  \\\n",
       "0                       0.849843  ...                                0.988157   \n",
       "1                       0.814957  ...                                0.974288   \n",
       "2                       0.828374  ...                                0.974871   \n",
       "3                       0.754150  ...                                0.999076   \n",
       "4                       0.791700  ...                                0.997272   \n",
       "...                          ...  ...                                     ...   \n",
       "73487                   0.793877  ...                                0.999178   \n",
       "73488                   0.790040  ...                                0.981707   \n",
       "73489                   0.900880  ...                                0.910137   \n",
       "73490                   0.874680  ...                                0.962007   \n",
       "73491                   0.653274  ...                                0.994676   \n",
       "\n",
       "       TOTUSJZ_gderivative_stddev  MEANPOT_max  MEANPOT_gderivative_mean  \\\n",
       "0                        0.019057     0.000029                  0.322544   \n",
       "1                        0.002260     0.000028                  0.322542   \n",
       "2                        0.007661     0.000025                  0.322542   \n",
       "3                        0.010761     0.000087                  0.322539   \n",
       "4                        0.004687     0.000122                  0.322533   \n",
       "...                           ...          ...                       ...   \n",
       "73487                    0.003487     0.000029                  0.322549   \n",
       "73488                    0.005644     0.000066                  0.322560   \n",
       "73489                    0.021655     0.000111                  0.322558   \n",
       "73490                    0.012777     0.000035                  0.322546   \n",
       "73491                    0.001097     0.000017                  0.322544   \n",
       "\n",
       "       TOTFX_stddev  SAVNCPP_slope_of_longest_mono_decrease  \\\n",
       "0          0.049426                                0.932794   \n",
       "1          0.014624                                0.987151   \n",
       "2          0.030668                                0.949625   \n",
       "3          0.055006                                0.938782   \n",
       "4          0.014136                                0.985678   \n",
       "...             ...                                     ...   \n",
       "73487      0.010079                                0.998463   \n",
       "73488      0.016209                                0.924634   \n",
       "73489      0.176565                                0.999444   \n",
       "73490      0.051507                                0.915419   \n",
       "73491      0.002210                                0.988084   \n",
       "\n",
       "       TOTPOT_avg_mono_decrease_slope  USFLUX_stddev  \\\n",
       "0                            0.999939       0.051082   \n",
       "1                            0.999982       0.013241   \n",
       "2                            0.999962       0.027575   \n",
       "3                            0.999863       0.052563   \n",
       "4                            0.999949       0.014227   \n",
       "...                               ...            ...   \n",
       "73487                        0.999983       0.008226   \n",
       "73488                        0.999961       0.022735   \n",
       "73489                        0.999772       0.201647   \n",
       "73490                        0.999905       0.110994   \n",
       "73491                        0.999996       0.001555   \n",
       "\n",
       "       TOTBSQ_dderivative_stddev  Unnamed: 0  \n",
       "0                       0.017805    0.000000  \n",
       "1                       0.003582    0.000011  \n",
       "2                       0.006493    0.000023  \n",
       "3                       0.018347    0.000034  \n",
       "4                       0.005817    0.000045  \n",
       "...                          ...         ...  \n",
       "73487                   0.005827    0.829836  \n",
       "73488                   0.007617    0.829848  \n",
       "73489                   0.030702    0.829859  \n",
       "73490                   0.028382    0.829870  \n",
       "73491                   0.001497    0.829882  \n",
       "\n",
       "[73492 rows x 722 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q1 (10 points)\n",
    "\n",
    "Just like you did with the previous assignment, you will be utilizing a few different types of feature selection to find subsets of descriptive features to use in the models we will be evaluating.  For this question you will again be utilizing the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). You will then be using 3 diferent feature evaluation functions.\n",
    "\n",
    "-  [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)\n",
    "\n",
    "- [scikit-learn mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "\n",
    "- [chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)\n",
    "\n",
    "For each of these combinations of evaluation functions, you need to construct a 20 feature training and testing dataset. This will be done by:\n",
    "<ol>\n",
    "    <li>Use the `SelectKBest` class with each of the evaluation functions to perform feature selection using Partition 1 as your input data</li>\n",
    "    <li>Construct a new train `DataFrame` for each instance of `SelectKBest` from 1 with the `lab` class labels using Partition 1</li>\n",
    "    <li>Construct a new test `DataFrame` for each instance of `SelectKBest` from 1 with the `lab` class labels using Partition 2</li>\n",
    "</ol>\n",
    "\n",
    "After this question, you should have a total of 6 `DataFrame`s to use in later questions, a train and test pair for each feature selection method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "abt_cpy = abt.copy()\n",
    "abt2_cpy = abt2.copy()\n",
    "y1 = abt_cpy['lab']\n",
    "y2 = abt2_cpy['lab']\n",
    "x1 = abt_cpy.drop(['lab'], axis=1)\n",
    "x2 = abt2_cpy.drop(['lab'], axis=1)\n",
    "labels = pd.DataFrame(y1, columns = ['lab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_classif\n",
    "selector = SelectKBest(k= numFeat)\n",
    "selector.fit_transform(x1.values, y1.values)\n",
    "col_index = selector.get_support(indices = True)\n",
    "columns = x1.columns[col_index]\n",
    "f_classif_abt = abt_cpy[columns.values].join(y1, rsuffix = \" \")\n",
    "f_classif_abt_2 = abt2_cpy[columns.values].join(y2, rsuffix = \" \")\n",
    "#m_classif\n",
    "selector = SelectKBest(mutual_info_classif, k= numFeat)\n",
    "selector.fit_transform(x1.values, y1.values)\n",
    "col_indices = selector.get_support(indices = True)\n",
    "columns = x1.columns[col_indices]\n",
    "m_classif_abt = abt_cpy[columns.values].join(y1, rsuffix = \" \")\n",
    "mi_classif_abt2 = abt2_cpy[columns.values].join(y2, rsuffix = \" \")\n",
    "#chi2\n",
    "selector = SelectKBest(chi2, k= numFeat)\n",
    "selector.fit_transform(x1.values, y1.values)\n",
    "col_indices = selector.get_support(indices = True)\n",
    "columns = x1.columns[col_indices]\n",
    "chi2_abt = abt_cpy[columns.values].join(y1, rsuffix = \" \")\n",
    "chi2_abt2 = abt_cpy[columns.values].join(y2, rsuffix = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2 (10 points)\n",
    "\n",
    "Now that we have our training and testing datasets for each of our feature subsets, we need to attempt to perform hyperparameter tuning on our model for each of the datasets. We want to see which combination of dataset and parameter settings seem to provide the best results. \n",
    "\n",
    "In order to do this, we must first dichotomize the training and testing data. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed in Q1.  \n",
    "\n",
    "With your binary classification dataset constructed, now it's time to start training and testing some models. We will start with the simple [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), and try several different settings to see how/if using different settings will improve our score. So, for each of your three copies of the Partition 1 training datasets that have had their `lab` columns converted to a binary label, train 4 different instances with the following settings. **(see documentation to know what these are)** In total you will train and evaluate 12 model setting and feature selected data pairings. \n",
    "\n",
    "|Model Number| n_neighbors | p |\n",
    "|------------|-------------|---|\n",
    "|1|3|1|\n",
    "|2|3|2|\n",
    "|3|5|1|\n",
    "|4|5|2|\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of the Partition 2 testing dataset that was cunstructed with the same features the model was trained on. You shall then calculate and print the TSS score for each result. **NOTE: The model does take a little while to evaluate.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "p = [1,2]\n",
    "temp = [n_neighbors, p]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=86045\tFP=1111\tFN=1038\tTP=363\n",
      "0.24635338460765865\n",
      "TN=85973\tFP=1183\tFN=1043\tTP=358\n",
      "0.24195840032045718\n",
      "TN=86182\tFP=974\tFN=1044\tTP=357\n",
      "0.24364262343639792\n",
      "TN=86094\tFP=1062\tFN=1057\tTP=344\n",
      "0.23335385328412084\n"
     ]
    }
   ],
   "source": [
    "train1, train2 = dichotomize_X_y(f_classif_abt)\n",
    "test1, test2 = dichotomize_X_y(f_classif_abt_2)\n",
    "for neigh, ps in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neigh, p=ps)\n",
    "    classifier.fit(train1, train2)\n",
    "    y_pred = classifier.predict(test1)\n",
    "    score = calc_tss(test2, y_pred)\n",
    "    print(score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=86081\tFP=1075\tFN=1123\tTP=278\n",
      "0.18609548774340784\n",
      "TN=86035\tFP=1121\tFN=1117\tTP=284\n",
      "0.18985035373820336\n",
      "TN=86253\tFP=903\tFN=1127\tTP=274\n",
      "0.18521385709918065\n",
      "TN=86241\tFP=915\tFN=1127\tTP=274\n",
      "0.1850761729466266\n"
     ]
    }
   ],
   "source": [
    "train2, train3 = dichotomize_X_y(m_classif_abt)\n",
    "test2, test3 = dichotomize_X_y(mi_classif_abt2)\n",
    "for neigh, ps in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neigh, p=ps)\n",
    "    classifier.fit(train2, train3)\n",
    "    y_pred = classifier.predict(test2)\n",
    "    score = calc_tss(test3, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=71113\tFP=1218\tFN=1128\tTP=33\n",
      "0.01158451973069399\n",
      "TN=71113\tFP=1218\tFN=1128\tTP=33\n",
      "0.01158451973069399\n",
      "TN=71124\tFP=1207\tFN=1129\tTP=32\n",
      "0.010875271926453621\n",
      "TN=71131\tFP=1200\tFN=1129\tTP=32\n",
      "0.010972049241850895\n"
     ]
    }
   ],
   "source": [
    "train3, train4 = dichotomize_X_y(chi2_abt)\n",
    "test3, test4 = dichotomize_X_y(chi2_abt2)\n",
    "for neigh, ps in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neigh, p=ps)\n",
    "    classifier.fit(train3, train4)\n",
    "    y_pred = classifier.predict(test3)\n",
    "    score = calc_tss(test4, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3 (10 points)\n",
    "\n",
    "After evaluating the various results from Q2, you will notice that the results are not all that great with greater than 1000 false negatives for nearly all of our settings tried. But, what can be done to improve our results? If you read the documentation for the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), which you certainly should have, you will see that we were only using the `MinkowskiDistance` metric with different values of `p`. If you look into the [DistanceMetric](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric) documentation for the neighbors classifiers, you will see there are several others available to use.\n",
    "\n",
    "So, for this question, train and evaluate two more instances of [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for each of our different feature selection train test datsets, but this time using the `ChebyshevDistance` metric instead of the `MinkowskiDistance` metric.  For these models you will only be changing the number neighbors to 3 and 5, as the values of `p` are not used for the `ChebyshevDistance` metric. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "temp = [n_neighbors]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=85934\tFP=1222\tFN=1056\tTP=345\n",
      "0.23223184045777573\n",
      "TN=86062\tFP=1094\tFN=1068\tTP=333\n",
      "0.22513516092584682\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrain, ytrain = dichotomize_X_y(f_classif_abt)\n",
    "xtest, ytest = dichotomize_X_y(f_classif_abt_2)\n",
    "for neighbor in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbor[0], metric=\"chebyshev\")\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=85990\tFP=1166\tFN=1144\tTP=257\n",
      "0.17006208955798865\n",
      "TN=86181\tFP=975\tFN=1171\tTP=230\n",
      "0.15298161371133678\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(m_classif_abt)\n",
    "xtest, ytest = dichotomize_X_y(mi_classif_abt2)\n",
    "for neighbor in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbor[0], metric=\"chebyshev\")\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=71123\tFP=1208\tFN=1130\tTP=31\n",
      "0.010000120152960791\n",
      "TN=71161\tFP=1170\tFN=1130\tTP=31\n",
      "0.010525482722260261\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(chi2_abt)\n",
    "xtest, ytest = dichotomize_X_y(chi2_abt2)\n",
    "for neighbor in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbor[0], metric=\"chebyshev\")\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4 (10 points)\n",
    "\n",
    "After evaluating the results from Q3, you will see that the results are no better than those we found for Q2. This leads to the thought that maybe the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) is just not a good fit for the problem we are applying it to. So, let's move on to another classifier for this problem. \n",
    "\n",
    "In this question, you will utilize the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and try several different settings to see how/if using different settings will improve our score. So, continuing to use our training/testing pairs constructed with different feature selection methods that have had their `lab` column converted to a binary label, train 8 different instances with the following settings. **(see documentation to know what these are)**\n",
    "\n",
    "|Model Number| criterion | max_depth | splitter |\n",
    "|------------|---------|-------------|---|\n",
    "|1|gini|5|best|\n",
    "|2|gini|5|random|\n",
    "|3|gini|None|best|\n",
    "|4|gini|None|random|\n",
    "|5|entropy|5|best|\n",
    "|6|entropy|5|random|\n",
    "|7|entropy|None|best|\n",
    "|8|entropy|None|random|\n",
    "\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of the Partition 2 testing dataset that was cunstructed with the same features the model was trained on. You shall then calculate and print the TSS score for each result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "depth = [5, None]\n",
    "splitter = ['best', 'random']\n",
    "temp = [criterion, depth, splitter]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=86976\tFP=180\tFN=1331\tTP=70\n",
      "0.04789904891797061\n",
      "TN=86827\tFP=329\tFN=1183\tTP=218\n",
      "0.15182830009799061\n",
      "TN=85844\tFP=1312\tFN=1073\tTP=328\n",
      "0.21906501944923784\n",
      "TN=85646\tFP=1510\tFN=1036\tTP=365\n",
      "0.24320293828398765\n",
      "TN=87002\tFP=154\tFN=1335\tTP=66\n",
      "0.04534226108433592\n",
      "TN=86833\tFP=323\tFN=1088\tTP=313\n",
      "0.21970585023993502\n",
      "TN=86131\tFP=1025\tFN=1155\tTP=246\n",
      "0.16382834373236874\n",
      "TN=85757\tFP=1399\tFN=1041\tTP=360\n",
      "0.2409076373232353\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrain, ytrain = dichotomize_X_y(f_classif_abt)\n",
    "xtest, ytest = dichotomize_X_y(f_classif_abt_2)\n",
    "for criteria, deep, split in params:\n",
    "    classifier = DecisionTreeClassifier(criterion = criteria, max_depth = deep, splitter = split)\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=86847\tFP=309\tFN=1313\tTP=88\n",
      "0.0592669100167727\n",
      "TN=87028\tFP=128\tFN=1339\tTP=62\n",
      "0.04278547325070122\n",
      "TN=85652\tFP=1504\tFN=1028\tTP=373\n",
      "0.24898198735526825\n",
      "TN=85417\tFP=1739\tFN=1038\tTP=363\n",
      "0.2391479139573305\n",
      "TN=86978\tFP=178\tFN=1356\tTP=45\n",
      "0.030077599417343465\n",
      "TN=86993\tFP=163\tFN=1340\tTP=61\n",
      "0.041670118598043156\n",
      "TN=85824\tFP=1332\tFN=1108\tTP=293\n",
      "0.19385339025850715\n",
      "TN=85706\tFP=1450\tFN=1121\tTP=280\n",
      "0.18322040972484493\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(m_classif_abt)\n",
    "xtest, ytest = dichotomize_X_y(mi_classif_abt2)\n",
    "for criteria, deep, split in params:\n",
    "    classifier = DecisionTreeClassifier(criterion = criteria, max_depth = deep, splitter = split)\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=71471\tFP=860\tFN=1139\tTP=22\n",
      "0.007059397276786134\n",
      "TN=71680\tFP=651\tFN=1145\tTP=16\n",
      "0.004780932751602473\n",
      "TN=71110\tFP=1221\tFN=1128\tTP=33\n",
      "0.011543043738380873\n",
      "TN=71110\tFP=1221\tFN=1128\tTP=33\n",
      "0.011543043738380873\n",
      "TN=71812\tFP=519\tFN=1154\tTP=7\n",
      "-0.0011460615711165424\n",
      "TN=71986\tFP=345\tFN=1151\tTP=10\n",
      "0.0038435253112095655\n",
      "TN=71110\tFP=1221\tFN=1128\tTP=33\n",
      "0.011543043738380873\n",
      "TN=71110\tFP=1221\tFN=1128\tTP=33\n",
      "0.011543043738380873\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(chi2_abt)\n",
    "xtest, ytest = dichotomize_X_y(chi2_abt2)\n",
    "for criteria, deep, split in params:\n",
    "    classifier = DecisionTreeClassifier(criterion = criteria, max_depth = deep, splitter = split)\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5 (10 points)\n",
    "\n",
    "After evaluating results from Q4, you will see that the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) was able to accomplish a bit of an improvement over the best resutls we found for the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).  This is indeed great, but can we do better than this if we use yet another classifier? Let's move on to yet another and find out.\n",
    "\n",
    "For this question you will be utilizing the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier. We won't be changing any of the default settings, just train 1 model for each of our feature selected data subsets. You will again be using your training/testing pairs constructed with different feature selection methods that have had their `lab` column converted to a binary label. You will then test each of your models using your binary classification copy of the Partition 2 testing dataset that was cunstructed with the same features the model was trained on. You shall then calculate and print the TSS score for each result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=78609\tFP=8547\tFN=113\tTP=1288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8212777885389588"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrain, ytrain = dichotomize_X_y(f_classif_abt)\n",
    "xtest, ytest = dichotomize_X_y(f_classif_abt_2)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "score = calc_tss(ytest, y_pred)\n",
    "score\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=76146\tFP=11010\tFN=176\tTP=1225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7480502361415888"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(m_classif_abt)\n",
    "xtest, ytest = dichotomize_X_y(mi_classif_abt2)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "score = calc_tss(ytest, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=62761\tFP=9570\tFN=999\tTP=162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00722646824208989"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(chi2_abt)\n",
    "xtest, ytest = dichotomize_X_y(chi2_abt2)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "score = calc_tss(ytest, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q6 (10 points)\n",
    "\n",
    "If you recall from a lecture some time back, it was shown that another way of improving the results of classification is to perform some form of sampling to balance the number of samples there are for the various classes. The reason why this works for specific classifiers, and methods for doing the sampling, are numerious and we don't have enough time to cover all of them in this course.  However, it is still beneficial to know this works and that it is something that you should be considering when you are training models.  \n",
    "\n",
    "So, for this question, we will implement a very naive method for sampling so we can use the results for training our models again.  Below you will find a function stub, complete the function and have it return a copy of the input dataframe where each class (except for the smallest one) have been undersampled to match the size of the smallest class in the dataset. In this function you should assume the `lab` column is the class label and not the dicotomized binary classification converted label.\n",
    "\n",
    "To do this you may want to use the [groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function of the DataFrame to get groups of rows from your DataFrame.  You may also wish to use the [sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) function to select a number of rows from a group. You can also use the [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method to process each group from your grouped rows. These are just hints, you can solve the problem how you see fit.\n",
    "\n",
    "Once this function is complete, apply it to each of your training datasets that have been constructed with different feature selection methods from partition 1 (the ones with all the NF, C, .., X labels). You will not be applying this to your testing sets. After you have your sampled feature selected datasets, you will then apply your function that converts the multi-class problem to a binary problem to each of the resultant selected subsets so we can use these new undersampled data for the next several questions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_under_sample(data:DataFrame)->DataFrame:\n",
    "    #----------------------------------------------\n",
    "    output = pd.DataFrame()\n",
    "    data.groupby('lab')\n",
    "    labels_set = list(set(data['lab'].tolist()))\n",
    "    values = dict(data['lab'].value_counts())\n",
    "    target = values[min(values, key=values.get)]\n",
    "    for label in labels_set:\n",
    "        data1 = data[data['lab']==label]\n",
    "        data1 = data1.sample(n = target)\n",
    "        output = pd.concat([output, data1])\n",
    "    return output\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    training_sets = [f_classif_abt, m_classif_abt, chi2_abt]\n",
    "    f_classif_train, m_classif_train, chi2_train = perform_under_sample(f_classif_abt), perform_under_sample(m_classif_abt), perform_under_sample(chi2_abt)\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q7\n",
    "\n",
    "For this question repeat what you did for Q2, but with your balanced binary classification datasets constructed in Q6, uese the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), and try several different settings to see how/if using different settings will improve our score. \n",
    "\n",
    "So, train 4 different instances with the following settings for each of your feature selected subsets, for a total of 12 different evaluations. **(see documentation to know what these are)**\n",
    "\n",
    "|Model Number| n_neighbors | p |\n",
    "|------------|-------------|---|\n",
    "|1|3|1|\n",
    "|2|3|2|\n",
    "|3|5|1|\n",
    "|4|5|2|\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (these should not have been balanced). You shall then calculate and print the TSS score for each result. **NOTE: The model now takes less time to evaluate!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "p = [1,2]\n",
    "temp = [n_neighbors, p]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=82433\tFP=4723\tFN=428\tTP=973\n",
      "0.6403137380579144\n",
      "TN=82308\tFP=4848\tFN=424\tTP=977\n",
      "0.6417346316329783\n",
      "TN=82750\tFP=4406\tFN=387\tTP=1014\n",
      "0.6732157052706104\n",
      "TN=82579\tFP=4577\tFN=391\tTP=1010\n",
      "0.6683986025992134\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "train1, train2 = dichotomize_X_y(f_classif_train)\n",
    "test1, test2 = dichotomize_X_y(f_classif_abt_2)\n",
    "for neigh, ps in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neigh, p=ps)\n",
    "    classifier.fit(train1, train2)\n",
    "    y_pred = classifier.predict(test1)\n",
    "    score = calc_tss(test2, y_pred)\n",
    "    print(score)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=81353\tFP=5803\tFN=531\tTP=870\n",
      "0.5544032492673798\n",
      "TN=81150\tFP=6006\tFN=571\tTP=830\n",
      "0.5235230573783227\n",
      "TN=81476\tFP=5680\tFN=457\tTP=944\n",
      "0.6086339265348417\n",
      "TN=81280\tFP=5876\tFN=489\tTP=912\n",
      "0.5835442573964448\n"
     ]
    }
   ],
   "source": [
    "train2, train3 = dichotomize_X_y(m_classif_train)\n",
    "test2, test3 = dichotomize_X_y(mi_classif_abt2)\n",
    "for neigh, ps in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neigh, p=ps)\n",
    "    classifier.fit(train2, train3)\n",
    "    y_pred = classifier.predict(test2)\n",
    "    score = calc_tss(test3, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=71113\tFP=1218\tFN=1128\tTP=33\n",
      "0.01158451973069399\n",
      "TN=71113\tFP=1218\tFN=1128\tTP=33\n",
      "0.01158451973069399\n",
      "TN=71124\tFP=1207\tFN=1129\tTP=32\n",
      "0.010875271926453621\n",
      "TN=71131\tFP=1200\tFN=1129\tTP=32\n",
      "0.010972049241850895\n"
     ]
    }
   ],
   "source": [
    "train3, train4 = dichotomize_X_y(chi2_abt)\n",
    "test3, test4 = dichotomize_X_y(chi2_abt2)\n",
    "for neigh, ps in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neigh, p=ps)\n",
    "    classifier.fit(train3, train4)\n",
    "    y_pred = classifier.predict(test3)\n",
    "    score = calc_tss(test4, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q8\n",
    "\n",
    "After evaluating the various results from Q7, you will notice that some of the results are improved over the same experiments we conducted in Q2. Additionally, you should also notice a improvement in the speed at which the results were obtained. The question now is will we continue to see these improvements for all of our experiments? So, let's move on and see.\n",
    "\n",
    "For this question, you will repeat the experiments from Q3, but using the balanced binary classification datasets constructed in Q6. You will still be using the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) like you did in Q7, but you will again be changing from using the `MinkowskiDistance` metric with different values of `p` to using the `ChebyshevDistance` metric. You will construct two models for each of your feature selected datasets by changing the number neighbors to 3 and 5.\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (these should not have been balanced), then calculate and print the TSS score for each result. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "temp = [n_neighbors]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=82114\tFP=5042\tFN=438\tTP=963\n",
      "0.6295158755920983\n",
      "TN=82473\tFP=4683\tFN=374\tTP=1027\n",
      "0.6793165824493687\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrain, ytrain = dichotomize_X_y(f_classif_train)\n",
    "xtest, ytest = dichotomize_X_y(f_classif_abt_2)\n",
    "for neighbor in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbor[0], metric=\"chebyshev\")\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=81310\tFP=5846\tFN=575\tTP=826\n",
      "0.5225037425815415\n",
      "TN=81110\tFP=6046\tFN=491\tTP=910\n",
      "0.5801661801531782\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(m_classif_train)\n",
    "xtest, ytest = dichotomize_X_y(mi_classif_abt2)\n",
    "for neighbor in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbor[0], metric=\"chebyshev\")\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=68023\tFP=4308\tFN=1077\tTP=84\n",
      "0.01279189622699578\n",
      "TN=67791\tFP=4540\tFN=1070\tTP=91\n",
      "0.015613704587167349\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(chi2_train)\n",
    "xtest, ytest = dichotomize_X_y(chi2_abt2)\n",
    "for neighbor in params:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbor[0], metric=\"chebyshev\")\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q9\n",
    "\n",
    "After evaluating the results of Q8 things are looking a little less encouraging, since none of those results look to be better than the results of Q7. However, the results from Q3 weren't really any better than Q2 in the first place, so not all is lost.  Let's continue on and see how things turn out with models like we used in Q4 since those were actaully an improvement over Q2 originally.\n",
    "\n",
    "So, in this question, you will utilize the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), like you did in Q4, and try several different settings to see how/if using different settings will improve our score. The difference will again be that you are now using the balanced binary classification datasets constructed in Q6 to train 8 different instances for each of your feature selected datasets using the following settings. **(see documentation to know what these are)**\n",
    "\n",
    "|Model Number| criterion | max_depth | splitter |\n",
    "|------------|---------|-------------|---|\n",
    "|1|gini|5|best|\n",
    "|2|gini|5|random|\n",
    "|3|gini|None|best|\n",
    "|4|gini|None|random|\n",
    "|5|entropy|5|best|\n",
    "|6|entropy|5|random|\n",
    "|7|entropy|None|best|\n",
    "|8|entropy|None|random|\n",
    "\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (this should not have been balanced), then calculate and print the TSS score for each result. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "depth = [5, None]\n",
    "splitter = ['best', 'random']\n",
    "temp = [criterion, depth, splitter]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=82502\tFP=4654\tFN=468\tTP=933\n",
      "0.6125543869600824\n",
      "TN=80443\tFP=6713\tFN=201\tTP=1200\n",
      "0.779508239575929\n",
      "TN=82448\tFP=4708\tFN=554\tTP=847\n",
      "0.5505500830773008\n",
      "TN=82311\tFP=4845\tFN=562\tTP=839\n",
      "0.5432679820073053\n",
      "TN=82666\tFP=4490\tFN=325\tTP=1076\n",
      "0.7165060204140097\n",
      "TN=82644\tFP=4512\tFN=344\tTP=1057\n",
      "0.702691857854527\n",
      "TN=82227\tFP=4929\tFN=525\tTP=876\n",
      "0.5687139002913184\n",
      "TN=81913\tFP=5243\tFN=620\tTP=781\n",
      "0.497302456900487\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrain, ytrain = dichotomize_X_y(f_classif_train)\n",
    "xtest, ytest = dichotomize_X_y(f_classif_abt_2)\n",
    "for criteria, deep, split in params:\n",
    "    classifier = DecisionTreeClassifier(criterion = criteria, max_depth = deep, splitter = split)\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=79726\tFP=7430\tFN=484\tTP=917\n",
      "0.5692830390125737\n",
      "TN=77417\tFP=9739\tFN=336\tTP=1065\n",
      "0.6484291427328662\n",
      "TN=80629\tFP=6527\tFN=573\tTP=828\n",
      "0.5161177186728506\n",
      "TN=79835\tFP=7321\tFN=642\tTP=759\n",
      "0.4577570819136191\n",
      "TN=81426\tFP=5730\tFN=532\tTP=869\n",
      "0.554527051987708\n",
      "TN=80669\tFP=6487\tFN=502\tTP=899\n",
      "0.5672547529286874\n",
      "TN=81537\tFP=5619\tFN=687\tTP=714\n",
      "0.44516536987063876\n",
      "TN=81316\tFP=5840\tFN=678\tTP=723\n",
      "0.4490536695971476\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(m_classif_train)\n",
    "xtest, ytest = dichotomize_X_y(mi_classif_abt2)\n",
    "for criteria, deep, split in params:\n",
    "    classifier = DecisionTreeClassifier(criterion = criteria, max_depth = deep, splitter = split)\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=67804\tFP=4527\tFN=1067\tTP=94\n",
      "0.018377413215356228\n",
      "TN=69025\tFP=3306\tFN=1101\tTP=60\n",
      "0.005973043034253556\n",
      "TN=68194\tFP=4137\tFN=1085\tTP=76\n",
      "0.008265416247069071\n",
      "TN=68380\tFP=3951\tFN=1096\tTP=65\n",
      "0.0013623369005425628\n",
      "TN=69243\tFP=3088\tFN=1092\tTP=69\n",
      "0.016738903126836117\n",
      "TN=68563\tFP=3768\tFN=1085\tTP=76\n",
      "0.013366963301582352\n",
      "TN=68418\tFP=3913\tFN=1089\tTP=72\n",
      "0.007916984568894572\n",
      "TN=67911\tFP=4420\tFN=1080\tTP=81\n",
      "0.008659479852474075\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(chi2_train)\n",
    "xtest, ytest = dichotomize_X_y(chi2_abt2)\n",
    "for criteria, deep, split in params:\n",
    "    classifier = DecisionTreeClassifier(criterion = criteria, max_depth = deep, splitter = split)\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    y_pred = classifier.predict(xtest)\n",
    "    score = calc_tss(ytest, y_pred)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q10\n",
    "\n",
    "Unlike with [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), it seems that the sampling didn't really help much for the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).  Where before we saw a 3X improvement with the Decision Tree over the KNN classifier, we now see similar results for both classifiers.  Let's see how switching to the sampled data affectes our best performing classifier when we were using the full dataset.\n",
    "\n",
    "For this question you will again be utilizing the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier as you did in Q5 but using your balanced binary classification dataset constructed in Q6 to train just 1 model for each feature selected dataset. Once you have done that, test the model using your binary classification copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (this should not have been balanced), then calculate and print the TSS score. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=83160\tFP=3996\tFN=348\tTP=1053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7057571729168491"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "xtrain, ytrain = dichotomize_X_y(f_classif_train)\n",
    "xtest, ytest = dichotomize_X_y(f_classif_abt_2)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "score = calc_tss(ytest, y_pred)\n",
    "score\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutual_info_classif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=81410\tFP=5746\tFN=378\tTP=1023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6642649577714547"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(m_classif_train)\n",
    "xtest, ytest = dichotomize_X_y(mi_classif_abt2)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "score = calc_tss(ytest, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chi2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=62761\tFP=9570\tFN=999\tTP=162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00722646824208989"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, ytrain = dichotomize_X_y(chi2_abt)\n",
    "xtest, ytest = dichotomize_X_y(chi2_abt2)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "score = calc_tss(ytest, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we don't see much improvement for our [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier. \n",
    "\n",
    "**Note:The TA would like you to turn in assignments that have been run and have results, so make sure to do a restart and run all from the kernel menu. Then make sure to save before you turn it in. You might find it necessary to use the toy dataset if you have time constraints.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
